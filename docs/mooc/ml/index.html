<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Машинное обучение #  Введение в машинное обучение и основные понятия статистики #  Представление данных #  Данные представляются в виде таблицы, в которой:
 строки - объекты столбцы - признаки  Ограничение табличного представления данных - все объекты должны иметь одинаковое количество признаков.
Типы признаков #   Количественный (числовой) - область значения - вещественные числа, сам имеет числовую природу Порядковый - задаёт порядок, последовательность объектов Номинальный (категориальный) - не имеет числовой природы, как правило, число возможных значений конечно."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Машинное обучение"><meta property="og:description" content="Машинное обучение #  Введение в машинное обучение и основные понятия статистики #  Представление данных #  Данные представляются в виде таблицы, в которой:
 строки - объекты столбцы - признаки  Ограничение табличного представления данных - все объекты должны иметь одинаковое количество признаков.
Типы признаков #   Количественный (числовой) - область значения - вещественные числа, сам имеет числовую природу Порядковый - задаёт порядок, последовательность объектов Номинальный (категориальный) - не имеет числовой природы, как правило, число возможных значений конечно."><meta property="og:type" content="article"><meta property="og:url" content="https://morggoth.github.io/docs/mooc/ml/"><meta property="article:published_time" content="2019-12-12T00:45:53+03:00"><meta property="article:modified_time" content="2019-12-12T00:45:53+03:00"><title>Машинное обучение | Morggoth's wiki</title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.6cd8553a6854f4812343f0f0c8baca31271e686434f381fbe3c7226f66639176.css integrity="sha256-bNhVOmhU9IEjQ/DwyLrKMSceaGQ084H748cib2ZjkXY="><script defer src=/en.search.min.04ef32ff2e1369424b9e223ba6948c21f934f56c052c5ff7bebaeb7976c5952e.js integrity="sha256-BO8y/y4TaUJLniI7ppSMIfk09WwFLF/3vrrreXbFlS4="></script></head><body><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><h2 class=book-brand><a href=/><span>Morggoth's wiki</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/misc/rpi/>Raspberry Pi</a></li><li><a href=/docs/misc/kafka/>Kafka basics</a></li><li><a href=/docs/mooc/ class=collapsed>Courses notes</a><ul><li><a href=/docs/mooc/datastax_academy/cassandra_operations/>DS210: DataStax Enterprise 6 Operations with Apache Cassandra™</a></li><li><a href=/docs/mooc/datastax_academy/casandra_foundations/>DS201: DataStax Enterprise 6 Foundations of Apache Cassandra™</a></li><li><a href=/docs/mooc/foundation_of_kubernetes/>Foundation of Kubernetes</a></li><li><a href=/docs/mooc/math/ class=collapsed>Книга для чтения по высшей математике</a></li><li><a href=/docs/mooc/ml/ class=active>Машинное обучение</a></li><li><a href=/docs/mooc/sams_cv/ class=collapsed>Нейронные сети и компьютерное зрение</a></li></ul></li><li><a href=/docs/misc/old_hardware/>Old hardware</a></li><li><a href=/docs/languages/ class=collapsed>Languages</a></li><li><a href=/docs/misc/chromeos/>ChromeOS</a></li><li><a href=/docs/misc/macos/>macOS</a></li><li><a href=/docs/misc/mikrotik/>Mikrotik</a></li><li><a href=/docs/misc/tips_and_tricks/>Tips & Tricks</a></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Машинное обучение</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#введение-в-машинное-обучение-и-основные-понятия-статистики>Введение в машинное обучение и основные понятия статистики</a><ul><li><a href=#представление-данных>Представление данных</a></li><li><a href=#типы-признаков>Типы признаков</a></li><li><a href=#характеристики-признаков>Характеристики признаков</a></li><li><a href=#коэффициент-корреляции>Коэффициент корреляции</a></li></ul></li><li><a href=#восстановление-пропущенных-значений>Восстановление пропущенных значений</a><ul><li><a href=#метрика-восстановление-пропущенных-объектов-по-их-близости-друг-к-другу>Метрика. Восстановление пропущенных объектов по их близости друг к другу</a></li><li><a href=#свойства-метрики>Свойства метрики</a></li><li><a href=#способы-нормирования-признаков>Способы нормирования признаков</a></li><li><a href=#использование-коэффициента-корреляции-для-восстановления-данных>Использование коэффициента корреляции для восстановления данных</a></li><li><a href=#применение-метрик-и-кк-в-рекомендательных-системах>Применение метрик и КК в рекомендательных системах</a></li></ul></li><li><a href=#поиск-выбросов-и-аномалий>Поиск выбросов и аномалий</a><ul><li><a href=#методы-обнаружения-выбросов>Методы обнаружения выбросов</a></li><li><a href=#методы-анализирующие-признаки-по-отдельности>Методы, анализирующие признаки по отдельности</a></li><li><a href=#простое-правило-для-определения-выброса-второй-метод>Простое правило для определения выброса (второй метод)</a></li><li><a href=#критерий-шовене-chauvenet>Критерий Шовене (Chauvenet)</a></li><li><a href=#поиск-выбросов-без-использования-среднего-и-отклонения>Поиск выбросов без использования среднего и отклонения</a></li><li><a href=#методы-анализирующие-несколько-признаков>Методы, анализирующие несколько признаков</a></li></ul></li><li><a href=#кластеризация-clustering>Кластеризация (clustering)</a><ul><li><a href=#кластеризация-с-помощью-графов>Кластеризация с помощью графов</a></li><li><a href=#алгоритм-forel-формальный-элемент>Алгоритм FOREL (формальный элемент)</a></li><li><a href=#алгоритмы-k-means-k-средних>Алгоритмы k-means (k-средних)</a></li><li><a href=#выбор-оптимального-числа-кластеров>Выбор оптимального числа кластеров</a></li><li><a href=#кластеризация-по-столбцам>Кластеризация по столбцам</a></li></ul></li><li><a href=#задача-предсказания-линейная-регрессия>Задача предсказания, линейная регрессия</a><ul><li><a href=#план-решения-задачи-регрессии>План решения задачи регрессии</a></li><li><a href=#построение-модели-линейной-регрессии>Построение модели линейной регрессии</a></li><li><a href=#проблемы-модели-линейной-регрессии>Проблемы модели линейной регрессии</a></li><li><a href=#полиноминальная-регрессия>Полиноминальная регрессия</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=машинное-обучение>Машинное обучение
<a class=anchor href=#%d0%bc%d0%b0%d1%88%d0%b8%d0%bd%d0%bd%d0%be%d0%b5-%d0%be%d0%b1%d1%83%d1%87%d0%b5%d0%bd%d0%b8%d0%b5>#</a></h1><h2 id=введение-в-машинное-обучение-и-основные-понятия-статистики>Введение в машинное обучение и основные понятия статистики
<a class=anchor href=#%d0%b2%d0%b2%d0%b5%d0%b4%d0%b5%d0%bd%d0%b8%d0%b5-%d0%b2-%d0%bc%d0%b0%d1%88%d0%b8%d0%bd%d0%bd%d0%be%d0%b5-%d0%be%d0%b1%d1%83%d1%87%d0%b5%d0%bd%d0%b8%d0%b5-%d0%b8-%d0%be%d1%81%d0%bd%d0%be%d0%b2%d0%bd%d1%8b%d0%b5-%d0%bf%d0%be%d0%bd%d1%8f%d1%82%d0%b8%d1%8f-%d1%81%d1%82%d0%b0%d1%82%d0%b8%d1%81%d1%82%d0%b8%d0%ba%d0%b8>#</a></h2><h3 id=представление-данных>Представление данных
<a class=anchor href=#%d0%bf%d1%80%d0%b5%d0%b4%d1%81%d1%82%d0%b0%d0%b2%d0%bb%d0%b5%d0%bd%d0%b8%d0%b5-%d0%b4%d0%b0%d0%bd%d0%bd%d1%8b%d1%85>#</a></h3><p>Данные представляются в виде таблицы, в которой:</p><ul><li>строки - объекты</li><li>столбцы - признаки</li></ul><p>Ограничение табличного представления данных - все объекты должны иметь одинаковое количество признаков.</p><h3 id=типы-признаков>Типы признаков
<a class=anchor href=#%d1%82%d0%b8%d0%bf%d1%8b-%d0%bf%d1%80%d0%b8%d0%b7%d0%bd%d0%b0%d0%ba%d0%be%d0%b2>#</a></h3><ul><li>Количественный (числовой) - область значения - вещественные числа, сам имеет числовую природу</li><li>Порядковый - задаёт порядок, последовательность объектов</li><li>Номинальный (категориальный) - не имеет числовой природы, как правило, число возможных значений конечно.<ul><li>Бинарный - частный случай номинального признака, имеющий два значения</li></ul></li></ul><h3 id=характеристики-признаков>Характеристики признаков
<a class=anchor href=#%d1%85%d0%b0%d1%80%d0%b0%d0%ba%d1%82%d0%b5%d1%80%d0%b8%d1%81%d1%82%d0%b8%d0%ba%d0%b8-%d0%bf%d1%80%d0%b8%d0%b7%d0%bd%d0%b0%d0%ba%d0%be%d0%b2>#</a></h3><ul><li><em>максимальное</em> и <em>минимальное</em> значения</li><li><em>среднее</em> арифметическое значение</li><li><em>медиана</em> - центральное значение в выборке, либо среднее от двух центральных значений, если количество элементов четное;
значение медианы в том, что на нее не так сильно влияет попадание в выборку аномальных данных</li><li><em>мода</em> - наиболее часто встречающееся значение в выборке; в отличии от среднего и медианы имеет смысл и для номинальных признаков</li><li><em>средне квадратичное отклонение</em> (<em>отклонение</em>) - отражает отличие элементов выборки друг от друга,
т.е. если все элементы одинаковы, отклонение - нулевое, в остальных случаях - положительное<ul><li>всегда неотрицательное</li><li>равно нулю, если все значение признака равны друг другу</li><li>чем больше величина отклонения, тем сильнее разброс значений выборки относительно среднего значения
<link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body);></script><span>\(
S_p = \sqrt{\frac{1}{n-1}\sum{i=1}_n(p_i - \overline{p})^2}
\)</span></li></ul></li></ul><p>Если медианное и среднее значения близки друг к другу, выборка называется <em>симметричной</em>. В таких выборках проще искать аномалии.</p><p>На практике симметричной считается выборка, для которой выполняется неравенство:</p><span>\(
|\overline{p} - h_p| \leqslant \frac{3S_p}{\sqrt{n}}
\)</span><p>Где <span>\(h_p\)</span>
- медиана</p><h3 id=коэффициент-корреляции>Коэффициент корреляции
<a class=anchor href=#%d0%ba%d0%be%d1%8d%d1%84%d1%84%d0%b8%d1%86%d0%b8%d0%b5%d0%bd%d1%82-%d0%ba%d0%be%d1%80%d1%80%d0%b5%d0%bb%d1%8f%d1%86%d0%b8%d0%b8>#</a></h3><p><em>Коэффициент корреляции</em> - величина, показывающая как значение одного признака определяет значение другого признака;
такая величина должна иметь смысл и для признаков с разными единицами измерения.</p><p>С геометрической точки зрения, КК - показатель того, как значения признаков ложатся на прямую.</p><p>Формула КК:</p><span>\(
r(P,Q) = \frac{\sum{i=1}^n {p_i}{q_i} - n\overline{p}\overline{q}}{(n-1){S_p}{S_q}}
\)</span><p>Свойства КК:</p><ul><li>принадлежит к отрезку [-1, 1]</li><li>если равен нулю или близок к нему, то очевидной зависимости между признаками нет.</li><li>КК > 0 - прямая зависимость между признаками</li><li>КК &lt; 0 - обратная зависимость</li><li>чем ближе модуль КК к 1, тем зависимость (при 1, она линейная)</li></ul><h2 id=восстановление-пропущенных-значений>Восстановление пропущенных значений
<a class=anchor href=#%d0%b2%d0%be%d1%81%d1%81%d1%82%d0%b0%d0%bd%d0%be%d0%b2%d0%bb%d0%b5%d0%bd%d0%b8%d0%b5-%d0%bf%d1%80%d0%be%d0%bf%d1%83%d1%89%d0%b5%d0%bd%d0%bd%d1%8b%d1%85-%d0%b7%d0%bd%d0%b0%d1%87%d0%b5%d0%bd%d0%b8%d0%b9>#</a></h2><p>Виды повреждения данных:</p><ul><li>пустые (пропущенные) значения</li><li>(заведомо) некорректные данные</li></ul><p>Способы борьбы с пропусками данных:</p><ul><li>удаление объекта (строки)</li><li>удаление столбца (в случае, если в нем много пустых объектов)</li><li>замена значения в ячейке:<ul><li>для числовых признаков:<ul><li>среднее</li><li>медиана</li></ul></li><li>для номинальных признаков:<ul><li>мода</li><li>генерация случайного значения с учетом вероятности, основанной на имеющихся признаках</li><li>приведение такого признака к числовому (может привести к появлению некорректных данных)</li></ul></li></ul></li></ul><h3 id=метрика-восстановление-пропущенных-объектов-по-их-близости-друг-к-другу>Метрика. Восстановление пропущенных объектов по их близости друг к другу
<a class=anchor href=#%d0%bc%d0%b5%d1%82%d1%80%d0%b8%d0%ba%d0%b0-%d0%b2%d0%be%d1%81%d1%81%d1%82%d0%b0%d0%bd%d0%be%d0%b2%d0%bb%d0%b5%d0%bd%d0%b8%d0%b5-%d0%bf%d1%80%d0%be%d0%bf%d1%83%d1%89%d0%b5%d0%bd%d0%bd%d1%8b%d1%85-%d0%be%d0%b1%d1%8a%d0%b5%d0%ba%d1%82%d0%be%d0%b2-%d0%bf%d0%be-%d0%b8%d1%85-%d0%b1%d0%bb%d0%b8%d0%b7%d0%be%d1%81%d1%82%d0%b8-%d0%b4%d1%80%d1%83%d0%b3-%d0%ba-%d0%b4%d1%80%d1%83%d0%b3%d1%83>#</a></h3><p>Метрика - обобщение понятия расстояния из геометрии и может быть вычислена для объектов произвольной природы.</p><p>Дано два набора данных:</p><span>\(
P = (p_1, p_2, ..., p_n)
Q = (q_1, q_2, ..., q_n)
\)</span><p>Формулы вычисления метрики:</p><ol><li>Евклидова метрика (геометрическая формула расстояния между точками):</li></ol><span>\(
\rho(P,Q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + ... + (p_n - q_n)^2}
\)</span><ol start=2><li>Метрика Манхеттен</li></ol><span>\(
\rho(P,Q) = |p_1 - q_1| + |p_2 - q_2| + ... + |p_n - q_n|
\)</span><ol start=3><li>Max-метрика</li></ol><span>\(
\rho(P,Q) = max(|p_1 - q_1|, |p_2 - q_2|, ..., |p_n - q_n|)
\)</span><h3 id=свойства-метрики>Свойства метрики
<a class=anchor href=#%d1%81%d0%b2%d0%be%d0%b9%d1%81%d1%82%d0%b2%d0%b0-%d0%bc%d0%b5%d1%82%d1%80%d0%b8%d0%ba%d0%b8>#</a></h3><ol><li>Расстояние от объекта до него же самого должно равняться нулю</li><li>Расстояние от одного объекта до другого должно равняться обратному расстоянию</li><li>Расстояние между двумя точками должно быть меньше чем сумма расстояний между этими точками и любой произвольной, не лежащей на прямой между ними (неравенство треугольника)</li></ol><p>Формула восстановления данных с помощью метрики</p><span>\(
P(A) = \frac{1}{\sum_{i=1}^n \frac{1}{\rho(A,A_i)}}(\sum_{j=1}^n \frac{P(A_j)}{\rho(A,A_j)})
\)</span><h3 id=способы-нормирования-признаков>Способы нормирования признаков
<a class=anchor href=#%d1%81%d0%bf%d0%be%d1%81%d0%be%d0%b1%d1%8b-%d0%bd%d0%be%d1%80%d0%bc%d0%b8%d1%80%d0%be%d0%b2%d0%b0%d0%bd%d0%b8%d1%8f-%d0%bf%d1%80%d0%b8%d0%b7%d0%bd%d0%b0%d0%ba%d0%be%d0%b2>#</a></h3><p>Нормирование - приведение всех признаков к единому масштабу.</p><p>Признак: <span>\(P = (p_1, p_2, ..., p_n)\)</span>
<span>\(\overline{p}\)</span>
- среднее значение
<span>\(s\)</span>
- отклонение</p><ol><li>Приведение всех признаков в интервал [0, 1]:</li></ol><span>\(
p' = \frac{p_i - min(p_i)}{max(p_i)-min(p_i)}
\)</span><ol start=2><li>Выполнить преобразование, после которого среднее значение и отклонение признака будут равны 0 и 1:</li></ol><span>\(
p'_i = \frac{p_i - \overline{p}}{s}
\)</span><ol start=3><li>Помимо приведенных формул, к значению признака можно предварительно применять различные функции, например логарифмирование.</li></ol><h3 id=использование-коэффициента-корреляции-для-восстановления-данных>Использование коэффициента корреляции для восстановления данных
<a class=anchor href=#%d0%b8%d1%81%d0%bf%d0%be%d0%bb%d1%8c%d0%b7%d0%be%d0%b2%d0%b0%d0%bd%d0%b8%d0%b5-%d0%ba%d0%be%d1%8d%d1%84%d1%84%d0%b8%d1%86%d0%b8%d0%b5%d0%bd%d1%82%d0%b0-%d0%ba%d0%be%d1%80%d1%80%d0%b5%d0%bb%d1%8f%d1%86%d0%b8%d0%b8-%d0%b4%d0%bb%d1%8f-%d0%b2%d0%be%d1%81%d1%81%d1%82%d0%b0%d0%bd%d0%be%d0%b2%d0%bb%d0%b5%d0%bd%d0%b8%d1%8f-%d0%b4%d0%b0%d0%bd%d0%bd%d1%8b%d1%85>#</a></h3><p>КК - выражает как сильно значение одного признака влияют на значения другого признака, т.е. меру близости признаков (т.е. столбцов таблицы).
Метрика - выражает меру близости объектов (строк таблицы).</p><p>Формула восстановления пропущенного значения с помощью среднего значения и КК:</p><p>Пусть P(A) - значение признака P объекта A
<span>\(\overline{P}\)</span>
- среднее значение признака P
Требуется определить P(A) по столбцам-признакам <span>\(P_1, P_2, ..., P_m\)</span></p><span>\(
P(A) = \overline{P} + \frac{\sum_{j=1}^n r(P,P_i)(P_i(A) - \overline{P_i})}{\sum_{i=1}^m |r(P,P_i|)}
\)</span><p>Все вычисления проводятся без учета строки с пропущенным значением.</p><p>При использовании КК в работе с данными признаки <em>можно не нормировать</em>. КК также <em>не изменится</em> при изменении масштаба признаков и переводе в другие единицы измерения.</p><h3 id=применение-метрик-и-кк-в-рекомендательных-системах>Применение метрик и КК в рекомендательных системах
<a class=anchor href=#%d0%bf%d1%80%d0%b8%d0%bc%d0%b5%d0%bd%d0%b5%d0%bd%d0%b8%d0%b5-%d0%bc%d0%b5%d1%82%d1%80%d0%b8%d0%ba-%d0%b8-%d0%ba%d0%ba-%d0%b2-%d1%80%d0%b5%d0%ba%d0%be%d0%bc%d0%b5%d0%bd%d0%b4%d0%b0%d1%82%d0%b5%d0%bb%d1%8c%d0%bd%d1%8b%d1%85-%d1%81%d0%b8%d1%81%d1%82%d0%b5%d0%bc%d0%b0%d1%85>#</a></h3><p>Алгоритмы восстановления данных можно использовать при проектировании рекомендательных систем.</p><p>Рекомендательная система:</p><ul><li>отслеживает историю действий пользователя</li><li>выдает рекомендации на основе действий других пользователей, предпочтения которых соотносятся с целевым пользователем</li></ul><p>С математической точки зрения это таблица, в которой:</p><ul><li>строки соответствуют пользователям</li><li>столбцы - товарам</li><li>на пересечении - оценка, которую пользователь поставил конкретному товару</li></ul><p>Таким образом, задача рекомендательной системы сводится к восстановлению пропущенного значения в таблице.</p><p>В случае, если пользователи анонимны, мерой близости может служить информация о том, как часто те или иные товары попадают в один заказ.</p><h2 id=поиск-выбросов-и-аномалий>Поиск выбросов и аномалий
<a class=anchor href=#%d0%bf%d0%be%d0%b8%d1%81%d0%ba-%d0%b2%d1%8b%d0%b1%d1%80%d0%be%d1%81%d0%be%d0%b2-%d0%b8-%d0%b0%d0%bd%d0%be%d0%bc%d0%b0%d0%bb%d0%b8%d0%b9>#</a></h2><p>Задача <em>поиска выбросов</em> (outlier detection) заключается в нахождении всех аномальных объектов в заданном множестве.</p><p><em>Выброс</em> - (зачастую) <em>реально существующий объект</em>, обладающий аномальными свойствами, сильно отличающийся от других объектов выборки.</p><ol><li>Если данные будут использоваться при решении задачи предсказания, то удаление выбросов, как правило, повышает точность предсказания</li><li>Удаление выбросов позволяет получить нормальные (типичные, эталонные) объекты</li><li>Многие характеристики (например, среднее значение) очень чувствительны к наличию выбросов</li></ol><p>Идеальных методов обнаружения выбросов не бывает потому, что&mldr;</p><ul><li>&mldr; не существует формального определения выброса</li><li>&mldr; алгоритм, беспощадный к выбросам, будет удалять и часть &ldquo;нормальных&rdquo; объектов</li><li>&mldr; алгоритм, гуманный к &ldquo;нормальным&rdquo; объектам, будет пропускать часть выбросов</li></ul><h3 id=методы-обнаружения-выбросов>Методы обнаружения выбросов
<a class=anchor href=#%d0%bc%d0%b5%d1%82%d0%be%d0%b4%d1%8b-%d0%be%d0%b1%d0%bd%d0%b0%d1%80%d1%83%d0%b6%d0%b5%d0%bd%d0%b8%d1%8f-%d0%b2%d1%8b%d0%b1%d1%80%d0%be%d1%81%d0%be%d0%b2>#</a></h3><ol><li>Поиск аномальных объектов с помощью здравого смысла; например, если известен нормальный диапазон для значений признака.</li><li>Методы, основанные на анализе одного признака (каждый признака берётся отдельно и ищутся объекты с аномальными значениями этого признака).</li><li>Методы, основанные на одновременном анализе нескольких признаков.</li></ol><h3 id=методы-анализирующие-признаки-по-отдельности>Методы, анализирующие признаки по отдельности
<a class=anchor href=#%d0%bc%d0%b5%d1%82%d0%be%d0%b4%d1%8b-%d0%b0%d0%bd%d0%b0%d0%bb%d0%b8%d0%b7%d0%b8%d1%80%d1%83%d1%8e%d1%89%d0%b8%d0%b5-%d0%bf%d1%80%d0%b8%d0%b7%d0%bd%d0%b0%d0%ba%d0%b8-%d0%bf%d0%be-%d0%be%d1%82%d0%b4%d0%b5%d0%bb%d1%8c%d0%bd%d0%be%d1%81%d1%82%d0%b8>#</a></h3><p>Дано:
<span>\(P = (p_1, p_2, ..., p_n) \)</span></p><p>Где,</p><span>\(\overline{p} - среднее значение\)</span>
<span>\(n - объём выборки\)</span>
<span>\(S_p - отклонение\)</span><p>Основная идея поиска аномалий - найти значения <span>\(p_i\)</span>
, расположенные вдали от среднего значения или медианы.</p><p>Простейшие методы поиска:</p><ol><li>Удалить все объекты, у которых величина <span>\(|p_i - \overline{p}|\)</span>
слишком велика
(этот метод не учитывает величину отклонения признака; для некоторых признаков большой показатель отклонения это норма)</li><li>Удалить все объекты, у которых величина <span>\(\frac{|p_i - \overline{p}|}{S_p}\)</span>
слишком велика</li><li>Более продвинутый критерий Шовене</li><li>Определение выбросов без использования среднего и отклонения</li></ol><h3 id=простое-правило-для-определения-выброса-второй-метод>Простое правило для определения выброса (второй метод)
<a class=anchor href=#%d0%bf%d1%80%d0%be%d1%81%d1%82%d0%be%d0%b5-%d0%bf%d1%80%d0%b0%d0%b2%d0%b8%d0%bb%d0%be-%d0%b4%d0%bb%d1%8f-%d0%be%d0%bf%d1%80%d0%b5%d0%b4%d0%b5%d0%bb%d0%b5%d0%bd%d0%b8%d1%8f-%d0%b2%d1%8b%d0%b1%d1%80%d0%be%d1%81%d0%b0-%d0%b2%d1%82%d0%be%d1%80%d0%be%d0%b9-%d0%bc%d0%b5%d1%82%d0%be%d0%b4>#</a></h3><ol><li>Пусть X - подозрительное значение</li><li>Исключим X из выборки и по оставшимся элементам вычислим среднее и отклонение</li><li>Если выборка симметричная, то X будет выбросом, если он не принадлежит интервалу
<span>\(
(\overline{p} - 3S_p, \overline{p} + 3S_p)
\)</span></li><li>Если выборка не симметричная, то X будет выбросом, если он не принадлежит интервалу
<span>\(
(\overline{p} - 5S_p, \overline{p} + 5S_p)
\)</span></li></ol><h3 id=критерий-шовене-chauvenet>Критерий Шовене (Chauvenet)
<a class=anchor href=#%d0%ba%d1%80%d0%b8%d1%82%d0%b5%d1%80%d0%b8%d0%b9-%d1%88%d0%be%d0%b2%d0%b5%d0%bd%d0%b5-chauvenet>#</a></h3><p>Значение <span>\(p_i\)</span>
является выбросом, если выполнено неравенство</p><span>\(erfc(\frac{|p_i - \overline{p}|}{S_p}) \le \frac{1}{2n}\)</span><p>Где
<span>\(erfc(x) = \frac{2}{\sqrt{\pi}}\int_x^\infty e^{-t^2}dt\)</span>
дополнение функции ошибок</p><p>Этот алгоритм применяется итерационно до тех пор, пока в выборке не перестанут находиться аномалии.</p><h3 id=поиск-выбросов-без-использования-среднего-и-отклонения>Поиск выбросов без использования среднего и отклонения
<a class=anchor href=#%d0%bf%d0%be%d0%b8%d1%81%d0%ba-%d0%b2%d1%8b%d0%b1%d1%80%d0%be%d1%81%d0%be%d0%b2-%d0%b1%d0%b5%d0%b7-%d0%b8%d1%81%d0%bf%d0%be%d0%bb%d1%8c%d0%b7%d0%be%d0%b2%d0%b0%d0%bd%d0%b8%d1%8f-%d1%81%d1%80%d0%b5%d0%b4%d0%bd%d0%b5%d0%b3%d0%be-%d0%b8-%d0%be%d1%82%d0%ba%d0%bb%d0%be%d0%bd%d0%b5%d0%bd%d0%b8%d1%8f>#</a></h3><p>Значение среднего и отклонения, сильно чувствительны к наличию выбросов. Таким образом, <em>возникает замкнутый круг</em>:
мы ищем выбросы с помощью характеристик, чьи значения обусловлены наличием выброса.</p><p><em>Квартили</em>:</p><ul><li><em>1-ая квартиль <span>\(Q_{25}\)</span></em> - это такое число, что ровно 25% выборки меньше его</li><li><em>2-ая квартиль <span>\(Q_{50}\)</span></em> - это такое число, что ровно 50% выборки меньше его (фактически это медиана)</li><li><em>3-ая квартиль <span>\(Q_{75}\)</span></em> - это такое число, что ровно 75% выборки меньше его</li></ul><p>Таким образом, 50% выборки принадлежат интервалу <span>\([Q_{25}, Q_{75}]\)</span>
, их можно считать &ldquo;эталонными&rdquo;.
Соответственно, элементы, достаточно удаленные от этого интервала можно считать выбросами.</p><p>Если элемент не попадает в интервал:
<span>\(
(Q_{25} - 1,5*(Q_{75} - Q_{25}), Q_{75} + 1,5*(Q_{75} - Q_{25}))
\)</span>
то он является выбросом.</p><h3 id=методы-анализирующие-несколько-признаков>Методы, анализирующие несколько признаков
<a class=anchor href=#%d0%bc%d0%b5%d1%82%d0%be%d0%b4%d1%8b-%d0%b0%d0%bd%d0%b0%d0%bb%d0%b8%d0%b7%d0%b8%d1%80%d1%83%d1%8e%d1%89%d0%b8%d0%b5-%d0%bd%d0%b5%d1%81%d0%ba%d0%be%d0%bb%d1%8c%d0%ba%d0%be-%d0%bf%d1%80%d0%b8%d0%b7%d0%bd%d0%b0%d0%ba%d0%be%d0%b2>#</a></h3><ul><li>необходимы в случаях, когда аномальное значение сходно, к примеру, со средним между нормальными значениями</li><li>аномалии могут характеризоваться не только экстремальными значениями одного признака, но и нестандартными комбинациями нескольких признаков
(например, сами по себе вес в 100 кг и рост 150 см не являются аномалиями, но их сочетание у одного объекта - является)</li></ul><ol><li>Метрические методы</li></ol><ul><li>опираются на функцию расстояния, то есть на метрику</li><li>основная идея: у выброса мало соседей, а у типичного объекта много<ul><li>для каждого объекта находится расстояние до всех остальных объектов и определяется ближайший сосед</li><li>если расстояние от объекта до ближайшего соседа велико, то такой объект - аномалия</li></ul></li></ul><ol start=2><li>Геометрические методы</li></ol><ul><li>объекты проецируются на n-мерное пространство (например, на плоскость, в случае, если объекты могут быть описаны двумя признаками)</li><li>строится выпуклая оболочка - многоугольник (для двумерного пространства) или многогранник, который неким образом описывает точки</li><li>аномалиями в данном случае будут точки, лежащие на оболочке</li></ul><ol start=3><li>Поиск с помощью кластеризации</li></ol><ul><li>при этом, объекты будут разделены на группы по некоторому признаку (или их сочетанию)</li><li>при таком подходе выбросами считаются элементы малых (в том числе одноэлементных групп)</li></ul><ol start=4><li>Поиск с помощью моделей предсказания</li></ol><ul><li>некоторые вариации метода опорных векторов (SVM)</li><li>вариация решающих деревьев (decision trees) под названием &ldquo;изолирующий лес&rdquo;</li><li>с помощью произвольной модели предсказания некоторого признака P по другим признакам таблицы<ul><li>в этом случае выбросом будет тот объект, для которого предсказанное и имеющееся значения разойдутся очень сильно</li></ul></li></ul><h2 id=кластеризация-clustering>Кластеризация (clustering)
<a class=anchor href=#%d0%ba%d0%bb%d0%b0%d1%81%d1%82%d0%b5%d1%80%d0%b8%d0%b7%d0%b0%d1%86%d0%b8%d1%8f-clustering>#</a></h2><p>Задача алгоритма кластеризации состоит в разбиении множества данных объектов на несколько групп(кластеров), состоящих из похожих друг на друга объектов.</p><p>Задачи кластеризации:</p><ul><li>вычисления степени сходства объектов</li><li>упрощение дальнейшей обработки данных (обработка меньших групп данных)</li><li>сокращение объема хранимых данных за счет оставления одного представителя (эталона) от каждого кластера (задача сжатия данных)</li><li>поиск выбросов</li><li>разбиение признаков на кластеры и оставление по одному признаку из каждого кластера (отбор признаков)</li></ul><p>Типы алгоритмов:</p><ol><li>разбивающие данные на заданное число кластеров (то есть число кластеров - входной параметр алгоритма)</li></ol><ul><li>пример: алгоритм k-means</li><li>недостатки:<ul><li>человеческий фактор(проблемы с предсказанием верного количества конечных кластеров)</li></ul></li></ul><ol start=2><li>в которых число кластеров не определено заранее, а вычисляется самим алгоритмом</li></ol><ul><li>пример: алгоритм FOREL</li><li>недостатки:<ul><li>алгоритм может выдать слишком много(мало) кластеров; в этом случае вся операция бесполезна</li></ul></li></ul><blockquote class="book-hint danger">Если алгоритм кластеризации использует метрику на множестве объектов, то значения всех признаков необходимо предварительно <em>нормировать</em>!!1!</blockquote><h3 id=кластеризация-с-помощью-графов>Кластеризация с помощью графов
<a class=anchor href=#%d0%ba%d0%bb%d0%b0%d1%81%d1%82%d0%b5%d1%80%d0%b8%d0%b7%d0%b0%d1%86%d0%b8%d1%8f-%d1%81-%d0%bf%d0%be%d0%bc%d0%be%d1%89%d1%8c%d1%8e-%d0%b3%d1%80%d0%b0%d1%84%d0%be%d0%b2>#</a></h3><p>Данные представляются в виде графа, где:</p><ul><li>вершина - объект</li><li>ребро - расстояние между объектами</li></ul><p>Соответственно перед построением графа необходимо вычислить расстояния между каждой парой объектов</p><p>Описание алгоритма:</p><ol><li>На вход алгоритма подается некоторое число R</li><li>Из графа удаляются все ребра, метрики которых > R</li><li>Оставшиеся после этого связными компоненты графа являются <em>кластерами</em></li></ol><p>Описание второго алгоритма</p><ol><li>На вход подается желаемое число кластеров k</li><li>Строится остовное дерево (это подграф, содержащий все вершины исходного графа и не имеющий циклов) c минимальной суммарной длиной ребер; для этой задачи могут применяться:</li></ol><ul><li>алгоритм Краскала</li><li>алгоритм Прима</li></ul><ol start=3><li>Удаляем из дерева k-1 самых длинных ребер</li><li>В один кластер попадают вершины из связанных компонент</li></ol><h3 id=алгоритм-forel-формальный-элемент>Алгоритм FOREL (формальный элемент)
<a class=anchor href=#%d0%b0%d0%bb%d0%b3%d0%be%d1%80%d0%b8%d1%82%d0%bc-forel-%d1%84%d0%be%d1%80%d0%bc%d0%b0%d0%bb%d1%8c%d0%bd%d1%8b%d0%b9-%d1%8d%d0%bb%d0%b5%d0%bc%d0%b5%d0%bd%d1%82>#</a></h3><p>Главное свойство алгоритма - количество кластеров не определено заранее
Идея - найти точки сгущения объектов и объявить эти сгущения кластерами</p><p>Описание алгоритма:</p><ul><li>На вход подается число R</li><li>Представление данных: объекты представляются точками в пространстве <span>\(R^m\)</span>
, где m - количество признаков объекта</li></ul><ol><li>В произвольную точку пространства добавляем формальный объект F (отсюда и название)</li><li>Пусть K - все объекты, до которых расстояние от F меньше R</li><li>Находим центр тяжести объектов из множества K и переносим туда объект F. Переходим к шагу 2</li></ol><ul><li>итерируемся по шагам 2-3 до тех пор, пока множество K не стабилизируется (то есть в него перестанут добавляться новые элементы и удаляться старые)</li></ul><ol start=4><li>После стабилизации множество K объявляется новым кластером и объекты, попавшие в него удаляются из выборки</li><li>Возвращаемся на шаг 1, если выборка не пуста, иначе алгоритм завершается</li></ol><p><em>Центр тяжести</em> - точка, координаты которой совпадают со средним значением признака.</p><h3 id=алгоритмы-k-means-k-средних>Алгоритмы k-means (k-средних)
<a class=anchor href=#%d0%b0%d0%bb%d0%b3%d0%be%d1%80%d0%b8%d1%82%d0%bc%d1%8b-k-means-k-%d1%81%d1%80%d0%b5%d0%b4%d0%bd%d0%b8%d1%85>#</a></h3><ul><li>Главное свойство: количество кластеров k определено заранее</li><li>Идея реализации: одновременно происходит поиск всех центров кластеров</li></ul><p>Описание алгоритма (одна из реализаций):</p><ul><li>Вход: число кластеров k</li><li>Представление данных: объекты представляются точками в пространстве <span>\(R^m\)</span>
, где m - количество признаков объекта</li></ul><ol><li>Генерируем k случайных точек - центры кластеров</li><li>Объект будет отнесен к тому кластеру, чей центр расположен ближе всех к этому объекту</li><li>В получившихся кластерах центр переносится в центр тяжести, возврат на шаг 2</li></ol><ul><li>шаги 2-3 повторяются до тех пор, пока центры кластеров не стабилизируются</li></ul><p>Недостатки алгоритма:</p><ul><li>результат зависит от выбора исходных центров кластеров, их оптимальный выбор неизвестен</li></ul><h3 id=выбор-оптимального-числа-кластеров>Выбор оптимального числа кластеров
<a class=anchor href=#%d0%b2%d1%8b%d0%b1%d0%be%d1%80-%d0%be%d0%bf%d1%82%d0%b8%d0%bc%d0%b0%d0%bb%d1%8c%d0%bd%d0%be%d0%b3%d0%be-%d1%87%d0%b8%d1%81%d0%bb%d0%b0-%d0%ba%d0%bb%d0%b0%d1%81%d1%82%d0%b5%d1%80%d0%be%d0%b2>#</a></h3><p>Эта проблема актуальна для алгоритмов, в которых число кластеров является входным параметром.</p><p>Идея - будем перебирать значения k, пока &ldquo;качество кластеризации&rdquo; не стабилизируется.</p><ul><li>Пусть <span>\(S_k\)</span>
- сумма расстояний от объектов до центров их кластеров (при условии, что объекты разбиты на k кластеров).</li><li>Тогда величину <span>\(|S_{k+1} - S_k|\)</span>
можно рассматривать как увеличение качества кластеризации при переходе от k кластеров к (k+1) кластеру.</li><li>Таким образом, &ldquo;качество кластеризации&rdquo; стабилизируется для такого k, где величина <span>\(|S_{k+1} - S_k|\)</span>
становится небольшой.</li></ul><h3 id=кластеризация-по-столбцам>Кластеризация по столбцам
<a class=anchor href=#%d0%ba%d0%bb%d0%b0%d1%81%d1%82%d0%b5%d1%80%d0%b8%d0%b7%d0%b0%d1%86%d0%b8%d1%8f-%d0%bf%d0%be-%d1%81%d1%82%d0%be%d0%bb%d0%b1%d1%86%d0%b0%d0%bc>#</a></h3><p><em>Транспонирование</em> - зеркальное отображение таблицы отнисительно ее диагонали; операция, позволяющая поменять местами строки и столбцы, в нашем случае - объекты и признаки.</p><p>Собственно сама кластеризация по столбцам выполняется в два шага:</p><ul><li>данные транспонируются</li><li>применяется один из стандартных алгоритмов кластеризации</li></ul><p>Назначение:</p><ul><li>позволяет найти близкие (по значению) друг к другу признаки. Можно из каждого кластера оставить по одному признаку и тем самым уменьшить размер данных</li><li>это оправдано, так как слишком большое число признаков часто мешает анализу данных</li></ul><h2 id=задача-предсказания-линейная-регрессия>Задача предсказания, линейная регрессия
<a class=anchor href=#%d0%b7%d0%b0%d0%b4%d0%b0%d1%87%d0%b0-%d0%bf%d1%80%d0%b5%d0%b4%d1%81%d0%ba%d0%b0%d0%b7%d0%b0%d0%bd%d0%b8%d1%8f-%d0%bb%d0%b8%d0%bd%d0%b5%d0%b9%d0%bd%d0%b0%d1%8f-%d1%80%d0%b5%d0%b3%d1%80%d0%b5%d1%81%d1%81%d0%b8%d1%8f>#</a></h2><p><em>Предсказание (prediction):</em></p><ul><li>есть множество объектов M с известными значениями признака Y (целевой признак)</li><li>требуется найти Y для нового объекта <span>\(A \notin M\)</span></li></ul><p>Задачи предсказания:</p><ol><li>Предсказание <em>количественного</em> признака Y называется задачей <em>регрессии</em></li><li>Предсказание номинального (категориального) признака Y называется задачей <em>классификации</em></li></ol><h3 id=план-решения-задачи-регрессии>План решения задачи регрессии
<a class=anchor href=#%d0%bf%d0%bb%d0%b0%d0%bd-%d1%80%d0%b5%d1%88%d0%b5%d0%bd%d0%b8%d1%8f-%d0%b7%d0%b0%d0%b4%d0%b0%d1%87%d0%b8-%d1%80%d0%b5%d0%b3%d1%80%d0%b5%d1%81%d1%81%d0%b8%d0%b8>#</a></h3><p>Общий план решения:</p><ul><li>множество объектов разбить на 2 множества:<ul><li>тренировочную выборку (Train)</li><li>тестовую или проверочную выборку (Test)</li></ul></li><li>модель предсказания будет строиться по объектам Train, а качество проверяться объектам Test</li></ul><p>Показатели качества регрессии:</p><ol><li>MAE (средняя абсолютная ошибка):</li></ol><span>\(
MAE = \frac{1}{n} \sum_{i=1}^n{|y_i - y'_i|}
\)</span><p>Где:</p><span>\(n - объем тестовой выборки\)</span>
<span>\(y_i - истинные значения\)</span>
<span>\(Y'_i - предсказанные значения\)</span><ol start=2><li>MAPE (средняя абсолютная ошибка в процентах):</li></ol><span>\(
MAPE = \frac{1}{n} \sum_{i=1}^n{\frac{|y_i - y'_i|}{|y_i|}} * 100\%
\)</span><blockquote class="book-hint danger">Модель регрессии не обязана давать точный ответ на объектах тренировочной выборки,
так как модель не запоминает значения признаков тренировочной модели, а выстраивает зависимость между значениями целевого и нецелевых признаков.</blockquote><p>Принцип работы модели (линейной) регрессии:</p><p>Узлы - это объекты тренировочной выборки</p><p>Предсказываться значения Y в новых точках можно с помощью:</p><ul><li>ломаной линии (линия, соединяющая все точки, лежащие на пересечении целевого и нецелевого признаков)<ul><li>недостатки:<ul><li>модель предсказания имеет сложность сравнимую с объемом данных</li><li>модель нельзя проинтерпретировать</li><li>нет уверенности, что на тестовой выборке будут небольшие ошибки</li><li>эту модель нельзя экстраполировать</li></ul></li></ul></li><li>прямой регрессии, проходящей примерно посередине между объектами</li></ul><p>Модель регрессии называется <em>линейной</em>, если значение предсказываемого признака Y вычисляется как сумма известных признаков <span>\(X_1, X_2,... X_m\)</span>
,
взятых с некоторыми коэффициентами</p><span>\(
y' = w_1x_1 + w_2x_2 + ... + w_mx_m + w_0
\)</span><p>Задача заключается в нахождении оптимальных весов (коэффициентов) <span>\(w_i\)</span>
.</p><p>Принцип: для объектов тренировочной выборки нужно минимизировать отклонение предсказываемых значений от истинных значений признака Y.</p><blockquote class="book-hint danger">Линейная регрессия неустойчива к выбросам, соответственно, их необходимо удалять заранее.</blockquote><h3 id=построение-модели-линейной-регрессии>Построение модели линейной регрессии
<a class=anchor href=#%d0%bf%d0%be%d1%81%d1%82%d1%80%d0%be%d0%b5%d0%bd%d0%b8%d0%b5-%d0%bc%d0%be%d0%b4%d0%b5%d0%bb%d0%b8-%d0%bb%d0%b8%d0%bd%d0%b5%d0%b9%d0%bd%d0%be%d0%b9-%d1%80%d0%b5%d0%b3%d1%80%d0%b5%d1%81%d1%81%d0%b8%d0%b8>#</a></h3><p>Отклонение истинного от предсказанного значения равно <span>\(|y' - y| = |w_1x + w_0 - y|\)</span>
.
Эту величину нужно минимизировать для всех объектов тренировочной выборки.</p><p>Поиск точки минимума этой функции осложняется тем, что модуль-функция - <em>недифференцируемая</em>.
Поэтому на практике минимизируют несколько иную функцию: <em>сумму квадратов отклонений</em>.
Для нахождения точки минимума применяют <em>частные производные</em>.</p><p>В общем случае:</p><p>Когда нецелевых признаков больше одного, все происходит аналогично, только параметров <span>\(w_i\)</span>
будет больше
(и полученная зависимость <span>\(y'=...\)</span>
будет уже определять не прямую, а гиперплоскость).</p><p>Таким образом, основная трудоемкость при построении линейной регрессии заключается в решении системы линейных уравнений на последнем шаге.</p><h3 id=проблемы-модели-линейной-регрессии>Проблемы модели линейной регрессии
<a class=anchor href=#%d0%bf%d1%80%d0%be%d0%b1%d0%bb%d0%b5%d0%bc%d1%8b-%d0%bc%d0%be%d0%b4%d0%b5%d0%bb%d0%b8-%d0%bb%d0%b8%d0%bd%d0%b5%d0%b9%d0%bd%d0%be%d0%b9-%d1%80%d0%b5%d0%b3%d1%80%d0%b5%d1%81%d1%81%d0%b8%d0%b8>#</a></h3><p>Получаемая при построении линейной регрессии система линейных уравнений может:</p><ol><li>Не иметь решений</li><li>Иметь более одного решения</li></ol><p>Такие проблемы возникают, когда между нецелевыми признаками существует <em>линейная зависимость</em> или <em>высокая корреляция</em>.
Такую проблему еще называют <em>&ldquo;проблемой мультиколлинеарности&rdquo;</em>.</p><p>Например, в случае, если значения нецелевых признаков совпадают, на выходе мы фактически получим систему, в которой неизвестных больше, чем уравнений
(потому что несколько уравнений будут идентичны), а такая система имеет бесконечное количество решений.</p><p>Проблемы системы с бесконечным количеством решений:</p><ul><li>зависимость целевого признака Y никак нельзя проинтерпретировать (так как любая выбранная модель будет одинаково хороша или плоха)</li><li>возможна большая ошибка на объектах, не попавших в тренировочную выборку; это произойдет, когда в качестве коэффициентов будут выбраны большие числа</li></ul><p>Советы по поиску хорошей модели регрессии:</p><ol><li><em>Отбор признаков</em>. Нужно удалять нецелевые признаки, которые линейно зависят от других или имеют высокую корреляцию с другими признаками</li><li>Коэффициент регрессии можно минимизировать (&ldquo;регуляризация&rdquo; и &ldquo;лассо&rdquo;)</li></ol><p>Обобщение модели линейной регрессии:</p><ol><li>Регуляризация (Ridge-regression)</li></ol><p>Основная идея:</p><ul><li>нужно стремиться сделать коэффициент регрессии минимальным</li><li>величины весов также должны быть минимальными (по модулю)</li></ul><p>Способ минимизации:</p><span>\(
R = L(w_0, w_1, w_2, ..., w_m) + C(w_0^2, w_1^2, w_2^2, ..., w_m^2)
\)</span><p>Где C - заданная константа</p><ol start=2><li>Лассо</li></ol><h3 id=полиноминальная-регрессия>Полиноминальная регрессия
<a class=anchor href=#%d0%bf%d0%be%d0%bb%d0%b8%d0%bd%d0%be%d0%bc%d0%b8%d0%bd%d0%b0%d0%bb%d1%8c%d0%bd%d0%b0%d1%8f-%d1%80%d0%b5%d0%b3%d1%80%d0%b5%d1%81%d1%81%d0%b8%d1%8f>#</a></h3></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#введение-в-машинное-обучение-и-основные-понятия-статистики>Введение в машинное обучение и основные понятия статистики</a><ul><li><a href=#представление-данных>Представление данных</a></li><li><a href=#типы-признаков>Типы признаков</a></li><li><a href=#характеристики-признаков>Характеристики признаков</a></li><li><a href=#коэффициент-корреляции>Коэффициент корреляции</a></li></ul></li><li><a href=#восстановление-пропущенных-значений>Восстановление пропущенных значений</a><ul><li><a href=#метрика-восстановление-пропущенных-объектов-по-их-близости-друг-к-другу>Метрика. Восстановление пропущенных объектов по их близости друг к другу</a></li><li><a href=#свойства-метрики>Свойства метрики</a></li><li><a href=#способы-нормирования-признаков>Способы нормирования признаков</a></li><li><a href=#использование-коэффициента-корреляции-для-восстановления-данных>Использование коэффициента корреляции для восстановления данных</a></li><li><a href=#применение-метрик-и-кк-в-рекомендательных-системах>Применение метрик и КК в рекомендательных системах</a></li></ul></li><li><a href=#поиск-выбросов-и-аномалий>Поиск выбросов и аномалий</a><ul><li><a href=#методы-обнаружения-выбросов>Методы обнаружения выбросов</a></li><li><a href=#методы-анализирующие-признаки-по-отдельности>Методы, анализирующие признаки по отдельности</a></li><li><a href=#простое-правило-для-определения-выброса-второй-метод>Простое правило для определения выброса (второй метод)</a></li><li><a href=#критерий-шовене-chauvenet>Критерий Шовене (Chauvenet)</a></li><li><a href=#поиск-выбросов-без-использования-среднего-и-отклонения>Поиск выбросов без использования среднего и отклонения</a></li><li><a href=#методы-анализирующие-несколько-признаков>Методы, анализирующие несколько признаков</a></li></ul></li><li><a href=#кластеризация-clustering>Кластеризация (clustering)</a><ul><li><a href=#кластеризация-с-помощью-графов>Кластеризация с помощью графов</a></li><li><a href=#алгоритм-forel-формальный-элемент>Алгоритм FOREL (формальный элемент)</a></li><li><a href=#алгоритмы-k-means-k-средних>Алгоритмы k-means (k-средних)</a></li><li><a href=#выбор-оптимального-числа-кластеров>Выбор оптимального числа кластеров</a></li><li><a href=#кластеризация-по-столбцам>Кластеризация по столбцам</a></li></ul></li><li><a href=#задача-предсказания-линейная-регрессия>Задача предсказания, линейная регрессия</a><ul><li><a href=#план-решения-задачи-регрессии>План решения задачи регрессии</a></li><li><a href=#построение-модели-линейной-регрессии>Построение модели линейной регрессии</a></li><li><a href=#проблемы-модели-линейной-регрессии>Проблемы модели линейной регрессии</a></li><li><a href=#полиноминальная-регрессия>Полиноминальная регрессия</a></li></ul></li></ul></nav></aside></main></body></html>