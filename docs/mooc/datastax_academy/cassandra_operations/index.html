<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Configuring clusters: YAML #  cassandra.yaml - the main configuration file
 Cassandra nodes read this file on start-up  restart of the node if needed for the changes to take effect   Located in the following directories:  cassandra package installations: /etc/dse/cassandra cassandra tarball installations: ${install_root}/resources/cassandra/conf    Minimal properties #   cluster_name listen_address native_transport_address (ip address, which use by the clients to connect to the node) seeds  Commonly user YAML settings #    endpoint_snitch"><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="DS210: DataStax Enterprise 6 Operations with Apache Cassandra™"><meta property="og:description" content="Configuring clusters: YAML #  cassandra.yaml - the main configuration file
 Cassandra nodes read this file on start-up  restart of the node if needed for the changes to take effect   Located in the following directories:  cassandra package installations: /etc/dse/cassandra cassandra tarball installations: ${install_root}/resources/cassandra/conf    Minimal properties #   cluster_name listen_address native_transport_address (ip address, which use by the clients to connect to the node) seeds  Commonly user YAML settings #    endpoint_snitch"><meta property="og:type" content="article"><meta property="og:url" content="https://morggoth.github.io/docs/mooc/datastax_academy/cassandra_operations/"><meta property="article:published_time" content="2020-07-06T01:20:35+03:00"><meta property="article:modified_time" content="2020-07-06T01:20:35+03:00"><title>DS210: DataStax Enterprise 6 Operations with Apache Cassandra™ | Morggoth's wiki</title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.6cd8553a6854f4812343f0f0c8baca31271e686434f381fbe3c7226f66639176.css integrity="sha256-bNhVOmhU9IEjQ/DwyLrKMSceaGQ084H748cib2ZjkXY="><script defer src=/en.search.min.cffb2ccd242e6197995677fbb5424994b675f59c584892e3321842a44ef1903d.js integrity="sha256-z/sszSQuYZeZVnf7tUJJlLZ19ZxYSJLjMhhCpE7xkD0="></script></head><body><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><h2 class=book-brand><a href=/><span>Morggoth's wiki</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/misc/kafka/>Kafka basics</a></li><li><a href=/docs/mooc/ class=collapsed>Courses notes</a><ul><li><a href=/docs/mooc/datastax_academy/cassandra_operations/ class=active>DS210: DataStax Enterprise 6 Operations with Apache Cassandra™</a></li><li><a href=/docs/mooc/datastax_academy/casandra_foundations/>DS201: DataStax Enterprise 6 Foundations of Apache Cassandra™</a></li><li><a href=/docs/mooc/foundation_of_kubernetes/>Foundation of Kubernetes</a></li><li><a href=/docs/mooc/math/ class=collapsed>Книга для чтения по высшей математике</a></li><li><a href=/docs/mooc/ml/>Машинное обучение</a></li><li><a href=/docs/mooc/sams_cv/ class=collapsed>Нейронные сети и компьютерное зрение</a></li></ul></li><li><a href=/docs/misc/old_hardware/>Old hardware</a></li><li><a href=/docs/languages/ class=collapsed>Languages</a></li><li><a href=/docs/misc/chromeos/>ChromeOS</a></li><li><a href=/docs/misc/macos/>macOS</a></li><li><a href=/docs/misc/mikrotik/>Mikrotik</a></li><li><a href=/docs/misc/tips_and_tricks/>Tips & Tricks</a></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label>
<strong>DS210: DataStax Enterprise 6 Operations with Apache Cassandra™</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#configuring-clusters-yaml>Configuring clusters: YAML</a><ul><li><a href=#minimal-properties>Minimal properties</a></li><li><a href=#commonly-user-yaml-settings>Commonly user YAML settings</a></li></ul></li><li><a href=#cluster-sizing>Cluster sizing</a></li><li><a href=#cassandra-stress>Cassandra stress</a></li><li><a href=#nodetool-for-performance-analysis>Nodetool for performance analysis</a></li><li><a href=#system-and-output-logs>System and output logs</a></li><li><a href=#jvm-gc-logging>JVM GC logging</a></li><li><a href=#addingremoving-nodes>Adding/removing nodes</a><ul><li><a href=#adding-nodes-best-practices>Adding nodes, best practices</a></li></ul></li><li><a href=#bootstrapping>Bootstrapping</a></li><li><a href=#removing-a-node>Removing a node</a></li><li><a href=#replacing-a-down-node>Replacing a down node</a></li><li><a href=#compaction>Compaction</a><ul><li><a href=#leveled-compaction>Leveled compaction</a></li><li><a href=#size-tiered-compaction>Size tiered compaction</a></li><li><a href=#major-compaction>Major compaction</a></li><li><a href=#time-window-compaction>Time window compaction</a></li></ul></li><li><a href=#repair>Repair</a></li><li><a href=#nodesync>Nodesync</a></li><li><a href=#sstabelesplit>sstabelesplit</a></li><li><a href=#multi-dc-concepts>Multi DC concepts</a></li><li><a href=#cql-copy>CQL copy</a></li><li><a href=#sstabledump>sstabledump</a></li><li><a href=#sstableloader>sstableloader</a></li><li><a href=#dse-dsbulk>DSE DsBulk</a></li><li><a href=#backup>Backup</a></li><li><a href=#jvm-settings>JVM settings</a></li><li><a href=#garbage-collection>Garbage collection</a></li><li><a href=#heap-dump>Heap dump</a></li><li><a href=#tuning-the-kernel>Tuning the kernel</a></li><li><a href=#hardware-selections>Hardware selections</a></li><li><a href=#security-considerations>Security considerations</a><ul><li><a href=#authentication>Authentication</a></li><li><a href=#authorisation>Authorisation</a></li></ul></li><li><a href=#opscenter-and-lifecycle>OpsCenter and Lifecycle</a></li></ul></nav></aside></header><article class=markdown><h2 id=configuring-clusters-yaml>Configuring clusters: YAML
<a class=anchor href=#configuring-clusters-yaml>#</a></h2><p>cassandra.yaml - the main configuration file</p><ul><li>Cassandra nodes read this file on start-up<ul><li>restart of the node if needed for the changes to take effect</li></ul></li><li>Located in the following directories:<ul><li>cassandra package installations: /etc/dse/cassandra</li><li>cassandra tarball installations: ${install_root}/resources/cassandra/conf</li></ul></li></ul><h3 id=minimal-properties>Minimal properties
<a class=anchor href=#minimal-properties>#</a></h3><ul><li>cluster_name</li><li>listen_address</li><li>native_transport_address (ip address, which use by the clients to connect to the node)</li><li>seeds</li></ul><h3 id=commonly-user-yaml-settings>Commonly user YAML settings
<a class=anchor href=#commonly-user-yaml-settings>#</a></h3><ul><li><p>endpoint_snitch</p></li><li><p>initial_token / num_token</p></li><li><p>commitlog_directory</p></li><li><p>data_file_directories</p></li><li><p>hints_directory</p></li><li><p>saved_caches_directory</p></li><li><p>hinted_handoff_enabled</p></li><li><p>max_hint_windows_in_ms</p></li><li><p>row_cache_size_in_mb</p></li><li><p>file_cache_size_in_mb</p></li><li><p>memtable_heap_space_in_mb/memtable_offheap_space_in_mb</p></li></ul><h2 id=cluster-sizing>Cluster sizing
<a class=anchor href=#cluster-sizing>#</a></h2><ul><li>estimates are only a rough-order of magnituge dua to metadata</li><li>things to consider when estimating cluster size:<ul><li>throughput - how much data per second?</li><li>growth rate - how fast does capacity increase?</li><li>latency - how quickly must the cluster respond?</li></ul></li></ul><p>Throughput:</p><ul><li>measure throughput in data movement per time period (e.g. GB/S)</li><li>consider reading and writing separately</li><li>a function of:<ul><li>operation generators (e.g. users)</li><li>rate of operation generations (e.g. 3 clicks per minute)</li><li>size of the operations (number of rows X row width)</li><li>operation mix (read/write ratio)</li></ul></li></ul><p>Growth rate:</p><ul><li>how big must the cluster be just to hold the data?</li><li>given the write throughput, we can calculate growth<ul><li>what is the new/update ratio?</li><li>what is the replication factor?</li><li>additional headroom for operations</li></ul></li></ul><p>Latency:</p><ul><li>calculating cluster capacity is not enough</li><li>understand you SALs<ul><li>in terms of latency</li><li>in terms of throughput</li></ul></li><li>relevant factors:<ul><li>IO rate</li><li>workload shape</li><li>access patterns</li><li>table width</li><li>node profile (i.e. cores, memory, storage, network)</li></ul></li><li>improve estimates with benchmarking</li></ul><h2 id=cassandra-stress>Cassandra stress
<a class=anchor href=#cassandra-stress>#</a></h2><p><em>cassandra-stress</em> = utility fir benchmarking/load testing a cluster</p><ul><li>simulates a user-defined workload</li><li>use the cassandra-stress to:<ul><li>determine schema performance</li><li>understand how your database scales</li><li>optimize your data model and settings</li><li>determine production capacity</li></ul></li><li>try out your database <em>berore</em> you go into production</li></ul><p>You can configure cassandra-stress with yaml file:</p><ul><li>define your schema</li><li>specify any compaction strategy</li><li>create a characteristic workload</li><li>without writing a custom tool</li></ul><p>config sections:</p><ul><li>schema description<ul><li>defines the keyspace and table information</li><li>if the schema is not yet defined the test will create it</li><li>if the schema already exists, only defines the keyspace and table names</li></ul></li><li>column description<ul><li>describes how to generate the data for each column</li><li>the data values are meaningless, but simulate the patterns in terms of size and frequency</li><li>generated values follow a specified distribution such as Uniform, Exponential, Gaussian</li><li>parameters include:<ul><li>data size</li><li>value population</li><li>cluster distribution (the number of values for the column appearing in a single partition (cluster columns only))<ul><li>EXP(min&mldr;max) - an exponential distribution over tha range</li><li>EXTREME(min&mldr;max, shape) - an extreme value distribution over the range</li><li>GAUSSIAN(min&mldr;max, stdvrng) - a gaussian/normal distribution, where mean=(min+max)/2 and stdev is (mean-min)/stdvrng</li><li>GAUSSIAN(min&mldr;max, mean, stdev) - a gaussian/normal distribution. with explicity defined mean and stdev</li><li>UNIFORM(min&mldr;max) - a uniform distribution over the range</li><li>FIXED(val) - a fixed distribution, always returning the same value</li></ul></li></ul></li></ul></li><li>batch description<ul><li>specifies how the test inserts data</li><li>for each insert operation, specifies the following distributions/ratios:<ul><li>partition distribution - number of partitions to update per batch (default FIXED(1))</li><li>select distribution ratio - portion of rows from a partition included in particular batch (default FIXED(1)/1)</li><li>batch type - the type if CQL batch to use; either LOGDEG/UNLOGDEG (default LOGDEG)</li></ul></li></ul></li><li>query description<ul><li>you can specify any CQL query on the table by naming them under the queries section</li><li>fields specifies if the bind variables should be from the same row or across all rows in the partition</li></ul></li></ul><h2 id=nodetool-for-performance-analysis>Nodetool for performance analysis
<a class=anchor href=#nodetool-for-performance-analysis>#</a></h2><p>nodetool sub-commands:</p><ul><li>info</li><li>compactionhistory</li><li>gcstats (gets Java&rsquo;s GC statistics)</li><li>gossipinfo</li><li>ring (gets info about tokens range assignments)</li><li>tablestats</li><li>teblehistograms</li><li>tpstats</li></ul><p>Low GC times are desirable so Cassandra can spend more time servicing requests.</p><h2 id=system-and-output-logs>System and output logs
<a class=anchor href=#system-and-output-logs>#</a></h2><ul><li>by default. the log file is in <em>/var/log/cassandra/system.log</em></li><li>also check <em>debug.log</em> in the same directory</li><li><em>system.log</em> logs INFO messages and above</li><li><em>debug.log</em> logs all messages</li><li>change the location by adding the following line to <em>/etc/dse/cassandra/jvm.options</em>:</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>-Dcassandra.logdir<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>PATH_TO_NEW_LOG_DIR<span style=color:#e6db74>}</span>
</code></pre></div><p>Logging levels:</p><ul><li>OFF</li><li>ERROR</li><li>WARN</li><li>INFO (default)</li><li>DEBUG</li><li>TRACE</li><li>ALL</li></ul><p>Logging configuration:</p><ul><li><em>logback.xml</em> (in the same directory as <em>cassandra.yaml</em>)</li><li><em>nodetool setlogginglevel</em> - sets log level for particular Java class, until node restart</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>nodetool getlogginglevels
</code></pre></div><h2 id=jvm-gc-logging>JVM GC logging
<a class=anchor href=#jvm-gc-logging>#</a></h2><p>Turn on GC logging:</p><ul><li>statically by editing <em>jvm.options</em><ul><li>-XX:+PrintGC - simple, prints a line for every GC and every full GC</li><li>-XX:+PrintGCDetails - detailed, young generation as well as old and perm gen</li><li>-XX:+PrintGCTimeStamps - adds time ti a simple or detailed GC log</li><li>-XX:+PrintGCDateStamps - adds date to a simple or detailed GC log</li></ul></li><li>dinamically by using <em>jinfo</em><ul><li>jinfo -flag +PrintGC ${NODE_PID}</li><li>jinfo -flag +PrintGCTimeStamps ${NODE_PID}</li><li>jinfo -flag +PrintGCDateStamps ${NODE_PID}</li></ul></li></ul><p>In either case, edit <em>/etc/dse/cassandra/jvm.options</em> git specify the GC log file:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>-Xloggc:<span style=color:#e6db74>${</span>PATH_TO_GC_LOG_FILE<span style=color:#e6db74>}</span>
</code></pre></div><h2 id=addingremoving-nodes>Adding/removing nodes
<a class=anchor href=#addingremoving-nodes>#</a></h2><ul><li>reached data capacity problem<ul><li>your data has outgrown the node&rsquo;s hardware capacity</li></ul></li><li>reached traffic problem<ul><li>your application needs more rapid response with less latency</li></ul></li><li>to increase operational headroom<ul><li>need more resources for node repair, compaction, and other resource intensive operations</li></ul></li></ul><h3 id=adding-nodes-best-practices>Adding nodes, best practices
<a class=anchor href=#adding-nodes-best-practices>#</a></h3><ul><li><p>Single-Token nodes</p><ul><li>double the size of your cluster</li></ul></li><li><p>VNodes</p><ul><li>we can add nodes incrementally</li></ul></li><li><p>adding a single node at a time will:</p><ul><li>result in more data movement</li><li>will have a gradual impact on cluster performance</li><li>will take longer to grow cluster</li></ul></li><li><p>adding multiple nodes at the same time:</p><ul><li>is possible</li><li>use extreme caution</li></ul></li></ul><h2 id=bootstrapping>Bootstrapping
<a class=anchor href=#bootstrapping>#</a></h2><p>Bootstrapping is a process of a new node joining a cluster:</p><ul><li>the joining node contacts a seed node</li><li>the seed node communicates cluster info, including token ranges, to the joining node</li><li>cluster nodes prepare to stream necessary SSTables</li><li>cluster nodes stream SSTables to the joining node (can be time consuming)</li><li>existing cluster nodes continue to satisfy writes, but also forward write to joining node</li><li>when streaming is complete, joining node changes to normal state and handles read/write requests</li></ul><p>To bootstrap a node:</p><ul><li>set up the node&rsquo;s configuration files (<em>cassandra.yaml</em>, etc.)<ul><li>four main parameters:<ul><li>cluster_name</li><li>native_transport_address</li><li>listen_address</li><li>-seeds</li></ul></li><li>start up the node normally</li></ul></li></ul><p>Seed node is a just one of the cluster&rsquo;s node.</p><p>When bootstrapping fails, we have two scenarios:</p><ul><li>bootstrapping node could not connect to cluster<ul><li>examine the log file to understand what&rsquo;s going on</li><li>change config and try again</li></ul></li><li>streaming portion fails<ul><li>node exists in cluster in joining state</li><li>first, try restarting the node</li><li>if restarting fails, try deleting data directories and rebooting</li><li>or, worst case, remove the node from the cluster and try again</li></ul></li></ul><p>Node cleanup</p><ul><li>perform cleanup after a bootstrap on the OTHER nodes</li><li>reads all SSTables to make sure there is no token out of range for that particular node</li><li>if the SSTable is not out of range, cleanup just a copy</li><li>there are options for running these operations is parallel</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>bin/nodetool cleanup
</code></pre></div><h2 id=removing-a-node>Removing a node
<a class=anchor href=#removing-a-node>#</a></h2><p>Other nodes need to pickup the removed-node&rsquo;s data</p><p>The cluster needs to know the node is gone.</p><p>Three options for dealing with the data:</p><ul><li>redistribute date from the node that is going away<ul><li>nodetool decomission<ul><li>need to decrease the size of the cluster</li><li>the node must still be active</li><li>decomission will transfer the data from the decomissioned node to other active nodes in the cluster<ul><li>with VNodes, the rebalance happens automatically</li><li>with Single-token nodes, you will need to manually rebalance the tiken ranges on the remaining nodes</li></ul></li><li>after running the nodetool decommossion command:<ul><li>the node is offline</li><li>the JVM process is still running (use dse cassandra-stop to kill the process)</li><li>the data is not deleted from the decommissioned node</li><li>if you want to add the node back to the cluster, delete the data first!!1!<ul><li>node deleting the data may cause data resurrection issues</li></ul></li></ul></li></ul></li></ul></li><li>redistribute the data from replicas<ul><li>nodetool removenode<ul><li>do this if node is offline and never coming back</li><li>you can run this command only from other node</li><li><em>nodetool removenode</em> will:<ul><li>make the remaining nodes in the cluster aware that the node is gone</li><li>copy data from online nodes ti the appropriate replicas to satysfy the replication factor</li></ul></li></ul></li></ul></li><li>don&rsquo;t redistribute the data, just make the node go away<ul><li>nodetool assassinate<ul><li>do this as a last resort if the node is offline and never coming back</li><li><em>nodetool assassinate</em> will:<ul><li>make the remaining nodes in the cluster aware that the node in gone</li><li>NOT copy any data</li></ul></li><li>you should use nodetool repair of the remaining nodes to fix the data replication</li></ul></li></ul></li></ul><h2 id=replacing-a-down-node>Replacing a down node
<a class=anchor href=#replacing-a-down-node>#</a></h2><p>Benefits of replacing a downed node:</p><ul><li>you don&rsquo;t have to move the data twice</li><li>backup for a node will work for a replaced node, because same tokens are used to bring replaced node into cluster</li><li>best option is to replace rather than remove and add</li></ul><p>Replacing a downed node using nodetool:</p><ul><li>configure a new node fir the cluster normally with one additional step:<ul><li>in <em>jvm.options</em> add a replace)address JVM option with IP address of the replaced node:</li></ul></li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>-Dcassandra.replace_address<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>DEAD_NODE_IP_ADDRESS<span style=color:#e6db74>}</span>
</code></pre></div><ul><li>once you have configured the node, start the node in the cluster</li><li>monitor the bootstrapping process using <em>nodetool netstats</em></li><li>after the new node if bootstrapperd, you need to remove this option from <em>jvm.options</em> manually</li></ul><p>What if the dewned node was also a seed node?</p><ul><li>make sure the old IP address does not appear in seeds list in <em>cassandra.yaml</em></li><li>also make sure the new IP address is not in the seeds list in <em>cassandra.yaml</em></li><li>perform a rolling restart on all nodes se the nodes are aware of the changes to the seeds list</li><li>start the replacement node using replace_address in the <em>jvm.options</em> file</li><li>once the replacement bode is fully up:<ul><li>renove replace_address from <em>jvm.options</em></li><li>add the replacement node&rsquo;s IP to the seeds lists in all the nodes' <em>cassandra.yaml</em></li></ul></li></ul><h2 id=compaction>Compaction
<a class=anchor href=#compaction>#</a></h2><h3 id=leveled-compaction>Leveled compaction
<a class=anchor href=#leveled-compaction>#</a></h3><ul><li><p>leveled compaction uses a multiplier of 10 per level by default</p></li><li><p>SSTable max size is 160MB (sstable_size_in_mb)</p></li><li><p>SSTable exceed this amount to ensure the last partioion written is complete</p></li><li><p>Leveled compaction is best for read-heavy workload</p><ul><li>occasional writes but high reads</li></ul></li><li><p>each pertition resides in only one SSTable per level (max)</p></li><li><p>generally reads handled by just a few SSTables</p><ul><li>partitions group together in a handful of levels as they compact down</li><li>90% of the data resides in the lowest level (due to 10x rule)</li><li>unless the lowest level is not yet full</li></ul></li><li><p>leveled compaction wastes less disk space</p></li><li><p>obsolete records compact out quickly</p><ul><li>a single partition&rsquo;s records group as they compact down</li><li>updated records merge with older records due to this grouping</li></ul></li></ul><p>Disadvantages:</p><ul><li>IO intensive</li><li>compacts many mode SSTables at once size tiered compaction</li><li>compacts mode frequently than size tierd</li><li>can&rsquo;t ingest data at high insert speeds</li></ul><h3 id=size-tiered-compaction>Size tiered compaction
<a class=anchor href=#size-tiered-compaction>#</a></h3><p>Default compaction type</p><p>Size tiered compaction triggers compaction based on the number of SSTables.</p><ul><li><p>groups similarly sized tables together</p></li><li><p>tiers with less than min_threshold (four) SSTables are not considered for compaction</p></li><li><p>the smaller the SSTables, the &ldquo;thinner&rdquo; the distance between min_threshold and max_threshold</p></li><li><p>SSTables qualifying for more than one tier distribute rabdomly amongst buckets</p></li><li><p>buckets with nore than max_threshold SSTables are trimmed to just that many SSTables</p><ul><li>32 by default</li><li>coldest SSTables dropped</li></ul></li><li><p>Size tiered compaction chooses the hottest tier first to compact</p></li><li><p>SSTable hotness determined by number of reads per second per partition key</p></li><li><p>cassandra compacts several tiers concurrently</p></li><li><p>concurrent_compactors</p><ul><li>default to smaller of number of disks or number of cores, with a minimum of 2 and a maximum of 8 per CPU core
tables concurrently compacting are not considered for new tiers</li></ul></li></ul><p>Triggering a compaction:</p><ul><li>compaction starts every time a MemTable flushes to as SSTable</li><li>MemTable too large, commit log too large or manual flush</li><li>or when the cluster streams SSTable segments to the node<ul><li>Bootstrap, rebuild, repair</li></ul></li><li>Compaction continues until there are no more tiers with at least min_threshold tables in it</li></ul><p>Tombstones</p><ul><li>if no eligible buckets, size tiered compaction compacts a single SSTable</li><li>this eliminates expired tombstones</li><li>the number of expired tombstones must be above 20%</li><li>largest SSTable chosen first</li><li>table must be at least one day old before considered<ul><li>tombstome_compaction_interval</li></ul></li><li>compaction ensures that tombstones DO NOT overlap old records in other SSTables</li></ul><p>Absorbs high write-heavy workloads ny procrastinating compaction as losg as possible</p><p>Other compaction strategies don&rsquo;t handle ingesting data as well as size tiered</p><p>compaction_throughput_mb_per_sec controls the compaction IO load on a node</p><h3 id=major-compaction>Major compaction
<a class=anchor href=#major-compaction>#</a></h3><ul><li>you can issue a major compaction via nodetool</li><li>compacts all SSTables into a singla SSTable</li><li>new monolithic SSTable will qualify for the largest tier</li><li>future updates/deletes will fall into smaller tiers</li><li>data in laegest tier will become obsolete yet still hog a log of disk space</li><li>takes a long tine for changes to propagate up to large tier</li><li>major compactions not recommended</li></ul><h3 id=time-window-compaction>Time window compaction
<a class=anchor href=#time-window-compaction>#</a></h3><p>Built for time series data</p><p>An SSTables spanning two windows simply falls into the second window</p><p>Good practice to aim for 50ish max SSTables on disk:</p><ul><li>20ish for active window</li><li>30ish for all past windows combined</li></ul><p>for example: one month of data would have window of a day</p><p>Tuning:</p><ul><li>expired_sstable_check_frequency_seconds determines how often to check for fully expired (tombstoned) SSTables</li><li>good to tune when using a TTL</li></ul><h2 id=repair>Repair
<a class=anchor href=#repair>#</a></h2><p>This is consistency check across all the node, than examine that all the data is correct.</p><ul><li>Think of repair as synchronizing replicas</li><li>Repair ensures that all replicas have identical copies of given partition</li><li>Repair occurs:<ul><li>if necessaty when detected by reads (e.g. CL=QUORUM)</li><li>randomly with non-quorum reads (table property read_repair_chance or dclocal_read_repair_chance)</li><li>manually using nodetool repair</li></ul></li></ul><p>How does repair work?</p><ul><li>nodes build Merkel trees from partitions to represent how current the data value are</li><li>nodes exchange the Merkel trees</li><li>nodes compare the Merkel trees to identify specific values that need synchronization</li><li>nodes exchange data values and update their data</li></ul><p>Merkel tree</p><ul><li>a binary tree of hash values</li><li>the leaves of the tree represent hashes of the values in the partition</li><li>each tree-nodes is a hash of its children&rsquo;s hash values</li><li>when tree-nodes hashes are the same, the sub-trees are the same</li></ul><p>When to perform a repair:</p><ul><li>if node has been down for a while</li><li>on a regular basis:<ul><li>once every gc_grace_seconds</li><li>mekr sure the repair can complete within the gc_grace_seconds window</li><li>schedule for lower utilization periods</li></ul></li></ul><p>Is repair a lot of work for the node:</p><ul><li>a full repair can be a lot of work</li><li>but there are ways to mitigate the work:<ul><li>primary range repair</li><li>sub-range repair</li></ul></li></ul><p>Primary range repair:</p><ul><li>the primary range is the set of tokens the node is assigned</li><li>repairing only the node&rsquo;s primary range will make sure that data is synchorized for that range</li><li>repairing only the node&rsquo;s primary range will eliminate redundant repairs</li></ul><p>Sub-range repair:</p><ul><li>repairs can consume significant resources depending on how much data is under consideration</li><li>targeting sub-ranges of the table will reduce the amount of work done by a single repair</li></ul><h2 id=nodesync>Nodesync
<a class=anchor href=#nodesync>#</a></h2><p>DSE 6+ replacement for repair.</p><p>Behaves like continues background repair that delivers:</p><ul><li>low overhead</li><li>consistent performance</li><li>easy to use</li></ul><p>How to use:</p><ul><li>create a cluster with at least 2 nodes</li><li>create keyspace with RF >= 2</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=color:#66d9ef>CREATE</span> KEYSPACE MyKeyspace
<span style=color:#66d9ef>WITH</span> replication<span style=color:#f92672>=</span><span style=color:#960050;background-color:#1e0010>{</span><span style=color:#e6db74>&#39;class&#39;</span>: <span style=color:#e6db74>&#39;SimpleStrategy&#39;</span>, <span style=color:#e6db74>&#39;replication_factor&#39;</span>: <span style=color:#ae81ff>2</span><span style=color:#960050;background-color:#1e0010>}</span>; 
</code></pre></div><ul><li>create table within the keyspace with NodeSync enables</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>TABLE</span> MyTable (k int <span style=color:#66d9ef>PRIMARY</span> <span style=color:#66d9ef>KEY</span>)
<span style=color:#66d9ef>WITH</span> nodesync<span style=color:#f92672>=</span><span style=color:#960050;background-color:#1e0010>{</span><span style=color:#e6db74>&#39;enabled&#39;</span>: <span style=color:#e6db74>&#39;true&#39;</span><span style=color:#960050;background-color:#1e0010>}</span>;
</code></pre></div><ul><li>NodeSync will now automatically make sure the table data is synchronized</li></ul><h2 id=sstabelesplit>sstabelesplit
<a class=anchor href=#sstabelesplit>#</a></h2><p>Brakes large SSTable files in a pieces. Before use this tool? you need to stop node.</p><h2 id=multi-dc-concepts>Multi DC concepts
<a class=anchor href=#multi-dc-concepts>#</a></h2><ul><li>node - the virtual or physical host of a single Cassandra instance</li><li>rack - a logiacl grouping os physically related nodes</li><li>DC - a logical grouping of set of racks</li><li>enables geographically aware read and write request routing</li><li>each node belongs to one rack in one DC</li><li>the identity of each node;s rack and DC may be configured in its <em>conf/cassandra-rackdc.properties</em> file</li></ul><p>implementing a multi DC cluster:</p><ul><li>use the NetworkTopologyStrategy rather than SimpleStrategy</li><li>use LOCAL_* consistency level for read/write operations to limit latency</li><li>specify the snitch</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=color:#66d9ef>ALTER</span> KEYSPACE MyKeyspace
<span style=color:#66d9ef>WITH</span> replication<span style=color:#f92672>=</span><span style=color:#960050;background-color:#1e0010>{</span><span style=color:#e6db74>&#39;class&#39;</span>: <span style=color:#e6db74>&#39;NetworkTopologyStrategy&#39;</span>, <span style=color:#e6db74>&#39;DC1&#39;</span>: <span style=color:#ae81ff>1</span>, <span style=color:#e6db74>&#39;DC2&#39;</span>: <span style=color:#ae81ff>2</span><span style=color:#960050;background-color:#1e0010>}</span>;
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>nodetool rebuild -- name_of_existing_data_center
</code></pre></div><h2 id=cql-copy>CQL copy
<a class=anchor href=#cql-copy>#</a></h2><ul><li>cassandra expects every row in the delimited input to contain the same number of columns</li><li>the number of columns in the delimited input is the same as the number of columns in the Cassandra table</li><li>empty data for a column is assumed by default NULL value</li><li>COPY FROM is intended for importing small datasets (a few million rows or less) into Cassandra</li><li>for impoting larger datasets, use DSBulk</li></ul><p>options:</p><ul><li>DELIMITER (default is comma)</li><li>HEADER (default is false)</li><li>CHUNKSIZE - set the size of chunks passed to worker process (default value is 1000)</li><li>SKIPROWS - the number of rows to slip *default value is 0</li></ul><h2 id=sstabledump>sstabledump
<a class=anchor href=#sstabledump>#</a></h2><p>dumps the content of the specified SSTable in the JSON format</p><p>you may wish to flush the table to the disk before dumping its contests</p><h2 id=sstableloader>sstableloader
<a class=anchor href=#sstableloader>#</a></h2><p>provides the ability to:</p><ul><li>bulk load external data into a cluster</li><li>load pre-existing cluster or new cluster</li><li>a cluster with the same number of nodes or a different number of nodes</li><li>a cluster with different replication strategy or partitioner</li></ul><p>it doesn&rsquo;t simply copy the set of SSTables to every node, but transfers the relevant parts of the data to each node
confirming of the replication strategy of the cluster.</p><h2 id=dse-dsbulk>DSE DsBulk
<a class=anchor href=#dse-dsbulk>#</a></h2><p>Moves Cassandra data to/from files in the file system</p><p>Uses both CSV or JSON formats</p><h2 id=backup>Backup
<a class=anchor href=#backup>#</a></h2><p>Cassandra uses spanshots fir backup data, because:</p><ul><li><p>they don;n copy out all the data from DB</p></li><li><p>it&rsquo;s a distributed system; every node has only a portion of the data</p></li><li><p>SSTables are immutable, which makes them easy to back up</p></li><li><p>snapshot create hadrlinks on the file system as opposed to coping data</p><ul><li>this is different than coping actual data files (takes less disk space)</li></ul></li><li><p>therefile very fast</p></li><li><p>represents the state of the data files at a particular point in the time</p></li><li><p>can consist of a single table, single keyspace ot multiple keyspace</p></li></ul><p>incremental backup:</p><ul><li>create a hard link to every SSTable upon flush<ul><li>user must manually delete them after creating a new shapshot</li></ul></li><li>incremental backups are disabled by default (<em>cassandra.yaml</em>, incremetal_backups: true)</li><li>need a snapshot before taking inkremental backup</li><li>snapshot information is stored in a snapshots directory under each table directory</li></ul><p>backups storred per node and contains only data from this node.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>nodetool snapshot
nodetool clearsnapshot
</code></pre></div><h2 id=jvm-settings>JVM settings
<a class=anchor href=#jvm-settings>#</a></h2><p>JVM memory areas:</p><ul><li><p>code</p></li><li><p>stack</p></li><li><p>heap (is where Java programs allocated and deallocated transient memory)</p></li><li><p>GC refers to when the JVM reclaims the deallocated memory in the heap</p></li></ul><p>Settings:</p><ul><li>MAX_HEAP_SIZE (set to max of 8 gb)<ul><li>large heaps can introduce GC pauses that lead to latency</li></ul></li><li>HEAP_NEWSIZE (set to 100MB per core)<ul><li>the larger this is, the longer GC pauses time will be; the shorter it is, the more frequently GC will run</li></ul></li></ul><h2 id=garbage-collection>Garbage collection
<a class=anchor href=#garbage-collection>#</a></h2><p>DSE 6+ keeps one core available for GC and other maintanance activities</p><p>What consider when tuning GC:</p><ul><li>pause time<ul><li>length of time the collector stops the application while it frees up memory</li></ul></li><li>throughput<ul><li>determined by how often the GC runs and pauses the application</li><li>more often the GC runs, the lower throughput</li></ul></li><li>we want to minimize length of pauses as well as frequency of collection</li></ul><p>JVM available memory:</p><ul><li>Permanent generation</li><li>new generation (ParNew)<ul><li>contains:<ul><li>eden</li><li>2 survivor spaces</li></ul></li><li>once eden fills up with new object, JVN trigger a minor GC</li><li>a minor GC stops execution, iterates over the objects in eden, copies any object that are not (yet) garbage to the
active survivor space, and clears eden</li><li>if the minor GC fills up the active survivor space, it performs the same process on the survivor space</li><li>objects that are still active are moved to the other survivor space, and the JVM clears the old survivor space</li><li>it&rsquo;s a stop-the-world algorithm</li><li>fast:<ul><li>findong and removing garbage</li></ul></li><li>slow:<ul><li>moving active objects from eden to survivor space</li><li>moving active objects from survivor spaces to the old gen</li></ul></li></ul></li><li>old generation (CMS)<ul><li>contains objects that have survived long enough to not be collected by a minor GC</li><li>the CSM collector runs then 75% full</li></ul></li></ul><p>Full GC:</p><ul><li>multi-second GC pauses = Major collections happening</li><li>if the old gen fills up before the CMS collector can finish, the application is paused while a full GC runs</li><li>checks everything: new gen, old gen and perm gen</li><li>significant (multi-second) pauses</li></ul><h2 id=heap-dump>Heap dump
<a class=anchor href=#heap-dump>#</a></h2><ul><li>useful when troubleshooting high memory utilization ot OutOfMemoryErrors</li><li>show exactly which objects are consuming most of the heap</li><li>Cassandra starts Java with the option -XX:+HeapDumpOnOutOfMemoryError</li></ul><h2 id=tuning-the-kernel>Tuning the kernel
<a class=anchor href=#tuning-the-kernel>#</a></h2><p>Time sync</p><ul><li>Cassandra nodes identify valid data using timestamps<ul><li>all nodes within a Cassandra cluster need to have synchronized clocks</li></ul></li><li>Time Stamp Counter (TSC) is a simple register within the CPU that counts the number of clock cycles<ul><li>over time, TSC will drift because the clock cycles may vary between CPUs</li></ul></li><li>Network Time Protocol (NTP) is a way to synchronize CPU clocks<ul><li>nodes communicate wirh a hierarchy of tine servers to djust their clock</li><li>adjustments occurs every 1-20 minutes</li></ul></li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># to view current limits</span>
ulimit -a
</code></pre></div><p>Since Cassandra nodes don&rsquo;t need to share resources, these limits are not helpful.
Turn them off globally by editing <em>limits.conf</em>. Limits take effect when you login. For Ubuntu, use <em>root</em> intead of <em>*</em>.</p><ul><li>*-nofile 1048576</li><li>*-memlock unlimited</li><li>*-fsize unlimited</li><li>*-data unlimited</li><li>*-rss unlimited</li><li>*-stack unlimited</li><li>*-cpu unlimited</li><li>*-nproc unlimited</li><li>*-as unlimited</li><li>*-locks unlimited</li><li>*-sigpending unlimited</li><li>*-msgqueue unlimited</li></ul><p>Swap:</p><ul><li>for cassandra, swapping if a very bad event</li><li>you are better having a node go down than limp along swapping</li><li>to thoroughly disable swap:<ul><li>turn off swap for the current kernel process</li><li>remove swap entries from fstab</li><li>change the swappiness setting</li></ul></li><li>you can check the current list of swap devices by:</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>swapon -s
</code></pre></div><ul><li>you can turn off swap without rebooting</li><li>this command will not persist (i.e. will not survive a reboot):</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>swapoff -a
</code></pre></div><ul><li>to look at the current swappiness settings use:</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cat /proc/sys/vm/swappiness
</code></pre></div><ul><li>this lavlue has range of 0-200 (0 is low an 200 is high)</li><li>to make sure your kernell deiables swapping after a reboot, edir <em>/etc/sysctl.conf</em></li><li>change or add a line to set <em>vm.swappiness = 0</em></li><li>use sysctl -p to get the kernel to reload the changes nade to <em>/etc/sysctl.conf</em></li></ul><p>Changing network kernel settings</p><ul><li>net.ipv4.ip_local_port_range = 10000 65535</li><li>net.ipv4.tcp_window_scaling = 1</li><li>net.ipv4.tcp_rmem = 4096 87380 16777216</li><li>net.ipv4.tcp_wmem = 4096 65536 16777216</li><li>net.core.rmem_max = 16777216</li><li>net.core.wmem_max = 16777216</li><li>net.core.netdev_max_backlog = 2500</li><li>net.core.somaxconn = 65000</li></ul><h2 id=hardware-selections>Hardware selections
<a class=anchor href=#hardware-selections>#</a></h2><ul><li>persistent storage type<ul><li>avoid:<ul><li>SAN storage</li><li>NAS diveces</li><li>NFS</li></ul></li><li>Need to use SSD</li></ul></li><li>memory<ul><li>for both bare metal and VMs:<ul><li>prod: 16-64GB; the minimum is 8GB</li><li>dev in non-loading testing environments: no less than 4GB</li></ul></li><li>more memory means:<ul><li>better read performanse due to caching</li><li>memtables hold more recently written data</li></ul></li></ul></li><li>CPU<ul><li>cassandra if highly concurent and uses as many CPU cores as available</li><li>prod:<ul><li>for bare metal: 16-core CPUs are the current price-performance sweet spot</li></ul></li><li>dev:<ul><li>2-4 core CPUs</li></ul></li></ul></li><li>network<ul><li>you should bind your OS interface to separate NetworkInterface Card (NIC)</li><li>recommended bandwidth os 1000 Mbits/s or grater</li><li>native protocols use the native_transport_address</li><li>cassandra&rsquo;s internal storage protocol uses the listen_address</li></ul></li></ul><h2 id=security-considerations>Security considerations
<a class=anchor href=#security-considerations>#</a></h2><h3 id=authentication>Authentication
<a class=anchor href=#authentication>#</a></h3><ul><li>desabled by default</li><li>when enabled, client programs must supply a username and password:</li><li>enable in <em>dse.yaml</em></li></ul><p>Apache Cassandra supports only pluggable authentication mechanisms.</p><p>DseAuthenticator options has three schemes:</p><ul><li>internal<ul><li>need to restart the node(s)</li><li>loggin as <em>cassandra</em> with password <em>cassandra</em></li><li>the <em>cassandra</em> user is a superuser - has all permissions:<ul><li>change the default <em>cassandra</em> password</li><li>Cassandra stores the credentials in the <em>sys_auth</em> keyspace, so lossing data here could be disastrous</li></ul></li></ul></li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=color:#66d9ef>ALTER</span> <span style=color:#66d9ef>USER</span> cassandra <span style=color:#66d9ef>WITH</span> PASSWORD <span style=color:#e6db74>&#39;new_pass&#39;</span>;

<span style=color:#66d9ef>ALTER</span> KEYSPACE <span style=color:#e6db74>&#34;system_auth&#34;</span>
<span style=color:#66d9ef>WITH</span> REPLICATION <span style=color:#f92672>=</span> <span style=color:#960050;background-color:#1e0010>{</span><span style=color:#e6db74>&#39;class&#39;</span>: ;NetworkTopologyStrategy<span style=color:#e6db74>&#39;, &#39;</span>dc1<span style=color:#e6db74>&#39;: 2};
</span></code></pre></div><ul><li>LDAP</li><li>Kerberos</li></ul><p>Cassandra users:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>TABLE</span> system_auth.roles (
  <span style=color:#66d9ef>role</span> text <span style=color:#66d9ef>PRIMARY</span> <span style=color:#66d9ef>KEY</span>,
  can_login boolen.
  is_superuser boolen,
  member_os <span style=color:#66d9ef>set</span><span style=color:#f92672>&lt;</span>text<span style=color:#f92672>&gt;</span>,
  salted_hash text
)
</code></pre></div><p>Role operations:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>ROLE</span> SomeRole
<span style=color:#66d9ef>WITH</span> PASSWORD <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;some_pass&#39;</span>
<span style=color:#66d9ef>AND</span> LOGIN <span style=color:#f92672>=</span> <span style=color:#66d9ef>true</span>;

LIST ROLES;

<span style=color:#66d9ef>DROP</span> <span style=color:#66d9ef>ROLE</span> SomeRole;
</code></pre></div><p>Auth best practices:</p><ul><li>create a second superuser role</li><li>change the default <em>cassandra</em> password and forget it</li><li>be sure to replicate <em>system_auth</em> keyspace</li></ul><h3 id=authorisation>Authorisation
<a class=anchor href=#authorisation>#</a></h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=color:#66d9ef>GRANT</span> <span style=color:#66d9ef>SELECT</span> <span style=color:#66d9ef>ON</span> someKeyspace.someTable <span style=color:#66d9ef>to</span> somerole;
</code></pre></div><p>Permissions:</p><ul><li>ALTER<ul><li>A. KEYSPACE</li><li>A. TABLE</li><li>CREATE INDEX</li><li>DROP INDEX</li></ul></li><li>AUTHORIZE<ul><li>GRANT</li><li>REVOKE</li></ul></li><li>CREATE<ul><li>C. KEYSPACE</li><li>C. TABLE</li></ul></li><li>DROP<ul><li>D. KEYSPACE</li><li>D. TABLE</li></ul></li><li>MODIFY<ul><li>INSERT</li><li>DELETE</li><li>UPDATE</li><li>TRUNCATE</li></ul></li><li>SELECT<ul><li>SELECT</li></ul></li></ul><h2 id=opscenter-and-lifecycle>OpsCenter and Lifecycle
<a class=anchor href=#opscenter-and-lifecycle>#</a></h2><p>WebUI for DSE</p><p>Life Cycle manager (LCM) - mostly configuration and deployment
OpsCenter Monitoring - monitoring and management</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#configuring-clusters-yaml>Configuring clusters: YAML</a><ul><li><a href=#minimal-properties>Minimal properties</a></li><li><a href=#commonly-user-yaml-settings>Commonly user YAML settings</a></li></ul></li><li><a href=#cluster-sizing>Cluster sizing</a></li><li><a href=#cassandra-stress>Cassandra stress</a></li><li><a href=#nodetool-for-performance-analysis>Nodetool for performance analysis</a></li><li><a href=#system-and-output-logs>System and output logs</a></li><li><a href=#jvm-gc-logging>JVM GC logging</a></li><li><a href=#addingremoving-nodes>Adding/removing nodes</a><ul><li><a href=#adding-nodes-best-practices>Adding nodes, best practices</a></li></ul></li><li><a href=#bootstrapping>Bootstrapping</a></li><li><a href=#removing-a-node>Removing a node</a></li><li><a href=#replacing-a-down-node>Replacing a down node</a></li><li><a href=#compaction>Compaction</a><ul><li><a href=#leveled-compaction>Leveled compaction</a></li><li><a href=#size-tiered-compaction>Size tiered compaction</a></li><li><a href=#major-compaction>Major compaction</a></li><li><a href=#time-window-compaction>Time window compaction</a></li></ul></li><li><a href=#repair>Repair</a></li><li><a href=#nodesync>Nodesync</a></li><li><a href=#sstabelesplit>sstabelesplit</a></li><li><a href=#multi-dc-concepts>Multi DC concepts</a></li><li><a href=#cql-copy>CQL copy</a></li><li><a href=#sstabledump>sstabledump</a></li><li><a href=#sstableloader>sstableloader</a></li><li><a href=#dse-dsbulk>DSE DsBulk</a></li><li><a href=#backup>Backup</a></li><li><a href=#jvm-settings>JVM settings</a></li><li><a href=#garbage-collection>Garbage collection</a></li><li><a href=#heap-dump>Heap dump</a></li><li><a href=#tuning-the-kernel>Tuning the kernel</a></li><li><a href=#hardware-selections>Hardware selections</a></li><li><a href=#security-considerations>Security considerations</a><ul><li><a href=#authentication>Authentication</a></li><li><a href=#authorisation>Authorisation</a></li></ul></li><li><a href=#opscenter-and-lifecycle>OpsCenter and Lifecycle</a></li></ul></nav></aside></main></body></html>