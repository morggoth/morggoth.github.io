<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Foundation of Kubernetes"><meta property="og:title" content="Foundation of Kubernetes"><meta property="og:description" content="https://github.com/amorozov87/kubernetes-traning
Kubernetes quick start K8S - opensource система оркестрации контейнеров
Основная задача - распределять (по нодам) и менеджить контейнеры с приложениями
Предоставляет:
 service discovery load balancing autoscaling (как приложения, так и самого кластера) HA декларативный механизм обновлений полезной нагрузки  Контроллер - демон, следящий за состоянием некоторого объекта, например приложения, которое задеплоили
Conceptions  nodes - узлы кластера  master - нода, на которой задеплоен control plane worker - нода с бизнес приложениями   namespace - виртуальное кластерное пространство внутри одного кластера; нужен для логического разделения задеплоенных приложений pods - базовая сущность кластера, абстракция над одним или несколькими контейнерами controllers  controller manager - основной контроллер кластера, входящий в control plane operator - кастомный контроллер; пишется самостоятельно   labels volumes jobs - одноразово запущенный контейнер, который будет рестартоваться только если процесс завершился с ненулевым кодом выхода (обычный контейнер будет рестартоваться при любом завершении процесса) kubectl - утилита управления кластером  Кластерная роль затрагивает весь кластер."><meta property="og:type" content="article"><meta property="og:url" content="https://morggoth.github.io/docs/misc/foundation_of_kubernetes/"><meta property="article:published_time" content="2020-06-18T01:33:53+03:00"><meta property="article:modified_time" content="2020-06-18T01:33:53+03:00"><title>Foundation of Kubernetes | Morggoth's wiki</title><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.daeb3603c687f9ac8113081371aff99b3a3a6406ce59d6f8e0a2d0e545b7bd24.css integrity="sha256-2us2A8aH+ayBEwgTca/5mzo6ZAbOWdb44KLQ5UW3vSQ="><script defer src=/en.search.min.42ddc3059a32b4e42346ba92bb3b2857cef45b086cc1cc70d4602d4e24443ab8.js integrity="sha256-Qt3DBZoytOQjRrqSuzsoV870Wwhswcxw1GAtTiREOrg="></script></head><body><input type=checkbox class=hidden id=menu-control><main class="flex container"><aside class="book-menu fixed"><nav><h2 class=book-brand><a href=https://morggoth.github.io><span>Morggoth's wiki</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64><div class="book-search-spinner spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/misc/datastax_academy/cassandra_operations/>DS210: DataStax Enterprise 6 Operations with Apache Cassandra™</a></li><li><a href=/docs/misc/datastax_academy/casandra_foundations/>DS201: DataStax Enterprise 6 Foundations of Apache Cassandra™</a></li><li><a href=/docs/misc/foundation_of_kubernetes/ class=active>Foundation of Kubernetes</a></li><li><a href=/docs/misc/old_hardware/>Old hardware</a></li><li><a href=/docs/languages/scala/>Scala</a></li><li><a href=/docs/languages/c_sharp/>C#</a></li><li><a href=/docs/misc/math/>Книга для чтения по высшей математике</a></li><li><a href=/docs/languages/c/>C</a></li><li><a href=/docs/misc/chromeos/>ChromeOS</a></li><li><a href=/docs/languages/java/>Java</a></li><li><a href=/docs/misc/macos/>macOS</a></li><li><a href=/docs/misc/mikrotik/>Mikrotik</a></li><li><a href=/docs/misc/tips_and_tricks/>Tips & Tricks</a></li><li><a href=/docs/mooc/>MOOC</a></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class="flex align-center justify-between book-header"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Foundation of Kubernetes</strong></header><article class=markdown><p><a href=https://github.com/amorozov87/kubernetes-traning>https://github.com/amorozov87/kubernetes-traning</a></p><h2 id=kubernetes-quick-start>Kubernetes quick start</h2><p>K8S - opensource система оркестрации контейнеров</p><p>Основная задача - распределять (по нодам) и менеджить контейнеры с приложениями</p><p>Предоставляет:</p><ul><li>service discovery</li><li>load balancing</li><li>autoscaling (как приложения, так и самого кластера)</li><li>HA</li><li>декларативный механизм обновлений полезной нагрузки</li></ul><p>Контроллер - демон, следящий за состоянием некоторого объекта, например приложения, которое задеплоили</p><h3 id=conceptions>Conceptions</h3><ul><li>nodes - узлы кластера<ul><li>master - нода, на которой задеплоен control plane</li><li>worker - нода с бизнес приложениями</li></ul></li><li>namespace - виртуальное кластерное пространство внутри одного кластера; нужен для логического разделения задеплоенных приложений</li><li>pods - базовая сущность кластера, абстракция над одним или несколькими контейнерами</li><li>controllers<ul><li>controller manager - основной контроллер кластера, входящий в control plane</li><li>operator - кастомный контроллер; пишется самостоятельно</li></ul></li><li>labels</li><li>volumes</li><li>jobs - одноразово запущенный контейнер, который будет рестартоваться только если процесс завершился с ненулевым кодом выхода (обычный контейнер будет рестартоваться при любом завершении процесса)</li><li>kubectl - утилита управления кластером</li></ul><p>Кластерная роль затрагивает весь кластер.
Простая роль доступна только в том неймспейсе, в котором она создана.</p><p>Роль даёт права на выполнение каких-либо действий (внутри неймспейса)</p><p>Правили роли:</p><ul><li>verbs - действия, которые мы можем выполнять</li><li>apiGroups - каждая сущность кластера имеет собственное API (пустое значение означает, все существующие группы)</li><li>resources - ресурсы кластера, к которым можно применять перечисленные действия</li></ul><p>Роли могут наследовать правила</p><h3 id=kubernetes-cluster-architecture>Kubernetes cluster architecture</h3><ul><li>master - содержит весь control plane<ul><li>etcd - KV db, содержит весь стейт кластера; работает по принципу кворума, поэтому должно быть нечетное количество экземпляров</li><li>API server - центральное звено, все запросы идет через него; для настройки HA требуется внешний балансер, так как kubelet не может смотреть в несколько API серверов</li><li>Scheduler - отвечает за распределение подов по кластеру; *</li><li>Controller Manager - следит за работой компонентов control plane; *</li></ul></li><li>worker node<ul><li>Container engine, например Docker</li><li>Kubelet - агент, общается с API сервером и управляет подами на локальной ноде; ответственен за то, чтобы состояние рабочей нагрузки соответствовало тому, что указано для нее на API сервере</li><li>Kubernetes proxy - отвечает за сетевое взаимодействие внутри кластера</li></ul></li></ul><p>* - умеют самостоятельно выбирать лидера в HA-режиме</p><p>Основная сетевая концепция Kubernetes - любой под должен быть доступен для любого другого пода напрямую, без участия NAT`а</p><h2 id=deployment-exposed>Deployment exposed</h2><h3 id=namespace>Namespace</h3><p>Создавался для разграничение энвайроментов с большим количеством пользователей.</p><p>Ресурсы уникальны в рамках неймспейса.</p><p>Обладает своими квотами.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl completion -h                   <span style=color:#75715e># добавление автодополнения</span>

kubectl get ns                          <span style=color:#75715e># список неймспейсов</span>
kubectl create ns $NS_NAME              <span style=color:#75715e># создание неймспейса</span>

kubectl edit clusterrole $ROLE_NAME     <span style=color:#75715e># редактирование роли</span>
</code></pre></div><h3 id=deployment>Deployment</h3><p>Сущность, которая отвечает за абстракцию над задеплоеным приложением</p><p>Предоставляет декларативные механизмы апдейтов для подов и replicaSet`ов</p><p>меняет актуальное состояние на декларированное</p><p>использование дейлойментов - best practice, так как при создании отдельных подов, отсутствует автоматический механизм контроля за его состоянием</p><p>при обновлении дейплоймента создается новый replicaSet и появляется возможность быстро откатиться к предыдущему состоянию</p><p>Deployment workflow:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>YAML<span style=color:#ae81ff>\J</span>SON Template -&gt; Deployment -&gt; ReplicaSet -&gt; Pod -&gt; Containers
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl create deployment deploymentName --image<span style=color:#f92672>=</span>imageName
kubectl delete deployment deploymentName
</code></pre></div><h3 id=deployment-use-cases>Deployment use cases</h3><ul><li>развернуть replicaSet</li><li>задекларировать новое состояние для подов</li><li>откатиться на более раннюю версию деплоймента</li><li>масштабировать дейлоймент</li><li>приостановить деплоймент для применения множественных фиксов (вероятно не применяется на практике)</li><li>просмотр статуса деплоймента</li></ul><h3 id=pods-and-containers>Pods and Containers</h3><ul><li><p>базовый &ldquo;строительный блок&rdquo; кластера</p></li><li><p>абстракция над одним или несколькими контейнерами</p></li><li><p>всегда имеет уникальный в пределах кластера IP-адрес</p></li><li><p>контейнеры в рамках одного пода:</p><ul><li>всегда расположены на одной ноде</li><li>имеют единый сетевой неймспейс, соответственно всегда общаются напрямую друг с другом</li></ul></li></ul><h4 id=init-containers>Init containers</h4><p>Специализированный контейнер, который стартует перед контейнером с приложением</p><ul><li>могут содержать и запускать дополнительные утилиты, которые не желательно включать в основной контейнер с приложением
(например по соображениям безопасности)</li><li>предоставляют механизм контролируемой задержки старта основного контейнера</li></ul><h3 id=services-discovery>Services (discovery)</h3><p>Это абстракция, которая определяет логический набор подов и политики доступа к ним. Поды определяются по лейблам.</p><p>Типы сервисов:</p><ul><li>Normal - имя сервиса резолвится в IP кластера</li><li>Headless - резолвится напрямую в IP-адрес пода</li></ul><p>Пример сервиса:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>kind: Service
apiVersion: v1
metadata:
  name: nginx-service
spec:
  clusterIP: None     <span style=color:#75715e># для создания headless сервиса</span>
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: <span style=color:#ae81ff>80</span>
      targetPort: <span style=color:#ae81ff>80</span>
</code></pre></div><p>В качестве протоколов поддерживаются TCP (по умолчанию) и UDP</p><p>Сервис без селектора нужен для того чтобы K8S мог ссылаться на внешние объекты. Примеры использования:</p><ul><li>использование внешней БД в проде и внутренней в тесте</li><li>использование сервиса в другом неймспейсе</li><li>миграция нагрузки с K8S на сторонние бакенды</li></ul><p>Для такого сервиса EndPoint не будет создан автоматически.</p><p>Способы открыть доступ к сервису:</p><ul><li>ClusterIP<ul><li>открывает доступ по внутреннему кластерному IP</li><li>соответственно, в этом случае сервис доступен только внутри кластера</li><li>дефолтный тип</li></ul></li><li>NodePort<ul><li>открывает доступ к сервису по адресу ноды на статическом порте</li><li>с ним можно общаться снаружи</li><li>${NodeIP}:${NodePort}</li></ul></li><li>LoadBalancer<ul><li>открывает доступ к сервису снаружи с использованием LA, предоставляемого облачным провайдером</li></ul></li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl patch...        <span style=color:#75715e># обновление ресурсов</span>
</code></pre></div><h3 id=ingress>Ingress</h3><ul><li>Предоставляет доступный извне URL</li><li>Балансирует трафик (RR)</li><li>Терминирует SSL-соединение</li><li>предоставляет основанный на именах виртуальный хостинг</li><li>требует наличия Ingress-контроллера (одна из имплементаций - NGINX)</li></ul><p>Типы ингрессов:</p><ul><li>Single service ingress</li><li>Simple fanout</li><li>Name based virtual hosting</li></ul><h2 id=running-you-app>Running you app</h2><h3 id=dry-run>Dry run</h3><p>Опция kubectl, которая только печатает объект, который должен был бы быть отправлен.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl create deployment deploymentName --image<span style=color:#f92672>=</span>imageName --dry-run<span style=color:#f92672>=</span>true
kubectl create deployment deploymentName --image<span style=color:#f92672>=</span>imageName --dry-run<span style=color:#f92672>=</span>true -o yaml &gt; deployment.yaml <span style=color:#75715e># пример создания файла деплоймента</span>
</code></pre></div><h3 id=using-env-vars>Using env vars</h3><p>При создании пода можно установить переменные окружения, которые будут переданы в контейнер.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  containers:
    ...
    env:
    - name: DEMO_KUBERNETES
      value: <span style=color:#e6db74>&#34;Hello from k8s&#34;</span>
</code></pre></div><p>Также можно передавать в переменные окружения информацию о самом контейнере:</p><ul><li>metadata.name</li><li>metadata.namespace</li><li>metadata.labels</li><li>metadata.annotations</li><li>spec.nodeName</li><li>spec.serviceAccountName</li><li>status.hostIP</li><li>status.podIP</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  containers:
    ...
    env:
    - name: MY_NODE_NAME
      valueFrom:
        fieldPath:
          fieldPath: metadata.name
</code></pre></div><h3 id=commands-and-arguments>Commands and arguments</h3><p>Аналог ENTRYPOINT и CMD инструкций Docker. Задаются в полях command и args конфига. Не могут быть изменены после того,
как Под был создан.</p><p>Перетирают дефолтные команды и аргументы, которые указаны при сборке образа.</p><p>Если указаны только аргументы, дефолтная команда из образа будет использована с новыми аргументами.</p><table><thead><tr><th>Description</th><th>Docker field</th><th>Kubernetes field</th></tr></thead><tbody><tr><td>Команда, которая будет запущена контейнером</td><td>Entrypoint</td><td>command</td></tr><tr><td>Аргументы, которые будут переданы команде</td><td>Cmd</td><td>args</td></tr></tbody></table><table><thead><tr><th>Image Entrypoint</th><th>Image Cmd</th><th>Container command</th><th>Container args</th><th>Command run</th></tr></thead><tbody><tr><td>[/ep1]</td><td>[foo bar]</td><td></td><td></td><td>[ep-1 foo bar]</td></tr><tr><td>[/ep1]</td><td>[foo bar]</td><td>[/ep-2]</td><td></td><td>[ep-2]</td></tr><tr><td>[/ep1]</td><td>[foo bar]</td><td></td><td>[zoo boo]</td><td>[ep-1 zoo boo]</td></tr><tr><td>[/ep1]</td><td>[foo bar]</td><td>[/ep-2]</td><td>[zoo boo]</td><td>[ep-2 foo bar]</td></tr></tbody></table><h3 id=scheduling>Scheduling</h3><p>Механизмы распределения Подов:</p><ul><li>все механизмы используют лейблы</li><li>Node Selector (позволяет указать, что деплоймент должен быть размещен на ноде с определенным лейблом; устаревший)<ul><li>можно объединять несколько лейблов</li></ul></li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  nodeSelector:
    labelName: labelValue
</code></pre></div><ul><li>Node affinity and anti-affinity (похож на первый, но более гибкий)<ul><li>позволяет операции над лейблами:<ul><li>And</li><li>In</li><li>NotIn</li><li>Exists</li><li>DoesNotExists</li><li>Gt</li><li>Lt</li></ul></li><li>правила бывают:<ul><li>soft/preferred - предпочтение отдаётся нодам с указанными лейблами, но если они недоступны, под будет задеплоен на любую доступную ноду</li><li>hard/required - требуется полное соответствие лейблам</li></ul></li><li>все правила применяются только в момент деплоя пода, позднейшие изменения не будут иметь эффекта, пока под не будет передеплоен</li><li>под может быть задеплоен на ноду, если одно из условий nodeSelectorTerms удовлетворено</li><li>&ndash;--, если все условия matchExpressions были удовлетворены</li></ul></li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  ...
  spec:
    affinity:
      requiredDuringSchedulingInnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: lableName
            operator: In
            values:
            - labelValue
</code></pre></div><ul><li>inter-pod affinity and inti-affinity<ul><li>аналогично, но с подами, а не с нодами</li><li>вариант применения - исключить деплой реплик приложения на одну ноду</li></ul></li><li>Taints<ul><li>имеет:<ul><li>ключ</li><li>значение</li><li>эффект<ul><li>NoSchedule</li><li>PreferNoSchedule</li><li>NoExecute</li></ul></li></ul></li></ul></li><li>Tolerations<ul><li>имеет:</li><li>ключ</li><li>значение</li><li>оператор<ul><li>Exists</li><li>Equal</li></ul></li><li>эффект</li></ul></li></ul><h3 id=image-pull-policies>Image pull policies</h3><ul><li>ifNotPresent(DEFAULT) - скачивает образ, если его нет в локальном кеше той ноды, куда приезжает приложение</li><li>Always(:LATEST)</li><li>Never</li></ul><h3 id=application-restarts>Application restarts</h3><ul><li>Always (default)</li><li>OnFailure</li><li>Never</li></ul><p>Применяется ко всем контейнерам в поде. Поды рестартуют с экспоненциальной дажержкой (10с, 20с, 40с&mldr;)</p><h3 id=application-logs>Application logs</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl logs <span style=color:#e6db74>${</span>podName<span style=color:#e6db74>}</span>

--<span style=color:#ae81ff>\\</span>-- --previous       <span style=color:#75715e># следует использовать в случае, если контейнер завалился</span>
</code></pre></div><p>Если под имеет несколько контейнеров, то нужно указывать имя конкретного контейнера.</p><p>Если контейнер рестартует, k8s сохраняет один остановленный контейнер с логами.</p><p>Если под выселяется с ноды, то все относящиеся к нему контейнеры выселяются вместе с логами.</p><p>В логи пишется только stdout и stderr PID1</p><h2 id=application-scaling>Application scaling</h2><h3 id=replica-sets>Replica sets</h3><p>Может использовать независимо от других сущностей</p><p>Но рекомендуется использовать деплойменты, так как в этом случае, не нужно следить за жизнью контейнера</p><p>Описывает конкретное число подов с приложением, которое должно существовать в конкретный момент времени</p><h3 id=scaling>Scaling</h3><ul><li>по дефолту стартует один под</li><li>можно скейлить до нуля (выключение)</li><li>пропорциональное обновление</li></ul><h3 id=autoscaling>Autoscaling</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl sutoscale deployment podName
</code></pre></div><ul><li>предствален в виде отдельно ресурса и контроллера</li><li>дефолтное значение задержки 30 сек</li><li>собирает как ресурсные метрики, так и кастомные</li><li>заданный процент потреблённых ресурсов высчитывается от requests (см. ниже); если это значение не задано, работать не будет</li></ul><h3 id=compute-resources-managing>Compute resources managing</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>containers:
  ...
  resources:
    requests:
      memory: <span style=color:#e6db74>&#34;64Mi&#34;</span>
      cpu: <span style=color:#e6db74>&#34;250m&#34;</span>
    limits:
      memory: <span style=color:#e6db74>&#34;128Mi&#34;</span>
      cpu: <span style=color:#e6db74>&#34;500m&#34;</span>
</code></pre></div><h2 id=updating-an-application>Updating an application</h2><h3 id=deployment-update>Deployment update</h3><ul><li>Kubectl set image</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl run nginx --image<span style=color:#f92672>=</span>nginx:1.12 --replicas<span style=color:#f92672>=</span>3
kubectl rollout status deploy nginx
kubectl set image deploy nginx nginx<span style=color:#f92672>=</span>nginx:1.13
</code></pre></div><ul><li>Kubectl edit</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl edit deploy nginx
kubectl rollout status nginx
</code></pre></div><ul><li>Kubectl apply</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get deploy nginx -o yaml &gt; nginx_deploymetn.yaml
kubectl apply -f nginx_deployment.yaml
kubectl rollout status deploy nginx
</code></pre></div><ul><li>У k8s есть политика, говорящая сколько подов может быть недоступно в ходе апдейта</li><li>&ndash;\&ndash;, может быть создано сверх желаемого значение</li><li>раскатка деплоймента стриггерится только, если поменялся сам шаблон (изменение метадаты к этому не приводит)</li><li>каждый раз создается новый ReplicaSet</li></ul><h3 id=deployment-rollouts-and-rollbacks>Deployment rollouts and rollbacks</h3><ul><li>Rollout</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl apply -f nginx_deployment.yaml --record
kubectl rollout status deploy nginx
kubectl get deployments
</code></pre></div><ul><li>Update</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl set image deploy nginx nginx<span style=color:#f92672>=</span>nginx:1.13 --record<span style=color:#f92672>=</span>true
kubectl rollout status deploy nginx
kubectl get pods
</code></pre></div><ul><li>Rollback</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl rollout history deploy nginx
kubectl history deploy nginx --revision<span style=color:#f92672>=</span>2
kubectl rollout ubdo deploy nginx
</code></pre></div><h3 id=the-deployment-lifecycle>The deployment lifecycle</h3><ul><li>Pregressing<ul><li>создается новый ReplicaSet</li><li>поднимается новый ReplicaSet</li><li>тушится старый ReplicaSet</li></ul></li><li>Complete<ul><li>все реплики обновлены до последней версии</li><li>все реплики доступны</li><li>ни одна старая реплика не запущена</li></ul></li><li>Failed<ul><li>insufficient quota</li><li>insufficient permissions</li><li>readiness probe failures</li><li>image pull errors</li><li>limit ranges</li><li>application runtime misconfiguration</li></ul></li></ul><h2 id=dealing-with-storage>Dealing with storage</h2><h3 id=empty-dir>Empty dir</h3><ul><li>создается, когда под добавляется на ноду</li><li>существует, пока под запущен на ноде</li><li>изначально пустой</li><li>может быть смонтирован по тому же или отличающемуся пути</li><li>когда под (по любой причине) удаляется с ноды, данные их emptyDir удалятся безвозвратно</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  containers:
  ...
  volumeMounts:
  - mountPath: /cashe
    name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}
</code></pre></div><h3 id=host-path>Host path</h3><ul><li>монтирует директорию с хостовой машины</li><li>может вести себя по разному на различных нодах (поэтому не рекомендуется в проде)</li><li>нужны привелегии для использования</li></ul><h3 id=git-repo>Git repo</h3><ul><li>монтирует пустую директорию и клинирует в нее репу</li></ul><h3 id=persistent-volumes>Persistent volumes</h3><ul><li>абстракция над работой с хранилищем</li><li>может быть запровиженен как автоматически, так и администратором</li><li>PersistentVolumeClaim (PVC) - запрос на хранилище со стороны пользователя</li><li>StorageClass - механизм автоматического привиженинга</li></ul><h2 id=application-configuration>Application configuration</h2><h3 id=configmap>Configmap</h3><p>Объект k8s, предоставляющий механизм хранения KV данных.</p><p>Может быть использована как:</p><ul><li>аргумент командной строки</li><li>переменна окружения</li><li>файл в волюме</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl create configmap nginx-config --from-file<span style=color:#f92672>=</span>/path/to/dir <span style=color:#75715e># k - имя файла, v - содержимое файла</span>

kubectl create configmap nginx-config --from-file<span style=color:#f92672>=</span>/path/to/file <span style=color:#75715e># k - имя файла, v - содержимое файла</span>
kubectl create configmap nginx-config --from-file<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>myKeyName<span style=color:#e6db74>}</span><span style=color:#f92672>=</span>/path/to/file

kubectl create configmap nginx-config --from-literal<span style=color:#f92672>=</span>someKey<span style=color:#f92672>=</span>someValue
</code></pre></div><p>Директивы &ndash;from-file и &ndash;from-literal можно совмещать.</p><p>Использование:</p><ul><li>env var:<ul><li>from a single ConfigMap</li><li>from multiple ConfigMaps</li></ul></li><li>in pod commands:<ul><li>echo ${KEY_NAME} (правда нужно предварительно создать переменную окружения)</li></ul></li><li>volume:<ul><li>монтирование тома с данными, сохраненными в ConfigMap</li><li>добавление ключей ConfigMap в конкретный путь на томе</li></ul></li></ul><h3 id=secrets>Secrets</h3><p>Аналогично, но хранится в зашифрованном виде</p><p>Ограничения:</p><ul><li>должны быть созданы до пода</li><li>должны быть созжаны в том же неймспейсе</li><li>каждый секрет не может быть больше 1 мб</li></ul><h2 id=jobs-and-daemons>Jobs and daemons</h2><h3 id=jobs>Jobs</h3><ul><li>отдельный ресурс со своим контроллером</li><li>создает один или несколько подов, обеспечивая успешное выполнение указанной в образе команды</li><li>как только задача успешно завершается, удаляются все поды, относящиеся к ней</li><li>если задача завершается с ненулевым кодом ответа, под рестартуется (с экспоненциальной задержкой)</li><li>поды могут выполнять задачу параллельно</li></ul><h3 id=daemons-sets>Daemons sets</h3><ul><li>обеспечивают запуск одному экземпляру пода на каждой (или некоторых) нодах</li><li>если в кластер будет добавлена новая нода, приложение приедет и на нее (с учетом всех ограничений шедулинга)</li><li>удаление Daemon set приведет к удалению всех подов</li></ul><p>Примеры использования:</p><ul><li>storage daemon (glusterfs, ceph, etc)</li><li>агенты агрегатора логов</li><li>мониторинг агенты</li></ul><h2 id=stateful-applications>Stateful applications</h2><ul><li>StatefulSet - разрабатывался для обслуживания stateful приложений и распределенных систем</li><li>предоставляет гарантии очередности старта подов</li><li>каждый под представляет из себя уникальный объект</li><li>спецификации подов такие же как в деплойменте, но поды не взаимозаменяемые</li><li>каждый под имеет собственное не шареное хранилище</li><li>каожый под имеет уникальное имя вида ${StatefulSetName}-${Ordinal}</li></ul><p>Ограничения:</p><ul><li>удаление не удаляет хранилища</li><li>требует Headless сервис, так как обычно распределенные приложения плохо работают с прокси</li></ul><p>Политики управления подами:</p><ul><li>OrderedReady (default)</li><li>Parallel</li></ul><p>Стратегии обновления:</p><ul><li>On Delete</li><li>Rolling Updates</li><li>Partitions</li></ul></article><div class="book-footer justify-between"></div></div><aside class="book-toc levels-6 fixed"><nav id=TableOfContents><ul><li><a href=#kubernetes-quick-start>Kubernetes quick start</a><ul><li><a href=#conceptions>Conceptions</a></li><li><a href=#kubernetes-cluster-architecture>Kubernetes cluster architecture</a></li></ul></li><li><a href=#deployment-exposed>Deployment exposed</a><ul><li><a href=#namespace>Namespace</a></li><li><a href=#deployment>Deployment</a></li><li><a href=#deployment-use-cases>Deployment use cases</a></li><li><a href=#pods-and-containers>Pods and Containers</a></li><li><a href=#services-discovery>Services (discovery)</a></li><li><a href=#ingress>Ingress</a></li></ul></li><li><a href=#running-you-app>Running you app</a><ul><li><a href=#dry-run>Dry run</a></li><li><a href=#using-env-vars>Using env vars</a></li><li><a href=#commands-and-arguments>Commands and arguments</a></li><li><a href=#scheduling>Scheduling</a></li><li><a href=#image-pull-policies>Image pull policies</a></li><li><a href=#application-restarts>Application restarts</a></li><li><a href=#application-logs>Application logs</a></li></ul></li><li><a href=#application-scaling>Application scaling</a><ul><li><a href=#replica-sets>Replica sets</a></li><li><a href=#scaling>Scaling</a></li><li><a href=#autoscaling>Autoscaling</a></li><li><a href=#compute-resources-managing>Compute resources managing</a></li></ul></li><li><a href=#updating-an-application>Updating an application</a><ul><li><a href=#deployment-update>Deployment update</a></li><li><a href=#deployment-rollouts-and-rollbacks>Deployment rollouts and rollbacks</a></li><li><a href=#the-deployment-lifecycle>The deployment lifecycle</a></li></ul></li><li><a href=#dealing-with-storage>Dealing with storage</a><ul><li><a href=#empty-dir>Empty dir</a></li><li><a href=#host-path>Host path</a></li><li><a href=#git-repo>Git repo</a></li><li><a href=#persistent-volumes>Persistent volumes</a></li></ul></li><li><a href=#application-configuration>Application configuration</a><ul><li><a href=#configmap>Configmap</a></li><li><a href=#secrets>Secrets</a></li></ul></li><li><a href=#jobs-and-daemons>Jobs and daemons</a><ul><li><a href=#jobs>Jobs</a></li><li><a href=#daemons-sets>Daemons sets</a></li></ul></li><li><a href=#stateful-applications>Stateful applications</a></li></ul></nav></aside></main></body></html>