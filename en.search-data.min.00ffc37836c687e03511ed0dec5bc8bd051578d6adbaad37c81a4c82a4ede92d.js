'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/mooc/math/sets/','title':"Теория множеств",'section':"Книга для чтения по высшей математике",'content':"Теория множеств #  Основные понятия #  Множество - совокупность элементов, объединенных по какому-либо признаку.\nМножества:\n конечные бесконечные  Обозначения:\n  \\(x \\in X - x \\text{ является элементом множества } Х \\\\ x \\forall - \\text{ для всех } x \\\\ x \\exists - \\text{ существует } x \\\\ \\alpha \\implies \\beta - \\text{ следовательно(импликация) } \\\\ \\alpha \\iff \\beta - \\text{ эквивалентно } \\\\ U - \\text{ универсум } \\\\\\)  Универсум (универсальное множество) - разумное множество элементов, за пределы которого не выходят при рассмотрении больших множеств.\nПодмножества #  Обозначается:  \\(A \\subset B - \\text{ (теоретико-множественное) включение }\\)  Определение:  \\(x \\in A \\implies x \\in B\\)  Т.е. A есть часть B.\n \\(A = B \\text{ if } A \\subset B \\text{ and } B \\subset A\\)  Т.e. множества A и B состоят из одних и тех же элементов.\nОперации над множествами #   Объединение(\u0026ldquo;ИЛИ\u0026rdquo;)  \\(A \\cup B\\)   Пересечение(\u0026ldquo;И\u0026rdquo;)  \\(A \\cap B\\)   Дополнение (\u0026ldquo;НЕ\u0026rdquo;; множество всех элементов U, не входящих в А)  \\(\\overline{A}\\)    Пустое множество #   \\(\\varnothing\\)  Не имеет ни одного элемента. Таким образом, пересечение непересекающихся множеств даёт:\n \\(A \\cap B = \\varnothing\\)  Основные свойства:  \\(A \\cup \\varnothing = A \\\\ A \\cap \\varnothing = \\varnothing \\\\ A \\cap \\overline{A} = \\varnothing\\\\ \\overline{\\varnothing} = U \\\\ \\varnothing = \\overline{U} \\\\\\)  Производные свойства множеств #   \\(A \\cup B = B \\cup A \\\\ A \\cup A = A \\\\ A \\cap B = B \\cap A \\\\ A \\cap A = A \\\\ A \\cup \\overline{A} = U \\\\ \\overline{\\overline{A}} = A \\\\ A \\cup ( B \\cup C ) = ( A \\cup B ) \\cup C \\\\ A \\cap ( B \\cap C ) = ( A \\cap B ) \\cap C \\\\ A \\cap ( B \\cup C ) = ( A \\cap B ) \\cup ( A \\cap C ) \\\\ A \\cup ( B \\cap C ) = ( A \\cup B ) \\cap ( A \\cup C ) \\\\\\)  Объединение и пересечение произвольного числа множеств  \\(A_1, A_2, ..., A_m\\)  обозначается:\n \\(\\bigcup_{i=m}^m A_i \\Big( \\bigcap_{i=m}^m A_i\\Big)\\)  Формулы Де Моргана #   \\(\\overline{A \\cup B} = \\overline{A} \\cap \\overline{B} \\\\ \\overline{A \\cap B} = \\overline{A} \\cup \\overline{B}\\)  Теория бесконечных множеств #  Пусть  \\(f : X \\to Y\\)  т.е. f - отображение (\u0026ldquo;функция\u0026rdquo;), у которой множеством задания является Х, на множество значений лежит в Y. Тогда каждому:\n \\(x \\in X\\)  сопоставляется элемент  \\(y = f(x) \\in Y\\)  Отображение такого вида называется взаимно однозначным соответствием, если для каждого  \\(y \\in Y\\)  существует единственный элемент  \\(x \\in X\\)  такой, что  \\(f(x) = y\\)  Если для множеств X и Y существует взаимно однозначное соответствие, то эти множества называются эквивалентными или равномощными. Обозначается  \\(X \\sim Y\\)  Свойства эквивалентности:  \\(X \\sim X \\\\ X \\sim Y \\implies Y \\sim X \\\\ X \\sim Y, Y \\sim Z \\implies X \\sim Z\\)  Мощность множества А - класс всех эквивалентных ему множеств. Соответственно, мощности множеств равны тогда и только тогда, когда они эквивалентны.\nИзначально мощность множества выражалась через количество его элементов.\nМножество, эквивалентное натуральному ряду называется счетным, не эквивалентное - несчетным.\nМощности четных(и нечетных), натуральных, целых и рациональных чисел равны. Кроме того, они равны и мощностям плоскости и трёхмерного пространства.\nМощность числовой прямой также носит название континуум.\nТеория конечных множеств #  Является частью дискретной математики.\nКомбинаторика - раздел математики, посвященный решению задач выбора и расположения элементов некоторого, обычно конечного множества в соответствии с заданными правилами.\nЕсли мы имеем дело с непустым конечным множеством M, состоящим из m элементов, это обозначается как:\n \\(|M| = m\\)  Отличие от обозначения модуля числа определяется только по контексту.\nРазмещение из m элементов по n - упорядоченный набор, составленный из n различных элементов множества M.  \\(1 \\le n \\le m\\)  Обозначается:  \\(A^n_m = \\frac{m!}{(m-n)!}\\)  Перестановкой называют размещение из m элементов по m.\n \\(P_m = m!\\)  Сочетанием из m элементов по n называется набор, состоящий из n различных элементов множества M.\n \\(C^n_m = \\frac{A^n_m}{P_m} = \\frac{m!}{(m-n)n!}\\)  Сочетание может быть использовано, например, для определения коэффициентов многочлена  \\((1 \u0026#43; x)^m\\)  при некотором фиксированном m:\n \\((1\u0026#43;x)^m = C_m^0 \u0026#43; C_m^1x \u0026#43; C_m^2x^2\u0026#43;...\u0026#43;C_m^mx^m = \\sum_{n=0}^m C_m^nx^n\\)  Эта формула называется биномом Ньютона, а сочетания в ней - биноминальными коэффициентами. Их сумма:  \\(\\sum_{n=0}^m C^n_m = 2^m\\)  "});index.add({'id':1,'href':'/docs/mooc/math/numbers/','title':"Числа",'section':"Книга для чтения по высшей математике",'content':"Числа #  Появление чисел #  Натуральное число в десятичной записи имеет вид:   \\(n = \\sum_{m=0}^{k-1} a_m 10^m \\\\ 0 \\le a-m \\le 9 \\text{, } m = 0, 1,...,k-1\\)  Расширяя эту формулу можно записать, что любое натуральное число  \\(n \\in \\N\\)  может быть представлено в виде:  \\(n = \\sum_{m=0}^{k-1} b_mp^m \\\\ 0 \\le b_m \\le p-1, m = 0, 1,..., k-1\\)  Это число записывается в p-ричной системе счисления, имеющей p цифр.\n9376 - единственное четырехзначное число, у которого при возведении в квадрат сохраняются последние 4 цифры: 9376^2 = 87909376  Целые числа(расширение натуральных чисел отрицательными и нулём):\n \\(x \u0026#43; q = p \\\\ p, q \\in \\N\\)  Рациональные числа(расширение целых чисел дробями):\n \\(qx = p \\\\ p,q \\in \\Z, q \\ne 0\\)  Вещественные числа(расширяют рациональные иррациональными; возникли из необходимости измерения непрерывных величин)\nТеорема: корень квадратный из 2 - иррациональное число #  Доказательство от противного. Предположим, что это рациональное число, тогда:  \\(\\sqrt 2 = \\frac{p}{q} \\\\ p,q \\in \\N\\)  p и q - взаимно простые числа(то есть не имеют общих делителей, кроме 1) =\u0026gt; по крайней мере одно из них - нечетное. Возводя обе части равенства в квадрат, получаем:  \\(p^2 = 2q^2\\)  Откуда следует, что p - четное число, таким образом,  \\(p = 2m \\\\ m \\in \\N\\)  Подставляя это выражение вместо p в предыдущее равенство, получаем:  \\(4m^2 = 2q^2 \\implies 2m^2 = q^2\\)  откуда следует, что и q - четное, что является противоречием.\nТеория чисел #  Теория чисел - раздел математики, изучающий целые числа.\nТеория простых чисел - натуральное число p \u0026gt; 1 называется простым, если оно делится только на единицу и на само себя. Натуральное число q \u0026gt; 1, не являющееся простым, называется составным.\nТеорема Евклида: Множество простых чисел бесконечно.\nДоказательство. Предположим обратное. Тогда:  \\(n_1 \u0026lt; n_2 \u0026lt; ... \u0026lt; n_m\\)  все простые числа. Составим число:  \\(p = n_1n_2...n_m \u0026#43; 1\\)  Так как p - больше наибольшего простого числа, оно составное. Но в этом случае, оно должно делиться на одно из простых чисел, однако из вида этого числа следует, что при делении его на любое простое число получится остаток 1, что и является искомым противоречием.\nЕвклидом же был введено понятие чисел-близнецов. Это такие простые числа:  \\(p, q, \\text{ } p \u0026gt; q \\\\ p - q = 2\\)  Он же поставил вопрос, является ли множество чисел близнецов бесконечным? Этот вопрос до сих пор не имеет ответа.\nТреугольник со сторонами 3,4,5 - называется египетский.  Теорема Ферма: Не существует натуральных чисел x, y, z, удовлетворяющих уравнению  \\(x^n \u0026#43; y^n = z^n \\\\ n \\ge 3\\)  "});index.add({'id':2,'href':'/docs/mooc/math/complex_numbers/','title':"Комплексные числа",'section':"Книга для чтения по высшей математике",'content':"Комплексные числа #  Появились в XVIв. в процессе исследования алгебраических уравнений(квадратных, кубических и т.д.). Как известно из школьного курса алгебры, некоторые такие уравнения \u0026ldquo;не имеют корней\u0026rdquo;. Но это верно только на множестве вещественных чисел. Для решения этой проблемы множество вещественных чисел было расширено так, чтобы подобные корни появились. Другое название комплексных чисел - мнимые.\nПо определению комплексно число - это упорядоченная пара вещественных чисел, обозначаемая (a, b). Эти числа можно интерпретировать как координаты точки(a - абсцисса, b - ордината).\nАбсцисса - координата точки по оси X\nОрдината - \u0026ndash;\\\u0026ndash; Y\nАппликата - \u0026ndash;\\\u0026ndash; Z  Два комплексных числа (a, b) и (c, d) считаются равными тогда и только тогда, когда:   \\(a = c \\\\ b = d\\)  Сложение комплексных чисел:  \\((a, b) \u0026#43; (c, d) = (a\u0026#43;c, b\u0026#43;d)\\)  Умножение:  \\((a, b)(c, d) = (ac - bd, ad \u0026#43; bc)\\)  Таким образом, произведение не зависит от порядка сомножителей.\nВычитание - нахождение неизвестного слагаемого\nДеление - нахождение неизвестного сомножителя   \\((a, b) - (c, d) = (a-c, b-d) \\\\ \\\\ \\dfrac{(a,b)}{(c,d)} = \\Big( \\dfrac{ac\u0026#43;bd}{c^2\u0026#43;d^2}, \\dfrac{bc-ad}{c^2\u0026#43;d^2} \\Big) \\text{ ,где}\\\\ (c,d) \\ne (0,0)\\)  Рассмотрим точки, лежащие на оси абсцисс, например (a, 0). Для таких комплексных чисел операции сложения и умножения дают следующие результаты:  \\((a,0) \u0026#43; (b,0) = (a\u0026#43;b,0) \\\\ (a,0)(b,0) = (ab, 0)\\)  из которых видно, что алгебра комплексных чисел, лежащих на лси абсцисс совпадает с алгеброй вещественных чисел, таким образом:  \\((a,0) = a\\)  Все вещественные числа (и только они) лежат на оси абсцисс, которая так же является числовой прямой. А все точки с ненулевой ординатой являются невещественными числами.\nДокажем, что в множестве комплексных чисел существует корень уравнения:  \\(x^2 \u0026#43; 1 = 0\\)  то есть, существует число, квадрат которого равен -1. Для этого возведем в квадрат число (0, 1):  \\((0,1)(0,1) = (-1,0) = -1\\)  Комплексное число (0,1), называемое мнимой единицей, Л. Эйлер предложил обозначать буквой i:  \\(i^2 = -1\\)  Таким образом, точкам оси ординат отвечают комплексные числа вида bi, где b - произвольное вещественное число.\nОбычно комплексные числа записывают в виде:  \\(a \u0026#43; bi\\)  Такая запись использует декартовы координаты точки. Кроме того, существует тригонометрическая форма записи комплексного числа. Положение точки на плоскости может быть определено с помощью расстояния от этой точки до начала координат и угла между положительным направлением оси абсцисс и лучом, выходящим из начала координат и проходящим через эту точку.\nДля комплексного числа a+bi упомянутые величины обозначаются через:  \\(r \\text{ - расстояние} \\\\ \\varphi \\text{ - угол}\\)  Таким образом, при любом расположении точки a + bi на координатной плоскости справедливы соотношения:  \\(x = r \\cos \\varphi, \\\\ y = r \\sin \\varphi\\)  Отсюда, а так же из теоремы Пифагора следует:  \\(r = \\sqrt{a^2 \u0026#43; b^2}\\)  Число r называется модулем комплексного числа a + bi, а угол - аргументом. При r = 0 модуль не определен. Единственное число с нулевым модулем - (0,0)\nК комплексным числам не применимы неравенства типа \u0026ldquo;больше\u0026rdquo; или \u0026ldquo;меньше\u0026rdquo;, поэтому сравнивать их можно только по модулю.\nОсновная теорема алгебры #  Пусть  \\(P(z) = a_0 \u0026#43; a_1 z \u0026#43; a_2 z^2 \u0026#43; ... \u0026#43; a_n z^n -\\)  многочлен(полином) с комплексными коэффициентами  \\(a_0, a_1, a_2,...,a_n\\)  заданный в комплексной плоскости, т.е. множестве чисел z = a + bi. Если  \\(a_n \\ne 0\\)  то говорят, что многочлен P имеет степень n.\nОсновная теорема алгебры: Если степень многочлена n \u0026gt;= 1, то уравнение  \\(P(z) = 0\\)  имеет, по крайней мере один комплексный корень.\nЭто утверждение также называется Теоремой Гауса.\nОсновным следствием этой теоремы является то, что любой многочлен степени n \u0026gt;= 1 может иметь не более n корней (при этом некоторые из них могут совпадать).\nСтоит отметить, что общей формулы для уравнений степени n \u0026gt;= 5 не существует.\n"});index.add({'id':3,'href':'/docs/mooc/math/matrices/','title':"Матрицы",'section':"Книга для чтения по высшей математике",'content':"Матрицы #  Матрицами в математике называется прямоугольная таблица чисел:   \\(\\begin{pmatrix} a_11 \u0026amp; a_12 \u0026amp; ... \u0026amp; a_1m \\\\ a_21 \u0026amp; a_22 \u0026amp; ... * a_2m \\\\ ... \u0026amp; ... \u0026amp; ... \u0026amp; ... \\\\ a_n1 \u0026amp; a_n2 \u0026amp; ... \u0026amp; a_nm \\end{pmatrix}\\)  Сами числа в этом случае называются элементами матрицы. Эту же матрицу для упрощения обозначают:  \\(\\Vert a_{ij} \\Vert\\)  Имея в виду, что i принимает значения от 1,\u0026hellip;,m, а j, соответственно, 1,\u0026hellip;,n. Если m = n, матрица называется квадратной, а число m=n ее порядком. Прямоугольная матрица, в которой n=1, называется столбцом. Квадратная матрица порядка 1 отождествляется с числом, поэтому понятие матрицы можно считать обобщением понятия числа.\n(0,1)-матрица - такая матрица, все элементы которой равны либо 0, либо 1. Наиболее известные подобные матрицы:\n O - нулевая матрица, то есть матрица, все элементы которой равны 0 E - единичная матрица, то есть квадратная матрица, в которой все \u0026ldquo;диагональные\u0026rdquo; элементы равны 1, а остальные - 0  Алгебра произвольных квадратных матриц #  Пусть  \\(A = \\Vert a_{ij} \\Vert , B = \\Vert b_{ij} \\Vert\\)  квадратные матрицы порядка n. Матрицы A и B считаются равными, если  \\(a_{ij} = b_{ij}\\)  для всех возможных i и j.\nСуммой A + B матриц A и B называется такая матрица C, элементы, которой равны сумме элементов исходных матриц, то есть матрицы складываются \u0026ldquo;поэлементно\u0026rdquo;.\n"});index.add({'id':4,'href':'/docs/cloud/','title':"Cloud",'section':"Docs",'content':"Notes about clouds #  "});index.add({'id':6,'href':'/docs/cloud/aws/reliability_and_business_continuity/','title':"Reliability and business continuity",'section':"Cloud",'content':"Scalability and elasticity #  Scalability - the ability of a system to increase resources to accommodate increased demand. This can be done vartically or horizontally, and is not necessarily automated.\nElasticity - the ability if a system to increase and decrease resources allocated (usually horizontally) to match demand, and implies automation.\nAWS autoscaling resources:\n Auto Scaling groups spot fleet ECS DynamoDB Aurora Read Replicas  Scaling strategies:\n availability balanced cost  Launch template:\n operational excellence:  versioning for rollback, re-use or even default templates use for regular EC2 launch tasks   cost optimization:  utilize T* unlimited instances, dedicated hosts on-demand and spot at the same time   reliability, performance efficiency:  multiple instance types    Autoscaling operations:\n autoscaling policies monitoring ASG limits deploy launch templates lifecycle hooks cooldown periods warm pool configuration  Autoscaling scenarios:\n stateless web apps unpredictable traffic steady-state groups message consumer apps  Autoscaling anti-scenarios:\n monolithic applications (singleton instance) applications with fixed IP address applications with many manual deploy steps applications with short, large, random traffic spikes  Caching solutions #  CloudFront #  CloudFront basics:\n global scope namaged CDN SSL termination cache reads and writes  CloudFront operations:\n HTTP redirect to HTTPS edge location selection ACM (Amazon certification manager) cert integration IAM cert integration multiple origins  ElasticCashe #  Elasticcache can help absorb traffic spikes, reduce latence, especially for same-AZ traffic.\nWe can use Memcached or Redis.\nElasticache basics\n Memcached:  AZ scoped managed in-memory volatile all endpoints writable   Redis:  AZ scoped managed in-memory persistent (it replicate the same cache for each node in the cluster) 1 write endpoint sharding option    RDS and aurora replicas #  RDS resilience #  RDS may has read replicas in each AZ (but in one region), that can be promoded to primary.\nAlso possible to use cross-region read replicas (except SQL server).\nMulti-AZ deployment as a configuration option allows you to deploy one or more standby node, which can be promote to primary in case of some issues with primary node.\nSatndby node will promote to the primary in next cases:\n resize instance OS Patching initiate reboot with failover AZ-scoped availability issue  RDS read replicas #   unique endpoint separate implementation, than Multi-AZ up to 5 RRs per primary  Aurora failover conditions #  Failover with existing replicas flips DNS CNAME, promotes replica - takes ~30s.\nIf multiple replicas are present, customer can set replicas priority and Promotion tiers.\nFailover with Aurora Serverless launches replacement in new AZ.\nIf no replicas and not running Serverless, Aurora attempts new DB instance in same AZ as primary on best-effort basis.\nAurora read replicas:\n single endpoint replace multi-AZ up to 15 RRs per primary +5 RDS RRs per primary promotion tier support  Loosely Coupled Architectures #  Route 53 may do the helth checks for multiple ELBs in defferent regions. And in case of failure of primary ELB it will switch DNS ALIAS to secondary ELB automatically.\nIn addition to helth checks Route 53 may use Latence-based routing. And if all ELBs are unavailable it may return static site from S3 as response.\nS3 + Lambda integration. We can trigger Lambda function by uploading a file to the S3.\nHA and resilience #  ELB and Route 53 Health Checks #  Route 53 Health cheks basics:\n monitor an endpoint  endpoint  IP address host name domain name   protocol  HTTP/HTTPS  path (optional)   TCP   port (single or range)   monitor other health cheks (calculated)  up to 256 other HCs   monitor CW alarms  only one CW alarm may be monitored    Load balancers:\n Classic Network Application Gateway  ELB HC basics:\n check an EC2 instance  supported by all 4 types of LBs   check an IP address  supported by all LBs, except Classic   check a Lambda function  supported only by Application LB    LB protocol support\n    TCP Ping HTTP HTTPS     CLB + + +   NLB +     ALB  + +   GWLB + + +    Single AZ and Multiple AZ Architectures #  EC2 Auto scaling:\n deploy into a single AZ for:  performance colocation with other resources   deploy into multiple AZ for:  resilience cost optimization    ELB:\n deploy into a single AZ for:  performance colocation with other resources   deploy into multiple AZ for:  resilience    FSx filesystem:\nOnly supports one AZ for data. But it creates an ENI in a single A which can be reached from multiple AZs.\nRDS:\n deploy into a single AZ for:  cost optimization   deploy using Multi-AZ:  resilience synchronous replication   deploy Read Replicas for:  further resilience horizontally scaled reads    Fault-tolerant Workloads #  EIP (elastic IP address):\n region scoped can be ressigned it is static  EFS:\n resion scoped durable data is replicated around the region you can acces to the FS directly with mount targets:  AZ-scoped can be accessed cross-AZ can specify directory and userID   can be mounted to:  EC2 ECS EKS lambda function    Route 53 Routing #  Routing options\n simple routing  traditional DNS - each resord has the same weight   weighted routing  you can assing weight from 0-255 for each endpoint in case of weight 0, this endpoint won\u0026rsquo;t be returned   latency-based routing  DNS response determined by lowest latence from client   failover routing  you have primary and secondary endpoints secondary will returned only if primary fails health cheks also you can have multiple primary endpoints requires health checks    Alias records - are pointers to some AWS resources:\n CloudFront Distributions Elastic Beanstalk Environment ELB S3 static site endpoints route 53 record in the same zone VPC endpoint API gateway Global accelerator  Backup and restore #  Automated Snapshots and Backups #  Authomated backups\n   Service Enabled by default Can be disabled Retention(days) PITR*     EC2  + 0-∞    RDS + + 0-35 +   Aurora +  1-35 +   Redshift +  1-35    DynamoDB +  35 +    * Point in time recovery\nEC2 has no native backup feature, you can use AWS Backup or DLM(Data Lifecycle Manager) for this task.\nManual backups\n   Service AWS backup Rotate Retention(days) Copy or share     EC2 +  ∞ +   RDS +  ∞ +   Aurora +  ∞ +   Redshift   ∞ +   DynamoDB +  ∞     You may implement rotation by your own or use AWS Backup to gain that ability.\nEC2 data lifecycle manager (DLM) allows you to create:\n EBS snapshots EC2 AMIs Cross-account copy Snapshot/AMI rotation No unique features against AWS Backup  AWS Backup resources\n backup vault backup plan backup job restore point  List of supported services\nAWS Backup workflow\n Resources assignments for Backup plans require tags to function. So you have to create a tag backup:true Backup vaults hold restore points and provide resource-level access control Backup plan provide the integration between resources, jobs abd vaults Each backup job generates a single restore point in a vault Vaults can hold multiple restore points from different resource types  Backup plan - is a siries of a tasks that you have to compleate. You need to determine:\n backup rules  ways to creation:  template interactive build JSON   anatomy  backup vault frequency  time period cron expression   retention copy (optional)  remote region rmeote account       resource assignments  specify tags    Database Restore Tasks #  RDS restore task\n# Automated backup # * aws rds restore-db-instance-to-point-in-time # Manual snapshot # * aws rds restore-db-instance-from-db-snapshot # Backup in S3 # * aws rds restore-db-instance-from-s3 # Promote read replica aws rds promote-read-replica * - these commands create a new DB instance (and potentially new endpoint) during restore\nAurora restore tasks\n# Continues backup # * aws rds restore-db-instance-to-point-in-time # Manual snapshot # * aws rds restore-db-instance-from-db-snapshot # Backup in S3 # * aws rds restore-db-instance-from-s3 # Promote read replica aws rds promote-read-replica # Backtrack aws rds backtrack-db-cluster * - these commands create a new DB instance (and potentially new endpoint) during restore\nBynamoDB\n# On-demand backup aws dynamodb restore-table-from-backup # PITR aws dynamodb restore-table-to-point-in-time Both of these create a new table during restore\nRedshift\n# all snapshots # * aws redshift restore-from-cluster-snapshot # restore table aws redshift restore-table-from-cluster-snapshot # relocate a cluster to another AZ in the same region aws redshift modify-cluster --availability-zone * this creates a new cluster endpoint during restore\nS3 Versioning and Lifecycle Rules #  By default S3 versioning is disabled. When you enable versioning:\n version ID attached to each version of an object delete operation attaches a delete marker to the object (and doesn\u0026rsquo;t actually delete the data)  Versioning is a good way to avoid accidental deletion.\nConsiderations:\n cost of many versions performance of many versions more complex lifecycle rules  Lifecycle rules - options\n transition current version transition previous version expire current version delete expired delete markers delete incomplete multipart uploads  Lifecycle rules considirations:\n object size object age requirements bucket or prefix scope tag scope conflicting rules  Glacier vault lock\n permission policy use to deny deletes enforce lifecycle locked after 24 hours  S3 replication options:\n same-region, same account cross-region, same account cross-region, cross-account (ownership can be changed to that of the destination bucket) multiple bucket destination  storage class of a destination can be changed for cost optimisation rules are evaluated in priority order   multy-way replication between 2+ buckets  pre-existing objects can only be replicated by AWS support    All replication requires:\n versioning enabled at source and destination and IAM Role for permissions  "});index.add({'id':7,'href':'/docs/cloud/aws/monitoring_logging_and_retention/','title':"Monitoring, logging and retention",'section':"Cloud",'content':"Collecting logs #  CloudWatch #   region scope fault tolerant durable push, not pull  Everything in CW logs is going to be stored in a log group\nA log stream - entity, which records into particular log group\nA log record - is a single record of some event\n# log group operations aws logs create-log-group aws logs delete-log-group aws logs associate-kms-key # log stream operations aws logs create-log-stream aws logs delete-log-stream # log record operations aws logs put-log-events aws logs get-log-events aws logs get-log-record Intagration with another services #     service type of logs     CloudTrail itself logs   API Gateway Access/Execution logs   ECS Container logs   Lambda Execution logs   RDS DB logs   Systems Manager Run   EC2 App/OS logs    CloudTrail #   region scope audit log logs both success and failure works with different types of events:  management data insights   organizations support  # trail operations aws cloudtrail create-trail aws cloudtrail delete-trail # logging operations aws cloudtrail start-logging aws cloudtrail stop-logging # validation operations aws cloudtrail validate-logs Analyzing logs #  CloudWatch insights #   use existing log group/stream (sql-like) query engine alarm integration  # Create query aws logs put-query-definition aws logs delete-query-definition # execute query and get results aws logs start-query aws logs stop-query aws logs get-query CloudTrail insights #   use existing events ML-driven no query engine  aws cloudtrail put-insights-selectors aws cloudtrail get-insights-selectors ElasticSearch and Kibana #   region scope analytics engine reporting engine CWL integration  Exporting logs #  CWL export #   S3 export CLI tail subscription filter - automatically delivers logs from CW to tne another service:  kinesis stream lambda function kinesis data firehose logical destination    # create or delete export task aws logs create-export-task aws logs cancel-export-task # live tail of log stream aws logs tail # create new subscription filter aws logs put-subscription-filter --log-group-name --filter-name --filter-pattern --destination-arn --role-arn Kinesis data firehose #  This is a sort of batch mechanism\n region scope streaming pipeline buffer and batch  Possible destinations:\n S3 Redshift ES HTTP endpoint Datadog MongoDB New Relic Splunk  Kinesis Firehose delivery to S3:\nCloudWatch Alarm #  Terms #  Period - length of time evaluate metric or expression to create an individual data point\nEvaluation Perion - number of data points to evaluate when determining alarm state\nDatapionts to Alarm - number of data points within evaluation period that must be breaching to cause ALARM state. Does not have to be consecutive.\nEvaluation range - number of data points retrieved by CW for alarm evaluation. Greater than Evaluation Period but varies.\nMissing data points #   missing - alarm doesn\u0026rsquo;t consider messing data points at all notBreaching - missing data points treated as being within threshold breaching - missing data points treated as breaching threshold ignore - current alarm state is maintained  Other alarms options #   high resolution alarm - period less than 1 minute math expression alarm - combine up to 10 metrics percentile alarm - percentile as monitored statistics anomaly detection alarm - use standard deviation composite alarm - combine alarm state of other alarms  alarm actions #   SNS topic:  email sms Lambda etc   EC2 actions:  stop reboot terminate recover   Auto Scaling action Systems Manager (SSM) OpsItem  CloudWatch Metric Filters #  You can create a metric from a string with Metric Filters.\nString Metric Filters basics #   match everything - \u0026quot;\u0026quot; single term - \u0026quot;ERROR\u0026quot; include/exclude terms - \u0026quot;ERROR\u0026quot; - \u0026quot;permissions\u0026quot; // will exclude all strings, which contains \u0026ldquo;permissions\u0026rdquo; multiple terms using AND - \u0026quot;ERROR memory exceprion\u0026quot; multiple terms using OR - ?ERROR ?WARN  Space-delimited metric filter basics #   specify all fields with a name, bounded by [] and separated by commas specify unknown number of fields with \u0026quot;...\u0026quot; add conditions: =, !=, \u0026lt;, \u0026lt;=, \u0026gt;, \u0026gt;= utilize * to match partial string or numbers implement AND with \u0026amp;\u0026amp; and OR with ||  # filter examples with Apache log # match all 4XX response codes [ip, id, user, timestamp, request, status_code = 4*, size] # match response size \u0026gt; 1000 bytes [ip, id, user, timestamp, request, status_code, size \u0026gt; 1000] # ignore all redirect response codes [ip, id, user, timestamp, request, status_code != 3*, size] JSON metric filter basics #   {SELECTOR EQUALITY_OPERATOR STRING}  EQUALITY_OPERATOR is = or !=   {SELECTOR NUMERIC_OPERATOR NUMBER}  NUMERIC_OPERATOR can be =, !=, \u0026lt;, \u0026gt;, \u0026lt;=, \u0026gt;=   SELECTOR starts with $, indicating the root of JSON  SELECTOR supports arrays implement AND with \u0026amp;\u0026amp; and OR with || publish numerical value using \u0026quot;metricValue:\u0026quot;    # filter example with CloudTrail log # match all console login failures { ($.eventName = ConsoleLogin) \u0026amp;\u0026amp; ($.responseElements.ConsoleLogin = \u0026#34;Failure\u0026#34;) } # match all console logins by IAM user john.doe { ($.eventName = ConsoleLogin) \u0026amp;\u0026amp; ($.userIdentity.userName = \u0026#34;john.doe\u0026#34;) } # match all root user activity { $.userIdentiry.type = \u0026#34;Root\u0026#34; \u0026amp;\u0026amp; $.userIdentity.invokeBy NOT EXISTS \u0026amp;\u0026amp; $.eventType != \u0026#34;AwsServiceEvent\u0026#34; } CloudWatch Dashboards #  Dashboard sharing:\n enable in settings share with specific IAM user(s) share publicly share via SSO share logs widgets  CloudWatch alarm remediation #  EC2 actions #  recover action mya be used only if underline HW has failed. It allows to move instance to the another HW. Also:\n certain instance types VPC only default or dedicated tenance no instance store  Autoscaling actions #  Manual actions #  If we recieve notification from CW alarm, we can fix the issue manually with AWS console or cli.\n performance metrics:  scale horiaontally scale vertically   availability metrics:  restore recover fail over    Automated actions #  An SNS topik can trigger:\n custom endpoint (with any lind of script) Lambda funcition via API Gateway Lambda function directly  EC2 status checks #   System reachability - represents host OS and hardware layer Instance reachability - represents guest OS and processes  EBS volume status checks #   if volume NOT provision IOPs:  ok warning impaired insufficient data   in other case:  Normal Degreded Stalled insufficient data    EventBridge rules #  EventBridge basics\n region based default event bus custom event bus sources and targets replay feature DLQ feature  Sources:\n CloudTrail API events GuardDuty findings Other service events Forwarded events Scheduled events  Rules is a JSON format match event properties:\n API gateway CW logs EC2 actions Remote event bus Lambda function SNS topic  Patterns:\n event schedule  Config rules and SSM automation documents #  Config basics\n region scope config streams partial coverage capture changes capture config snapshots  Config rule creation example\nWe have EC2 instance and AWS config. The last one will capture any changes in instance\u0026rsquo;s properties via config stream.\nYou can react for these changes via:\n AWS managed rules (for specific changes) custom rules via Lambda function  "});index.add({'id':8,'href':'/docs/cloud/as-a-service/','title':"As a Service schema",'section':"Cloud",'content':"+----------------+ | IaaS | | VM | | VPC | +----------------+ | | V +----------------+ | PaaS | | Managed DB | +----------------+ | | V +----------------+ | SaaS | | GitLab | | Shared hosting | +----------------+ "});index.add({'id':9,'href':'/docs/k8s/common_notes/','title':"Common k8s notes",'section':"Kubernetes",'content':"Secrets and ConfigMaps #  Possible alternatives:\n ConfigMaps:  ZooKeeper Consul   Secrets:  Vault Keywhiz Confidant    "});index.add({'id':10,'href':'/docs/k8s/','title':"Kubernetes",'section':"Docs",'content':"TODO:\n https://github.com/ContainerSolutions/k8s-deployment-strategies https://www.freecodecamp.org/news/the-kubernetes-handbook/ https://monicabhartiya.com/posts/kubernetes-syllabus-update https://github.com/kelseyhightower/kubernetes-the-hard-way https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-b190cc97f0f6 https://kustomize.io  "});index.add({'id':11,'href':'/docs/mooc/astronomer/','title':"Apache Airflow fundamentals",'section':"Courses notes",'content':"Start the project #  Astronomer CLI quickstart guide\nTo prevent error like this:\nEnv file \u0026#34;.env\u0026#34; found. Loading... buildkit not supported by daemon Error: command \u0026#39;docker build -t astronomer_f5dd6c/airflow:latest failed: failed to execute cmd: exit status 1 You need to update your Docker config with this piece of code:\n\u0026#34;features\u0026#34;: { \u0026#34;buildkit\u0026#34;: false } Source of the solution\nmkdir \u0026lt;directory-name\u0026gt; \u0026amp;\u0026amp; cd \u0026lt;directory-name\u0026gt; astro dev init astro dev start It starts 3 containers:\n Postgres: Airflow\u0026rsquo;s Metadata Database Webserver: The Airflow component responsible for rendering the Airflow UI Scheduler: The Airflow component responsible for monitoring and triggering tasks  The essentials #  Airflow if an orchestrator for creating dynamic data pipelines and executing tasks in the right order and at the right timec.\nAirflow benefits:\n dynamic - data pipelines are described as a python script. scalable - you can choose required type of executor (kubernetes for example) to run as many tasks as you need in distributed environment interactive - there are 3 ways to interact with Airflow:  Web UI CLI REST API   extensible - you can customize Airflow by creating new plugin  NB: Airflow IS NOT:\n a streaming solution a data processing framework (if you need to precess TBs of data, use the Airflow to trigger the Spark job)  Core components #   Web Server - Flask server with GUnicorn, serving the Web UI Scheduler - the heart of Airflow; responsible for schaduling and triggering tasks; for HA purposes you can run multiple schedulers Metadata Database - you can use any compatible with SQLAlchemy DB (even MongoDB, but with limitations, for example you can\u0026rsquo;t run multiple schadulers)  Additional components:\n Executor - defines how your tasks are going to be executed by Airflow  Queue   Worker - defines where your tasks are actually executed; it is a process there task is executed  2 Common Architectures #  Single node #  Node:\n Web Server Metastore Scheduler Executor  Queue    Workflow:\n Web Server interacts with Metastore and fetchs required data:  statuses of tasks users permissions etc   If the task is ready to be scheduled, the Scheduler will:  change the status of the task in the Metastore create the instance object of this task send the task object to the Queue of the Executor   After that the task is ready to be fetched by the Worcker Executor interacts with Metastore and updates the task status  The Web Server doesn\u0026rsquo;t interact directly with the Scheduler nor Executor.\nMulti nodes (Celery) #  Node1:\n Web Server Scheduler Executor  Node2:\n Metastore Queue (in this case we need dedicated queue like RabbitMQ or Redis)  Worker Node 1-n\n Airflow Worker  Workflow:\n Web Server interacts with Metastore like in the Single Node installation Scheduler interacts with the Metastore and the Executor When the task is ready to be scheduled, it sends to the Executor The Executor resends the task to the Queue From the Queue task will be pulled and executed by the Workers  Core Concepts #  DAG - in Airflow is a Data pipeline.\nOperator - is an object for creating tasks in the a DAG.\nTypes of operators:\n action - allow you to execute something in your data pipeline; examples:  python operator shell op postgres op etc   transfer - allow you to transfer data from the source to the destination; example:  mysql-to-presto operator   sensor - is a kind of a trigger for tasks; example:  file sensor - will wait until some file will placed in a specific directory    Task - is an instance of an operator in a DAG. When the task is ready to be scheduled, it becomes to:\nTask instance object - represents a specific run of a task: DAG + Task + Point of time\nDependencies - describes relations between operators in a DAG\nWorkflow - combination of all concepts described above.\n+---------------------------------+ | DAG | | +----------+ +----------+ | | | Operator | | Operator | | | | +------+ | | +------+ | | | | | Task | |------\u0026gt;| | Task | | | | | +------+ | | +------+ | | | +----------+ +----------+ | +---------------------------------+ Task Lifecycle #  When you add new DAG file into the DAGs folder it parses by:\n Web Server - it parses DAGs every 30 seconds by default Scheduler - parses NEW files every 5 minutes by default  After that the Scheduler creates a DagRun object in the Metastore. This object is an instance of your DAG.\nWhen the Task is ready to be scheduled, the Scheduler creates a TaskInstance object in the Metastore. And it has the status Scheduled.\nWhen the Scheduler asigns the Task to the Executor, it has the status Queued.\nWhen the Task has the Queued status, the Executer is ready to taje that task and execute it on the Worcker. Now the status is Runing.\nWhile the task is executes, the Executor updates its status in the Metastore and when it will be done, the status will changed to Success.\nThe Scheduler checks the status of the TaskInstance object and if it is Success and there are no more tasks in this DAG.\nOnly after that the Web Server updates the UI.\nExtras and providers #  Extras package - extends the functionality of the Airflow, for example it allows to run the AirFlow in the Celery mode. It extends the core of the Airflow.\nProveders allows you to add new functionality on top of the Airflow. Providers are separeted from the Airflow\u0026rsquo;s core. For example - PostgreSQL provider.\nUpgrading Airflow #   DB   create a backup of the Metastore  DAGs   make sure the is no deprecated features in your DAGs. pause all the DAGs and make sure no tasks is running  Upgrade Airflow Upgrade the DB by  airflow db upgrade Restart:   Scheduler Web Server Workers  Interacting with Apache Airflow #  CLI #  airflow db init # Initialize the Metastore DB and generate required files and directories airflow db upgrade # Upgrade DB\u0026#39;s schema airflow db reset # Erase the DB airflow webserver # Start the Web Server airflow scheduler # Start the Scheduler airflow celery worker # Start the Worker airflow dags (un)pause # (Un)pause a DAG airflow dags trigger # Trigger a DAG airflow dags list # List all DAGs airflow tasks list ${DAG_ID} # List all tasks of the specific DAG airflow tasks test ${DAG_ID} ${TASK_ID} ${EXECITION_DATE} # Usefull on adding new tasks, it allows to execute particular task regardles its dependencies; date format is YYYY-MM-DD airflow dags backfill -s ${YYYY-MM-DD} -e {YYYY-MM-DD} --reset_dagruns ${DAG_ID} # Allows to rerun the past DAGs REST API #  Stable API documentation\nDAGs and Tasks #  DAG seketon #  Example of the simples but valide empty DAG:\nfrom airflow import DAG with DAG(dag_id=\u0026#39;dag_name\u0026#39;) as dag: None You just need to put this file into the DAG\u0026rsquo;s folder.\nBy default the DAG has no owner and it is scheduled to execute every day at 00:00:00.\nDemystifying DAG Scheduling #   start_date - defines the date at which your DAG starts being scheduled schedule_interval - define a frequency at which your DAG will be triggered; 24 hours by default  NB: DAG will be triggered at start_date + scheduled_interval   execution_date - begining of the execution period (equals to start_date) start_date will shifted to start_date + scheduled_interval end_date - the date at whict your DAG won\u0026rsquo;t be scheduled anymore  from airflow import DAG from airflow.operators.dummy import DummyOperator from datetime import datetime # Define the start_date for the whole DAG with DAG(dag_id=\u0026#39;simple_dag\u0026#39;, schedule_interval=\u0026#34;*/10 * * * *\u0026#34;, start_date=datetime(2021, 1, 1)) as dag: task_1 = DummyOperator( task_id=\u0026#39;task_1\u0026#39;, # Define the start_date for the specific task, but it\u0026#39;s a bad practice start_date=datetime(2021, 1, 2) ) By default the datetime in UTC and all dates in Airflow are stored in UTC.\nIf you define the start_date in the past, by default Airflow will trigger all non triggered dagRuns between the start_date and the current date.\nNB: NEVER use datetime.now() as a start_date, because of start_date shifting before each DAG scheduling, this DAG will never run.\nschedule_interval parameter may be represented as:\n a cron-string (defines the absolute time):  */10 * * * *   a preseted value like:  @daily @weekly   timedelta object (defines relative for previous run time):  timedelta(hours=7) timedelta(days=1)   None (in this case DAG will never being autimatically triggered by scheduler)  Backfilling and Catchup #  Backfilling allows you to run or rerun past non triggered or already triggered dagRuns.\nBy default Airflow will rau all non triggered dagRuns between the start_date and the current date.\nWe can enable or disable the behavior with catchup: bool = True parameter of the DAG() object. If you set this parameter to False, only the latest non triggered dagRun will be scheduled.\nAt the addition you can set max_active_runs: int to specify max number of runs for specific DAG, which allowed to run at the same time. With catchup=True and this parameter you can limit number of simultaneously runned DAGs.\nEven if the catchup=False you still can start backfilling process from CLI.\nFocus on operators #  An operator is a task in your DAG.\nIf you have more than one buisness task you shouldn\u0026rsquo;t put all of them into a single operator, because if one of this tasks fails, on retry you will rerun all of this tasks.\nOnother words, you should to create a separate operator for each your task. For example data loading and data cleeaning should be executed in separete operators.\nOperator should be idempotent.\nYou can set default arguments for each task with default_args: dict parameter of DAG object:\ndefaults_args = { \u0026#39;retry\u0026#39;: 5, \u0026#39;retry_delay\u0026#39;: timedelta(minutes=5) } with DAG(dag_id=\u0026#39;simple_dag\u0026#39;, default_args=defaults_args, ...) as dag: task_1 = DummyOperator( task_id=\u0026#39;task_1\u0026#39;, ) task_2 = DummyOperator( task_id=\u0026#39;task_2\u0026#39;, # this parameter overrides the default retry=3 ) task_3 = DummyOperator( task_id=\u0026#39;task_3\u0026#39;, ) Documentation for the baseoperator class.\nExecuting python functions #  The Python operator is the most common used operator in the Airflow.\nfrom airflow import DAG from airflow.operators.python import PythonOperator from airflow.utils.dates import days_ago defaults_args = { \u0026#39;retry\u0026#39;: 5, \u0026#39;retry_delay\u0026#39;: timedelta(minutes=5) } def _downloading_data(my_param, ds, **kwargs): print(\u0026#39;Data is downloaded\u0026#39;) # call all kwargs print(kwargs) # call one specific common parameter print(ds) # call a custom parameter, defined in the Operator\u0026#39;s os_kwargs argument print(my_param) with DAG(dag_id=\u0026#39;simple_dag\u0026#39;, default_args=defaults_args, ...) as dag: downloading_data = PythonOperator( task_id = \u0026#39;downloading_data\u0026#39;, python_callable = _downloading_data # Here you can specify custom params op_kwargs = {\u0026#39;my_param\u0026#39;: 42} ) To get access to the DAG context you just need to pass **kwargs to the python function of the name of the specific parameter.\nPutting DAG on hold #  File sensor - special type of operator, which waits when a file will putted into specific place.\nIn UI:\nAdmin -\u0026gt; Connections -\u0026gt; +\n conn id - id, which you will use in DAG\u0026rsquo;s code conn type - is required connection unavailable, you can add it by install provider extra - this field is not secure; you have to put a JSON to it and for file conn type, this JSON should contain only path to directory, where file should exists:  { \u0026#34;path\u0026#34;: \u0026#34;/tmp/\u0026#34; } How to use the FileSensor operator:\nfrom airflow.sensors.filesystem import FileSensor with DAG(...) as dag: ... waiting_file = FileSensor( task_id = \u0026#34;waiting_for_data\u0026#34;, fs_conn_id = \u0026#34;fs_default\u0026#34;, filepath = \u0026#39;my_file.txt\u0026#39; # Determines how often the connection will be checked; 30 sec by default poke_interval = 15 ) Executing Bash commands #  from airflow.operators.bash import BashOperator with DAG(...) as dag: ... processing_data = BashOperator( task_id = \u0026#39;processing_data\u0026#39;, bash_command = \u0026#39;exit 0\u0026#39; ) Define the path #  After all tasks are implemented, you need to defile the dependencies between tasks.\nThere are two ways to define the dependencies between tasks:\n with set_upstream or set_downstream methods  with DAG(...) as dag: ... downloading_data.set_downstream(waiting_file) processing_data.set_upstream(waiting_file)  with right (\u0026raquo; set_downstream equivalent) or left (\u0026laquo; set_upstream equivalent) bit-shift operator (more commonely used option)  with DAG(...) as dag: ... downloading_data \u0026gt;\u0026gt; waiting_file \u0026gt;\u0026gt; processing_data chain() function #  Completely the same DAG you can get with the cain() function:\nfrom airflow.models.baseoperator import chain with DAG(...) as dag: ... chain(downloading_data, waiting_file, processing_data) Multiple tasks #  To execute multiple tasks on the same DAG level (i.e. simultaneously), you just need to put them into the list.\nwith DAG(...) as dag: ... downloading_data \u0026gt;\u0026gt; [ waiting_file, processing_data ] Cross dependencies #  Also you can define cross dependencies between tasks:\nfrom airflow.models.baseoperator import cross_downstream with DAG(...) as dag: ... cross_downstream([downloading_data, sone_another_task], [waiting_file, processing_data]) Here we will get:\ndownloading_data \u0026raquo; [waiting_file, processing_data] AND sone_another_task \u0026raquo; [waiting_file, processing_data]\nThis function required because you couldn\u0026rsquo;t create dependencies directly between lists of tasks.\nExchanging Data #  XCom (cross communication) - is the mechanism, which allows you to exchange smal amount of data between tasks.\nIn the UI you can access all the XComs via Admin-\u0026gt;XComs menu.\nThe 1st way to implement XUom is returning data from the function\nFor getting access to the XCom from another task, we have to use the context of a dagRun and use the ti (for taskinstance ) object. With help of this object we can pull the required XCom:\ndef _first_task(): ... return 42 def _seconf_task(ti): my_xcom = ti.xcom_pull(key=\u0026#39;return_value\u0026#39;, task_ids=[\u0026#39;first_task\u0026#39;]) ... Also you may push some value to XCom without returning from the function:\ndef _another_task(ti): ... ti.xcom_push(key=\u0026#39;my_key\u0026#39;, value=42) NB: XComs are storred in the Metadata DB and they are limited in size, based on the DB which you use:\n SQLite - up to 2Gb PostgreSQL - up to 1 Gb MySQL - up to 64 Kb (sic!)  Failure #  If the task is finished with an error, Airflow automatically trys to run it again.\nYou can rerun the task manually from the UI by clicking clear button for the required task_instance. For applying it to the multiple tasks at the same time, use Browse-\u0026gt;Task Instances menu and search filters on that page.\nYou can enable email notifications by setting:\ndefault_args = { ... \u0026#39;email_on_failure\u0026#39;: True, \u0026#39;email_on_retry\u0026#39;: True, \u0026#39;email\u0026#39;: \u0026#39;admin@example.com\u0026#39; } Also you may add some custom logic on failure, by specifying argument on_failure_callback for the operator.\nExecutors #  The default executor #  Executor defines where your tasks are going to executed in your Airflow instance.\nThe default executor is SequentialExecutor and it is based on SQLite), so it executes task only one after another.\nSequential is usefull for debugging purpose or for experiments.\nYou may configure the executor in Airflow\u0026rsquo;s main config: /usr/local/airflow/airflow.cfg (in the scheduler container).\nFor configuration you should pay attention for these params:\n executor sql_alchemy_conn  Concurrency. The parameters you must know. #  Parameters:\n parallelism (default is 32) - defines the max number of tasks that you can execute at the same time. dag_concurrency (16) - defines the number of tasks that can be executed in parallel across all of the dagRuns max_active_runs_per_dag (16) - defines the max number of dagRuns that can run at the same time for a given DAG max_active_runs - defines the max number of simultaneos dagRuns for a specific DAG concurrency - set the number of tasks for a given DAG  Start scaling the Airflow #  Local executor - allows you to execute multiple tasks at the same time on a single macine. Each triggered task is executed in a subprocess.\nTo use it, you need to change the executor parameter to LocalExecutor and add connection for DB like PostgreSQL.\nScaling to the infinity #  Salary executor #  Salary is a kind of a distributed task queue.\nExample of an architecture:\n+----------------+ +------------------+ +----------+ | Node 1 | | Node 2 | +--| Worker 1 | | +------------+ | | +--------------+ | | +----------+ | | Web Server | | | | Metadata DB* | | 2| +----------+ | +------------+ | | +--------------+ | | | Worker 2 | | +------------+ | 1 | +--------------+ | | +----------+ | | Scheduler |-|-------|\u0026gt;| Queue** |\u0026lt;|---+ +----------+ | +------------+ | | +--------------+ | | Worker 3 | +----------------+ +------------------+ +----------+ * - for example - PostgreSQL ** - RabbitMQ or Redis 1 - a task is pushed by te executor to the queue 2 - one of the workers pulls the task from the queue and execute the task Each worker machine must have Airflow installed as well as all dependencies for each DAG.\n"});index.add({'id':12,'href':'/docs/mooc/','title':"Courses notes",'section':"Docs",'content':"Courses notes #  "});index.add({'id':13,'href':'/docs/cloud/serverless/','title':"Serverless",'section':"Cloud",'content':"Оригинальная статья\nЧто такое Serverless #  Serverless - дополнительный слой абстракции, при использовании которого нельзя напрямую влиять на нижележащую инфраструктуру сервиса.\nОсновное отличие serverless-сервисов от традиционных - тарификация: плата начисляется не за время использования, а за:\n объём (хранимых) данных утилизацию ресурсов (количество запросов и трафик)  За счет этого можно более точно спрогнозировать стоимость эксплуатации такого приложения.\nServerless-приложение - такое приложение, которое использует только serverless-сервисы.\nАрхитектура Lambda-функции #   источник события (triggering event) сама функция:  runtime deployment package (логика и зависимости) layers extensions   точка назначения функции (если она требуется):  S3 bucket DynamoDB table SQS etc    Локальная разработка #  SAM\nАрхитектура приложения #  Следует максимально ограничить область применения функции (буквально - отдельная функция на каждый API-endpoint). Такой подход позволяет:\n сократить время выполнения функции (то есть уменьшить стоимость решения) ограничить права каждой отдельной функции в случае если требуется обращаться к нескольким сторонним ресурсам более гибко масштабировать приложение  Организация кодовой базы #  При использовании интерпретируемых языков зависимости и общий код следует складывать в Layer - отдельное хранилище, подобной слою в контейнере и подключаемое к runtime функции.\nПример структуры репозитория:\n. ├── README.md ├── requirements.txt ├── src # исходный код │ ├── lambda # handler\u0026#39;ы приложения │ └── lib # общие библиотеки ├── template.yaml ├── tests └── venv NB: с помощью слоёв нельзя хранить разные версии зависимостей.\nДеплой #  Очевидные варианты:\n TF CF  Надстройки над ними:\n SAM - как абстракция над CF serverless.tf - как абстракция над TF CDK cdktf  Последние два позволяют описывать инфраструктуру в высокоуровневых ЯП.\nСторонние решения:\n Serverless framework - проверить актуальность  Эксплуатация #  Внутренние метрики и логи функции складываются в CloudWatch (по дефолту и при наличии требуемых разрешений).\nДля отладки нескольких функций, работающих конвеером можно применять X-Ray - сервис трассировки, интегрированный в Serverless экосистему. Использование стоит отдельных денег, но его можно включать и выключать на лету.\nБезопасность #   у каждой функции должна быть отдельная IAM-роль с минимально необходимым набором разрешений. область применения каждой функции должна быть ограничена (Make each program function do one thing well)  Дополнительные материалы #   Агрегатор материалов по Serverless CDK lab AWS serverless labs  "});index.add({'id':14,'href':'/docs/misc/prometheus/','title':"Prometheus",'section':"Docs",'content':"Основные понятия мониторинга #  Система мониторинга - набор инструментов, позволяющий снимать метрики со всех узлов инфраструктуры и сохранять их в единой базе для последующего анализа.\nМетрика - любой показатель, который тем или иным образом харектеризует систему, например показатель LA, количество использованной памяти и п.т.\nУзел инфраструктуры - любой используемый компонент, будь то сервер, ВМ или сетевое устройство.\nPush model - агент отправляет метрики на центральный сервер системы мониторинга.\nPull model - центральный сервер самостоятельно опрашивает агентов.\nРассмотрение pull или push модели идет со стороны системы мониторинга.\nУровни мониторинга #   Системный - сбор метрик ОС и железа Сервисный - то же самое для всех используемых сторонних сервисов, например БД, веб-серверов, систем оркестрации и т.д. Приложения - мониторинг самописных приложений  Задачи, решаемые системой мониторинга #   Оповещение о возникновении проблем. Анализ инфраструктуры; прогнозирование потребности к масштабированию. Предоставление исторических данных для расследования инцидентов. Констроль оптимизации приложений.  Ключевые особенности Prometheus #   Встроенная TSDB. Multi-dimentional data model - в БД хранятся не только имена метрик, но и теги, привязанные к ним. Это позволяет создавать более гибкие выборки, что помогает при анализе. PromQL - язык запросов ко встроенной TSDB. Используется pull модель сбора метрик; сервер опрашивает агентов по HTTP с использованием собственного plain-text протокола. Множество вариантов Service Discovery, что упрощает интеграцию с различными платформами.  Архитектура #  Описание архитектуры в документации.\nОсновные компоненты:\n Prometheus сервер, содержащий:   TSDB для хранения метрик HTTP-сервер для работы web UI модуль retrival, отвечающий за опрос агентов и получение от них метрик модуль service discovery, определяющий цели для опроса метрик модуль алертинга, отправляющий сообщения о проблемах в Alertmanager  Exporters - агенты, устанавливаемые на узлах инфраструктуры, которые отдают метрики Alertmanager - отдельный сервис, принимающий от Prometheus-сервера оповещения о проблемах и отправляющий их в соответствующие каналы (email, slack, telegram) Дополнительные сервисы, такие как Grafana, которые могут подключаться к Prometheus и забирать из него данные для последующей обработки  Установка Prometheus #  Docker #  docker run -p 9090:9090 -v /path/to/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus Ручная #   Скачиваем архив с официального сайта Распаковываем содержимое и размещаем его по требуемым директориям Создаём systemd unit и запускаем сервис  Пример unit-файла для Prometheus-сервера [Unit] Description=Prometheus Wants=network-online.target After=network-online.target [Service] User=prometheus Group=prometheus ExecStart=/opt/prometheus/prometheus \\ --config.file=/path/to/prometheus.yml \\ --storage.tsdb.path=/path/to/data \\ --web.console.templates=/path/to/consoles \\ --web.console.libraries=/path/to/console_libraries [Install] WantedBy=default.target    Основные файлы и директории архива:\n console_libraries/ - содержит библиотеки для HTML-шаблонов consoles/ - содержит шаблоны HTML-страниц для web UI prometheus - исполняемый файл сервера prometheus.yml - его конфиг promtool - утилита для проверки конфигурации, работы с TSDB и получения метрик  Основные параметры запуска Prometheus-сервера:  \u0026ndash;config.file=\u0026ldquo;prometheus.yml\u0026rdquo; - путь до конфига \u0026ndash;web.listen-address=\u0026ldquo;0.0.0.0:9090\u0026rdquo; - адрес, которые булеи слушать сервер \u0026ndash;web.enable-admin-api - включить или отключить административный API через веб-интерфейс \u0026ndash;web.console.templates=\u0026ldquo;consoles\u0026rdquo; - путь к директории с шаблонами html \u0026ndash;web.console.libraries=\u0026ldquo;console_libraries\u0026rdquo; - путь к директории с библиотеками для шаблонов \u0026ndash;web.page-title - заголовок веб-страницы (title) \u0026ndash;web.cors.origin=\u0026quot;.*\u0026quot; - настройки CORS для веб-интерфейса \u0026ndash;storage.tsdb.path=\u0026ldquo;data/\u0026rdquo; - путь для хранения time series database \u0026ndash;storage.tsdb.retention.time - время хранения метрик по умолчанию 15 дней, все, что старше, будет удаляться \u0026ndash;storage.tsdb.retention.size - размер TSDB, после которого Prometheus начнет удалять самые старые данные \u0026ndash;query.max-concurrency - максимальное одновременное число запросов к Prometheus через PromQL \u0026ndash;query.timeout=2m - максимальное время выполнения одного запроса \u0026ndash;enable-feature - флаг для включения различных функций, описанных здесь \u0026ndash;log.level - уровень логирования    Экспортеры #  Принцип работы:\n прослушивает указанный в конфигурации порт на предмет входящих подключений при поступлении запроса, опрашивает систему, которую он мониторит (например ОС в целом или конкретный сервис) Prometheus обращается к экспортеру по заданному адресу с указанной периодичностью и получает в ответ все собранные метрики в текстовом формате Prometheus сохраняет полученные данные в локальную TSDB  Обычно экспортеры отдают метрики на http://exporter.url/metrics\nСписок экспортеров\nДля сбора системных метрик в UNIX-like системах используется node_exporter. Собираемые им метрики группируются и собираются отдельными коллекторами, что позволяет гибко настраивать собираемые данные. К примеру, при запуске экспортера можно отключить сбор метрик ФС или CPU, если возникнет такая необходимость.\nОфициальная документация не рекомендует запускать node_exporter в Docker-контейнере, так как для его нормальнйо работы потребуется примонтировать все существующие ФС. Кроме того, ему нужен доступ к host-системе для сбора всех метрик.  Пример unit-файла для _node\\_exporter_ [Unit] Description=Node Exporter Wants=network-online.target After=network-online.target [Service] User=root Group=root Type=simple ExecStart=/path/to/node_exporter [Install] WantedBy=multi-user.target    Формат метрик #  metric_name{tag1=\u0026#34;key1\u0026#34;, tag2=\u0026#34;key2\u0026#34;} X metric_name - имя метрики, используемое в запросах к Prometheus.\nВ фигурных скобках указына теги для данной метрики.\nX - здесь: текущее значение указанной метрики\nПодключение экспортера #  Добавить новую цель для сбора метрик Prometheus\u0026rsquo;ом можно через его конфиг. Ниже пример статической конфигурации:\nglobal: # there are global parameters of a Prometheus server in this section scrape_interval: 15s scrape_configs: # there are scrape targets described in this section # node_exporter target example - job_name: \u0026#39;node\u0026#39; # redefine scrape_interval for a particular job scrape_interval: 5s static_configs: - targets: [\u0026#39;localhost:9100\u0026#39;] PromQL запросы #  Проверить корректность подключения Prometheus-сервера к экспортерам можно при помощи promtool:\npromtool query instant http://localhost:9090 up # up{instance=\u0026#34;localhost:9100\u0026#34;, job=\u0026#34;node\u0026#34;} =\u0026gt; 1 @[1617970210.143] При помощи promtool можно выполнять произвольные запросы к встроенной TSBD, например:\npromtool query instant http://localhost:9090 \u0026#39;node_disk_written_bytes_total\u0026#39; # node_disk_written_bytes_total{device=\u0026#34;vda\u0026#34;, instance=\u0026#34;localhost:9100\u0026#34;, job=\u0026#34;node\u0026#34;} =\u0026gt; 52323148800 @[1617970275.39] Помимо тегов, отдаваемых непосредственно экспортером, Prometheus перед записью метрик в TSDB также добавляет собственные:\n instance - адрес экспортера, с которого получена метрика job - имя задания из, указанное в конфиге сервера  При необходимости можно добавить кастомные теги для каждой цели:\n- job_name: \u0026#39;node\u0026#39; scrape_interval: 5s static_configs: - targets: [\u0026#39;localhost:9100\u0026#39;] labels: env: \u0026#39;dev\u0026#39; Теперь метрики с этого из этой группы экспортеров будут содержать кастомный тег:\npromtool query instant http://localhost:9090 \u0026#39;node_disk_written_bytes_total\u0026#39; # node_disk_written_bytes_total{device=\u0026#34;vda\u0026#34;, env=\u0026#34;dev\u0026#34;, instance=\u0026#34;localhost:9100\u0026#34;, job=\u0026#34;node\u0026#34;} =\u0026gt; 52338947072 @[1617970356.777] Получить данные за определенный промежуток времени можно указав его в квадратных скобках после запроса:\npromtool query instant http://localhost:9090 \u0026#39;node_disk_written_bytes_total{env=\u0026#34;dev\u0026#34;}[1m]\u0026#39; Мониторинг приложений #  Существует множество как официальных, так и поддерживаемых сторонними разработчиками клиентских библиотек для имплементации Prometheus экспортеров. Это позволяет относительно просто добавить возможность собирать метрики непосредственно из приложения.\nGo Описание библиотеки\nGithub\nmkdir -p app/src/app cd app/src/app package main import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; ) func main() { http.Handle(\u0026#34;/metrics\u0026#34;, promhttp.Handler()) http.ListenAndServe(\u0026#34;:2112\u0026#34;, nil) } export GOPATH=/path/to/app go mod init go get go build main.go ./main curl localhost:2112 Python Библиотека\npython3 -m venv .venv source .venv/bin/activate pip install prometheus-client from prometheus_client import start_http_server, Summary import random import time # Create a metric to track time spent and requests made. REQUEST_TIME = Summary(\u0026#39;request_processing_seconds\u0026#39;, \u0026#39;Time spent processing request\u0026#39;) # Decorate function with metric. @REQUEST_TIME.time() def process_request(t): \u0026#34;\u0026#34;\u0026#34;A dummy function that takes some time.\u0026#34;\u0026#34;\u0026#34; time.sleep(t) if __name__ == \u0026#39;__main__\u0026#39;: # Start up the server to expose the metrics. start_http_server(8000) # Generate some requests. while True: process_request(random.random()) python app.py curl localhost:8000  Дополнительные материалы:\n Monitoring apps with prometheus Пишем собственный экспортер для Prometheus  Приём метрик по push модели #  Несмотря на то, что по умолчанию Prometheus работает по pull модели, вместе с ним можно воспользоваться и push моделью. Для её реализации существует специальный экспортер - pushgateway.\nPush-архитектура может быть применена, в случае, когда нам нужно собрать метрики с короткоживущего объекта. Таким объектом может быть, например, задание cron или, скажем, ETL-процесс, для которого важны итоговые метрики.\nОфициальное описание случаев, для которых следует применять pushgateway.\nУстановка и запуск #  Установка в целом такая же, как и у других компонентов.\nСтраница, где можно скачать релиз для ручной установки.\nВ качестве (предпочтительной) альтернативы можно воспользоваться Docker:\ndocker run -d -p 9091:9091 prom/pushgateway Основные параметры запуска _pushgateway_:  \u0026ndash;web.config.file=\u0026quot;\u0026quot; - Экспериментальная опция, позволяющая указать путь к конфигурации для включения TLS при обработке запросов. \u0026ndash;web.listen-address=\u0026quot;:9091\u0026quot; - адрес, на котором pushgateway будет ожидать входящих подключений. \u0026ndash;web.telemetry-path=\u0026quot;/metrics\u0026quot; - путь, по которому будут доступны метрики для Prometheus. \u0026ndash;web.enable-lifecycle - разрешает выключать pushgateway через запрос API. \u0026ndash;web.enable-admin-api - включает административный API, с помощью которого на данный момент можно только удалить все метрики. \u0026ndash;persistence.file=\u0026quot;\u0026quot; - файл, в котором pushgateway будет сохранять полученные метрики. По умолчанию они хранятся в памяти и теряются при рестарте. \u0026ndash;persistence.interval=5m - как часто следует сохранять метрики в файл.    Отправка метрик в pushgateway #  По умолчанию pushgateway слушает на порту 9091 и имеет собственный web UI.\nАлгоритм работы:\n метрика отправляется в pushgateway через HTTP API pushgateway добавляется в качестве одной из целей Prometheus-сервера Prometheus-сервер забирает метрики из pushgateway при обращении к стандартному эндпоинту /metrics.  echo \u0026#34;cron_app_processed_users 112\u0026#34; | curl --data-binary @- http://localhost:9091/metrics/job/cron_app echo \u0026#34;cron_app_payed_sum 13423\u0026#34; | curl --data-binary @- http://localhost:9091/metrics/job/cron_app Флаг --data-binary отправляет полученные данные POST-ззапросом как есть, никак их не изменяя.\nСтоит отметить сообенность формирования URL: job - это лейбл, а cron_app - его значение. По этому тегу pushgateway будет группировать метрики.\nПри запросе метрики будут выглядеть так:\ncurl http://localhost:9091/metrics # TYPE cron_app_payed_sum untyped cron_app_payed_sum{instance=\u0026#34;\u0026#34;,job=\u0026#34;cron_app\u0026#34;} 13423 # TYPE cron_app_processed_users untyped cron_app_processed_users{instance=\u0026#34;\u0026#34;,job=\u0026#34;cron_app\u0026#34;} 112 ... skipped ... # HELP push_failure_time_seconds Last Unix time when changing this group in the Pushgateway failed. # TYPE push_failure_time_seconds gauge push_failure_time_seconds{instance=\u0026#34;\u0026#34;,job=\u0026#34;cron_app\u0026#34;} 0 # HELP push_time_seconds Last Unix time when changing this group in the Pushgateway succeeded. # TYPE push_time_seconds gauge push_time_seconds{instance=\u0026#34;\u0026#34;,job=\u0026#34;cron_app\u0026#34;} 1.6208582543334155e+09 ... skipped ... Последние две метрики:\n push_failure_time_seconds - время, когда последний раз провалилась запись в эту группу push_time_seconds - время последней успешной записи  Их можно использовать для отслеживания самого факта отправки метрик.\n"});index.add({'id':15,'href':'/docs/misc/docker/','title':"Основы Docker",'section':"Docs",'content':"Виртуализация #  Типы виртуализации:\n эмуляция железа - использует ВИ для симуляции требуемого железа  pros: теоретически можно запустить любой софт на любом виртуальном железе cons: очень медленно    +----------+----------+----------+----------+ | Apps | Apps | Apps | Apps | +----------+----------+----------+----------+ | Guest OS | Guest OS | Guest OS | Guest OS | +----------+----------+----------+----------+ | Hardware VM A | Hardware VM B | +---------------------+---------------------+ | Hardware | +---------------------+---------------------+   полная виртуализация\n использует гипервизор для предоставления ВМ нижележащего физического железа в этом случае запускаемая на ВМ ОС должна иметь ту же архитектуру гипервизор может быть:  приложением частью ядра ОС самостоятельной ОС   примеры:  KVM+QEMU Xen VirtualBox VMware Parallels MS HyperV      пара-виртуализация\n разделение процессов с гостевой ОС могут использоваться различные ядра у ядра гостевой ОС есть модуль, который позволяет часть системный вызовов выполнять на хостовой ОС    изоляция/контейнеризация\n запуск некоторых процессов в изолированной среде на той же самой ОС мнение: просто новый способ запуска задач (процессов) в ОС примеры:  Docker Sandbox OpenVZ LXC Singularity (для использования не нужны root права)      +------------------------------------------------------------+ | Private Zone | Private Zone | ... | Private Zone | +------------------------------------------------------------+ | Operating System (host) | +------------------------------------------------------------+ | Hardware | +------------------------------------------------------------+ LXC - одна из первых реализаций контейнеров под Linux (вроде как родом из Google, проверить).\nТехнологии, на которых основана контейнеризация #  CGroups #  Механизм ядра, позволяющий:\n изолировать приоритизировать управлять рассчитывать  ресурсы, доступные системе.\nОсновные ресурсы:\n CPU  ядра процессорное время   memory disk I/O network I/O  Задачи, выполняемые CGroup:\n ограничение ресурсов для группы процессов приоритизировать группы процессов относительно количества доступных ресурсов учитывать ресурсы, которые были потреблены той или иной группой процессов управлять группой процессов, замораживая, восстанавливая или перезапуская ее  Namespaces #  Позволяет ограничить возможность взаимодействия процессов с процессами и иными ресурсами, принадлежащими другой группе (другому пространству имен).\n PID (процесс в таком NS имеет 2 PID - один из хостовой системы, второй из самого NS; при этом другие процессы того же NS могут узнать только второй) FS (mount; прародителем этого типа изоляции был chroot) UTS network SysV IPC  ФС для контейнеров #  ФС самого контейнера, именно на этом уровне реализуется многослойность. Каждый слой образа - RO.\nПри запуске контейнера из этого образа добавляется еще один слой, который уже доступен RW. Этот слой существует только в момент жизни контейнера, как только контейнер удаляется, все данные из внешнего слоя так же удаляются.\nDocker image #  Схема имени образа:\n[host][:port]/[repository]/\u0026lt;image-name\u0026gt;[:tag]\nЕсли:\n host не указан, то по умолчанию подразумевается hub.docker.com, однако это можно изменить в настройках Docker демона. на hub.docker.com в адресе образа отсутствует repository (username в случае именно этого ресурса), это значит, что образ официальный. tag не указан, но подразумевается latest.  Полезные команды #  # Последовательность, позволяющая отключиться от контейнера, не прерывая его работу ^p^q # Запуск остановленного контейнера docker start ${CONTAINER_ID} # Подключение к запущенному контейнеру docker attach ${CONTAINER_ID} # Добавление таймштампов к логу docker logs -t ${CONTAINER_ID} # Сравнение текущего RW слоя контейнера с образом docker diff ${CONTAINER_ID} # Показывает системные PID`ы процессов, запущенных в контейнере docker top # Показывает все контейнеры в системе и ресурсы, которые они потребляют docker stats # Создает tar-архив из ФС контейнера и подает его на stdout docker export # Создает образ из tar-архива docker import # Показывает историю изменений образа docker history Сборка образа #  На каждую строчку Dockerfile запускается контейнер и в нем выполняется инструкция. Если при этом был изменен верхний (RW) слой контейнера, происходит docker commit, тем самым, создаётся новый слой образа.\nДиректива ADD умеет распаковывать tar-архивы и копировать в образ удаленные файлы (при этом не поддерживается распаковка).\nРабота с локальным реджистри #  Для отправки контейнера в локальный (кастомный) реджистри, нужно добавить ему тег с именем реджистри, после чего выполнить docker push.\nDocker Machine #  Средство оркестрации, позволяющее управлять Docker-демонами на удаленных машинах с использованием REST API.\n"});index.add({'id':16,'href':'/docs/misc/cheatsheets/','title':"Cheatsheets",'section':"Docs",'content':"SQL   SELinux commands   Tools   Network   Linux permissions   x64 CPU   AWS   "});index.add({'id':17,'href':'/docs/misc/rpi/','title':"Raspberry Pi",'section':"Docs",'content':"Install CentOS 7 #  # Download an image wget -O centos7-aarch64.raw.xz http://mirrors.powernet.com.ru/centos-altarch/7/isos/aarch64/images/CentOS-Userland-7-aarch64-RaspberryPI-Minimal-4-2009-sda.raw.xz # List available devices diskutil list # Unmount sdcard diskutil unmountDisk /dev/diskN # Flash the image and flush write cache xzcat centos7-aarch64.raw.xz| sudo dd bs=1m of=/dev/rdiskN; sync # Eject sdcard sudo diskutil eject /dev/rdiskN # Resize root partition rootfs-expand Install k8s via kubeadm #  If you are faced into:\n[ERROR SystemVerification]: missing required cgroups: memory  you have to enable memory cgroups by adding cgroup_enable=memory to the /boot/cmdline.txt. Of course you must restart your machine.\n"});index.add({'id':18,'href':'/docs/k8s/operator_intro/','title':"Operator intro",'section':"Kubernetes",'content':"Оператор #  Доклад\n сущность, управляющая другими сущностями в кластере содержит шаблоны поведения основные задачи:  облегчение жизни операторов k8s уменьшение количества микроменеджмента (автоматизация типовых задач по обслуживанию кластера)    Оператор обслуживает весь жизненный цикл (например масштабирование, шардирование и т.д.; в отличии от Helm, который только ставит пакеты)\nПри построении оператора исходить из того, что он будет управлять необходимой сущностью как одним ресурсом (в качестве примера, оператор для кластера CH, который воспринимает его как одну сущность)\nМанифест для однонодной инсталяции CH:\napiVersion: \u0026#34;clickhouse.altinity.com/v1\u0026#34; # NB - apiVersion ссылается на другой URL kind: \u0026#34;ClickHouseInstallation\u0026#34; # NB - кастомная сущность metadata: name: \u0026#34;demo-01\u0026#34; spec: configuration: clusters: - name: \u0026#34;demo Persistent storage #  Local storages:\n emptyDir  доступ к каталогу на хост-машине через демон контейнеризации работает со скоростью доступа к локальному диску данные приколочены к ноде, при перемещении пода, данные теряются   hostPath  использование конкретного каталога на хостовой машине при перемещении на другую ноду данные не переносятся   local  все то же самое, но локальный каталог управляется самим k8s    Взаимодействие оператора с k8s #  kubectl apply -\u0026gt; новые объекты -\u0026gt; API -\u0026gt; Etcd\nController - сущность, которая реализует изменения в наборе объектов, которые были записаны в Etcd.\nCustom Resource Definition - описание \u0026ldquo;структуры данных\u0026rdquo;\nOperator (Custom Resource Controller) - контроллер, который может создать кастомный ресурс. До его появления в кластере, даже если кастомный ресурс будет добавлен в Etcd, ничего не произойдет, потому что дефолтный контроллер не знает что с ним делать.\nОператор может исполняться снаружи кластера.\nЦикл обработки событий оператором #   с помощью K8s API подписывается на те или иные события при наступлении события, реагирует на него тем или иным образом  составление плана действий исполнение поставленного плана    Разделение ответственности между k8s и оператором #  k8s - отвечает за системные ресурсы, базовый набор объектов Оператор - действует только в своей предметной области\nЕсли на момент наступления события, оператор не мог его обработать, k8s продублирует отправку этого события (то есть, видимо, должна быть обратная связь). Т.о., задача оператора, в том числе, обеспечение идемпотентности.\nJobs \u0026amp; CronJobs #  Вспомним, как в Kubernetes реализована концепция остановки подов. Когда приходит время остановить под, то есть все контейнеры в поде, контейнерам посылается sigterm-сигнал и Kubernetes ждёт определённое время, чтобы приложение внутри контейнера отреагировало на этот сигнал.\nВ нашем случае приложение — это простой bash-скрипт с бесконечным циклом, реагировать на сигнал некому. Kubernetes ждёт время, которое задано в параметре graceful shutdown. По дефолту — 30 секунд. То есть если за 30 секунд приложение на sigterm не среагировало, дальше посылается sigkill и процесс с pid 1 внутри контейнера убивается, контейнер останавливается. Поле restartPolicy\nПри проверке backoffLimit поды у нас перезагружались. При этом в манифесте указан параметр restartPolicy: Never. Но когда мы смотрели, как работает опция backoffLimit, поды перезагружались. Здесь нет противоречия: если вы посмотрите на весь yaml-файл, то заметите, что этот параметр относится не к Job, а к спецификации контейнера, который запускается внутри пода.\napiVersion: batch/v1 kind: Job metadata: name: hello spec: backoffLimit: 2 activeDeadlineSeconds: 60 template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: Never Этот параметр говорит kubelet, что делать с контейнером после того, как он был завершён с ошибкой. По умолчанию стоит политика Always, то есть если у нас контейнер в поде завершился, kubelet этот контейнер перезапускает. Причем, все остальные контейнеры в поде продолжают работать, а перезапускается только упавший контейнер.\nЭто политика по умолчанию, и если её применить в Job, то Job-контроллер не сможет получить информацию о том, что под был завершён с ошибкой. С его точки зрения под будет очень долго выполняться, а то, что kubelet перезапускает упавший контейнер, Job-контроллер не увидит.\nЕсли вы укажете только backoffLimit, но забудете указать restartPolicy, то Job будет выполняться бесконечно. Поэтому в Job надо всегда указывать: backoffLimit (количество попыток), activeDeadlineSeconds (общее время), restartPolicy: Never (сказать kubelet, чтобы он никогда не перезапускал контейнер в поде; если контейнер в поде упал, то и сам под считается упавшим, то есть завершённым. Пусть Job-контроллер разбирается, что произошло).\n"});index.add({'id':19,'href':'/docs/misc/kafka/','title':"Kafka basics",'section':"Docs",'content':"Kafka cluster #   broker  отвечает за хранение данных (в бинарном виде) не знаком со внутренней структурой хранимых данных топик - сущность для логического разделения хранимых данных:  на уровне топика можно задать ограничения на:  объём и/или возраст хранимых данных (retention.bytes, retention.ms) количество реплик данных (replication factor) максимальный размер одного сообщения (max.message.bytes) минимальное число согласованных реплик, при котором можно будет записать данные (min.insync.replicas) и многое другое   разбивается на партиции - сущность, непосредственно содержащая сообщения  распределяются по брокерам равномерно (насколько это возможно), что позволяет масштабировать нагрузку на RW в один топик на диске хранится в виде файлов-сегментов с дефолтным размером в 1GB (log.segment.bytes) данные из партиции можно удалить только целым сегментом (не активным) распределение событий (сообщений) по партициям:  нет ключа - round robin (с потерей упорядоченности) есть ключ - murmurHash(key) (с сохранением упорядоченности)   в рамках одной партиции может быть гарантирован порядок событий       zookeeper  роли:  хранилище метаданных координатор кластера   в том числе хранит информацию о:  состоянии брокеров том, какой брокер является контроллером синхронизированы ли партиции с репликами распределении топиков и партиций по брокерам если replication factor \u0026gt; 1:  какая партиция лидер (именно в нее будет идти RW)     в старых версиях также хранил оффсеты (сейчас перенесены в топик __consumer_offsets) при потере, данные с большой долей вероятности превратятся в фарш   producer  сервис, который непосредственно пишет данные в Kafka отправляет KV-сообщения в выбранный топик   consumer  получает данные из Kafka offset - номер последнего сообщения, полученного конкретным подписчиком consumer group - логическое объединение подписчиков, в котором читаемые данные распределяются между участниками группы; позволяет масштабировать скорость чтения    "});index.add({'id':20,'href':'/docs/mooc/datastax_academy/cassandra_operations/','title':"DS210: DataStax Enterprise 6 Operations with Apache Cassandra™",'section':"Courses notes",'content':"Configuring clusters: YAML #  cassandra.yaml - the main configuration file\n Cassandra nodes read this file on start-up  restart of the node if needed for the changes to take effect   Located in the following directories:  cassandra package installations: /etc/dse/cassandra cassandra tarball installations: ${install_root}/resources/cassandra/conf    Minimal properties #   cluster_name listen_address native_transport_address (ip address, which use by the clients to connect to the node) seeds  Commonly user YAML settings #    endpoint_snitch\n  initial_token / num_token\n  commitlog_directory\n  data_file_directories\n  hints_directory\n  saved_caches_directory\n  hinted_handoff_enabled\n  max_hint_windows_in_ms\n  row_cache_size_in_mb\n  file_cache_size_in_mb\n  memtable_heap_space_in_mb/memtable_offheap_space_in_mb\n  Cluster sizing #   estimates are only a rough-order of magnituge dua to metadata things to consider when estimating cluster size:  throughput - how much data per second? growth rate - how fast does capacity increase? latency - how quickly must the cluster respond?    Throughput:\n measure throughput in data movement per time period (e.g. GB/S) consider reading and writing separately a function of:  operation generators (e.g. users) rate of operation generations (e.g. 3 clicks per minute) size of the operations (number of rows X row width) operation mix (read/write ratio)    Growth rate:\n how big must the cluster be just to hold the data? given the write throughput, we can calculate growth  what is the new/update ratio? what is the replication factor? additional headroom for operations    Latency:\n calculating cluster capacity is not enough understand you SALs  in terms of latency in terms of throughput   relevant factors:  IO rate workload shape access patterns table width node profile (i.e. cores, memory, storage, network)   improve estimates with benchmarking  Cassandra stress #  cassandra-stress = utility fir benchmarking/load testing a cluster\n simulates a user-defined workload use the cassandra-stress to:  determine schema performance understand how your database scales optimize your data model and settings determine production capacity   try out your database berore you go into production  You can configure cassandra-stress with yaml file:\n define your schema specify any compaction strategy create a characteristic workload without writing a custom tool  config sections:\n schema description  defines the keyspace and table information if the schema is not yet defined the test will create it if the schema already exists, only defines the keyspace and table names   column description  describes how to generate the data for each column the data values are meaningless, but simulate the patterns in terms of size and frequency generated values follow a specified distribution such as Uniform, Exponential, Gaussian parameters include:  data size value population cluster distribution (the number of values for the column appearing in a single partition (cluster columns only))  EXP(min\u0026hellip;max) - an exponential distribution over tha range EXTREME(min\u0026hellip;max, shape) - an extreme value distribution over the range GAUSSIAN(min\u0026hellip;max, stdvrng) - a gaussian/normal distribution, where mean=(min+max)/2 and stdev is (mean-min)/stdvrng GAUSSIAN(min\u0026hellip;max, mean, stdev) - a gaussian/normal distribution. with explicity defined mean and stdev UNIFORM(min\u0026hellip;max) - a uniform distribution over the range FIXED(val) - a fixed distribution, always returning the same value       batch description  specifies how the test inserts data for each insert operation, specifies the following distributions/ratios:  partition distribution - number of partitions to update per batch (default FIXED(1)) select distribution ratio - portion of rows from a partition included in particular batch (default FIXED(1)/1) batch type - the type if CQL batch to use; either LOGDEG/UNLOGDEG (default LOGDEG)     query description  you can specify any CQL query on the table by naming them under the queries section fields specifies if the bind variables should be from the same row or across all rows in the partition    Nodetool for performance analysis #  nodetool sub-commands:\n info compactionhistory gcstats (gets Java\u0026rsquo;s GC statistics) gossipinfo ring (gets info about tokens range assignments) tablestats teblehistograms tpstats  Low GC times are desirable so Cassandra can spend more time servicing requests.\nSystem and output logs #   by default. the log file is in /var/log/cassandra/system.log also check debug.log in the same directory system.log logs INFO messages and above debug.log logs all messages change the location by adding the following line to /etc/dse/cassandra/jvm.options:  -Dcassandra.logdir=${PATH_TO_NEW_LOG_DIR} Logging levels:\n OFF ERROR WARN INFO (default) DEBUG TRACE ALL  Logging configuration:\n logback.xml (in the same directory as cassandra.yaml) nodetool setlogginglevel - sets log level for particular Java class, until node restart  nodetool getlogginglevels JVM GC logging #  Turn on GC logging:\n statically by editing jvm.options  -XX:+PrintGC - simple, prints a line for every GC and every full GC -XX:+PrintGCDetails - detailed, young generation as well as old and perm gen -XX:+PrintGCTimeStamps - adds time ti a simple or detailed GC log -XX:+PrintGCDateStamps - adds date to a simple or detailed GC log   dinamically by using jinfo  jinfo -flag +PrintGC ${NODE_PID} jinfo -flag +PrintGCTimeStamps ${NODE_PID} jinfo -flag +PrintGCDateStamps ${NODE_PID}    In either case, edit /etc/dse/cassandra/jvm.options git specify the GC log file:\n-Xloggc:${PATH_TO_GC_LOG_FILE} Adding/removing nodes #   reached data capacity problem  your data has outgrown the node\u0026rsquo;s hardware capacity   reached traffic problem  your application needs more rapid response with less latency   to increase operational headroom  need more resources for node repair, compaction, and other resource intensive operations    Adding nodes, best practices #    Single-Token nodes\n double the size of your cluster    VNodes\n we can add nodes incrementally    adding a single node at a time will:\n result in more data movement will have a gradual impact on cluster performance will take longer to grow cluster    adding multiple nodes at the same time:\n is possible use extreme caution    Bootstrapping #  Bootstrapping is a process of a new node joining a cluster:\n the joining node contacts a seed node the seed node communicates cluster info, including token ranges, to the joining node cluster nodes prepare to stream necessary SSTables cluster nodes stream SSTables to the joining node (can be time consuming) existing cluster nodes continue to satisfy writes, but also forward write to joining node when streaming is complete, joining node changes to normal state and handles read/write requests  To bootstrap a node:\n set up the node\u0026rsquo;s configuration files (cassandra.yaml, etc.)  four main parameters:  cluster_name native_transport_address listen_address -seeds   start up the node normally    Seed node is a just one of the cluster\u0026rsquo;s node.\nWhen bootstrapping fails, we have two scenarios:\n bootstrapping node could not connect to cluster  examine the log file to understand what\u0026rsquo;s going on change config and try again   streaming portion fails  node exists in cluster in joining state first, try restarting the node if restarting fails, try deleting data directories and rebooting or, worst case, remove the node from the cluster and try again    Node cleanup\n perform cleanup after a bootstrap on the OTHER nodes reads all SSTables to make sure there is no token out of range for that particular node if the SSTable is not out of range, cleanup just a copy there are options for running these operations is parallel  bin/nodetool cleanup Removing a node #  Other nodes need to pickup the removed-node\u0026rsquo;s data\nThe cluster needs to know the node is gone.\nThree options for dealing with the data:\n redistribute date from the node that is going away  nodetool decomission  need to decrease the size of the cluster the node must still be active decomission will transfer the data from the decomissioned node to other active nodes in the cluster  with VNodes, the rebalance happens automatically with Single-token nodes, you will need to manually rebalance the tiken ranges on the remaining nodes   after running the nodetool decommossion command:  the node is offline the JVM process is still running (use dse cassandra-stop to kill the process) the data is not deleted from the decommissioned node if you want to add the node back to the cluster, delete the data first!!1!  node deleting the data may cause data resurrection issues         redistribute the data from replicas  nodetool removenode  do this if node is offline and never coming back you can run this command only from other node nodetool removenode will:  make the remaining nodes in the cluster aware that the node is gone copy data from online nodes ti the appropriate replicas to satysfy the replication factor       don\u0026rsquo;t redistribute the data, just make the node go away  nodetool assassinate  do this as a last resort if the node is offline and never coming back nodetool assassinate will:  make the remaining nodes in the cluster aware that the node in gone NOT copy any data   you should use nodetool repair of the remaining nodes to fix the data replication      Replacing a down node #  Benefits of replacing a downed node:\n you don\u0026rsquo;t have to move the data twice backup for a node will work for a replaced node, because same tokens are used to bring replaced node into cluster best option is to replace rather than remove and add  Replacing a downed node using nodetool:\n configure a new node fir the cluster normally with one additional step:  in jvm.options add a replace)address JVM option with IP address of the replaced node:    -Dcassandra.replace_address=${DEAD_NODE_IP_ADDRESS}  once you have configured the node, start the node in the cluster monitor the bootstrapping process using nodetool netstats after the new node if bootstrapperd, you need to remove this option from jvm.options manually  What if the dewned node was also a seed node?\n make sure the old IP address does not appear in seeds list in cassandra.yaml also make sure the new IP address is not in the seeds list in cassandra.yaml perform a rolling restart on all nodes se the nodes are aware of the changes to the seeds list start the replacement node using replace_address in the jvm.options file once the replacement bode is fully up:  renove replace_address from jvm.options add the replacement node\u0026rsquo;s IP to the seeds lists in all the nodes' cassandra.yaml    Compaction #  Leveled compaction #    leveled compaction uses a multiplier of 10 per level by default\n  SSTable max size is 160MB (sstable_size_in_mb)\n  SSTable exceed this amount to ensure the last partioion written is complete\n  Leveled compaction is best for read-heavy workload\n occasional writes but high reads    each pertition resides in only one SSTable per level (max)\n  generally reads handled by just a few SSTables\n partitions group together in a handful of levels as they compact down 90% of the data resides in the lowest level (due to 10x rule) unless the lowest level is not yet full    leveled compaction wastes less disk space\n  obsolete records compact out quickly\n a single partition\u0026rsquo;s records group as they compact down updated records merge with older records due to this grouping    Disadvantages:\n IO intensive compacts many mode SSTables at once size tiered compaction compacts mode frequently than size tierd can\u0026rsquo;t ingest data at high insert speeds  Size tiered compaction #  Default compaction type\nSize tiered compaction triggers compaction based on the number of SSTables.\n  groups similarly sized tables together\n  tiers with less than min_threshold (four) SSTables are not considered for compaction\n  the smaller the SSTables, the \u0026ldquo;thinner\u0026rdquo; the distance between min_threshold and max_threshold\n  SSTables qualifying for more than one tier distribute rabdomly amongst buckets\n  buckets with nore than max_threshold SSTables are trimmed to just that many SSTables\n 32 by default coldest SSTables dropped    Size tiered compaction chooses the hottest tier first to compact\n  SSTable hotness determined by number of reads per second per partition key\n  cassandra compacts several tiers concurrently\n  concurrent_compactors\n default to smaller of number of disks or number of cores, with a minimum of 2 and a maximum of 8 per CPU core tables concurrently compacting are not considered for new tiers    Triggering a compaction:\n compaction starts every time a MemTable flushes to as SSTable MemTable too large, commit log too large or manual flush or when the cluster streams SSTable segments to the node  Bootstrap, rebuild, repair   Compaction continues until there are no more tiers with at least min_threshold tables in it  Tombstones\n if no eligible buckets, size tiered compaction compacts a single SSTable this eliminates expired tombstones the number of expired tombstones must be above 20% largest SSTable chosen first table must be at least one day old before considered  tombstome_compaction_interval   compaction ensures that tombstones DO NOT overlap old records in other SSTables  Absorbs high write-heavy workloads ny procrastinating compaction as losg as possible\nOther compaction strategies don\u0026rsquo;t handle ingesting data as well as size tiered\ncompaction_throughput_mb_per_sec controls the compaction IO load on a node\nMajor compaction #   you can issue a major compaction via nodetool compacts all SSTables into a singla SSTable new monolithic SSTable will qualify for the largest tier future updates/deletes will fall into smaller tiers data in laegest tier will become obsolete yet still hog a log of disk space takes a long tine for changes to propagate up to large tier major compactions not recommended  Time window compaction #  Built for time series data\nAn SSTables spanning two windows simply falls into the second window\nGood practice to aim for 50ish max SSTables on disk:\n 20ish for active window 30ish for all past windows combined  for example: one month of data would have window of a day\nTuning:\n expired_sstable_check_frequency_seconds determines how often to check for fully expired (tombstoned) SSTables good to tune when using a TTL  Repair #  This is consistency check across all the node, than examine that all the data is correct.\n Think of repair as synchronizing replicas Repair ensures that all replicas have identical copies of given partition Repair occurs:  if necessaty when detected by reads (e.g. CL=QUORUM) randomly with non-quorum reads (table property read_repair_chance or dclocal_read_repair_chance) manually using nodetool repair    How does repair work?\n nodes build Merkel trees from partitions to represent how current the data value are nodes exchange the Merkel trees nodes compare the Merkel trees to identify specific values that need synchronization nodes exchange data values and update their data  Merkel tree\n a binary tree of hash values the leaves of the tree represent hashes of the values in the partition each tree-nodes is a hash of its children\u0026rsquo;s hash values when tree-nodes hashes are the same, the sub-trees are the same  When to perform a repair:\n if node has been down for a while on a regular basis:  once every gc_grace_seconds mekr sure the repair can complete within the gc_grace_seconds window schedule for lower utilization periods    Is repair a lot of work for the node:\n a full repair can be a lot of work but there are ways to mitigate the work:  primary range repair sub-range repair    Primary range repair:\n the primary range is the set of tokens the node is assigned repairing only the node\u0026rsquo;s primary range will make sure that data is synchorized for that range repairing only the node\u0026rsquo;s primary range will eliminate redundant repairs  Sub-range repair:\n repairs can consume significant resources depending on how much data is under consideration targeting sub-ranges of the table will reduce the amount of work done by a single repair  Nodesync #  DSE 6+ replacement for repair.\nBehaves like continues background repair that delivers:\n low overhead consistent performance easy to use  How to use:\n create a cluster with at least 2 nodes create keyspace with RF \u0026gt;= 2  CREATE KEYSPACE MyKeyspace WITH replication={\u0026#39;class\u0026#39;: \u0026#39;SimpleStrategy\u0026#39;, \u0026#39;replication_factor\u0026#39;: 2};  create table within the keyspace with NodeSync enables  CREATE TABLE MyTable (k int PRIMARY KEY) WITH nodesync={\u0026#39;enabled\u0026#39;: \u0026#39;true\u0026#39;};  NodeSync will now automatically make sure the table data is synchronized  sstabelesplit #  Brakes large SSTable files in a pieces. Before use this tool? you need to stop node.\nMulti DC concepts #   node - the virtual or physical host of a single Cassandra instance rack - a logiacl grouping os physically related nodes DC - a logical grouping of set of racks enables geographically aware read and write request routing each node belongs to one rack in one DC the identity of each node;s rack and DC may be configured in its conf/cassandra-rackdc.properties file  implementing a multi DC cluster:\n use the NetworkTopologyStrategy rather than SimpleStrategy use LOCAL_* consistency level for read/write operations to limit latency specify the snitch  ALTER KEYSPACE MyKeyspace WITH replication={\u0026#39;class\u0026#39;: \u0026#39;NetworkTopologyStrategy\u0026#39;, \u0026#39;DC1\u0026#39;: 1, \u0026#39;DC2\u0026#39;: 2}; nodetool rebuild -- name_of_existing_data_center CQL copy #   cassandra expects every row in the delimited input to contain the same number of columns the number of columns in the delimited input is the same as the number of columns in the Cassandra table empty data for a column is assumed by default NULL value COPY FROM is intended for importing small datasets (a few million rows or less) into Cassandra for impoting larger datasets, use DSBulk  options:\n DELIMITER (default is comma) HEADER (default is false) CHUNKSIZE - set the size of chunks passed to worker process (default value is 1000) SKIPROWS - the number of rows to slip *default value is 0  sstabledump #  dumps the content of the specified SSTable in the JSON format\nyou may wish to flush the table to the disk before dumping its contests\nsstableloader #  provides the ability to:\n bulk load external data into a cluster load pre-existing cluster or new cluster a cluster with the same number of nodes or a different number of nodes a cluster with different replication strategy or partitioner  it doesn\u0026rsquo;t simply copy the set of SSTables to every node, but transfers the relevant parts of the data to each node confirming of the replication strategy of the cluster.\nDSE DsBulk #  Moves Cassandra data to/from files in the file system\nUses both CSV or JSON formats\nBackup #  Cassandra uses spanshots fir backup data, because:\n  they don;n copy out all the data from DB\n  it\u0026rsquo;s a distributed system; every node has only a portion of the data\n  SSTables are immutable, which makes them easy to back up\n  snapshot create hadrlinks on the file system as opposed to coping data\n this is different than coping actual data files (takes less disk space)    therefile very fast\n  represents the state of the data files at a particular point in the time\n  can consist of a single table, single keyspace ot multiple keyspace\n  incremental backup:\n create a hard link to every SSTable upon flush  user must manually delete them after creating a new shapshot   incremental backups are disabled by default (cassandra.yaml, incremetal_backups: true) need a snapshot before taking inkremental backup snapshot information is stored in a snapshots directory under each table directory  backups storred per node and contains only data from this node.\nnodetool snapshot nodetool clearsnapshot JVM settings #  JVM memory areas:\n  code\n  stack\n  heap (is where Java programs allocated and deallocated transient memory)\n  GC refers to when the JVM reclaims the deallocated memory in the heap\n  Settings:\n MAX_HEAP_SIZE (set to max of 8 gb)  large heaps can introduce GC pauses that lead to latency   HEAP_NEWSIZE (set to 100MB per core)  the larger this is, the longer GC pauses time will be; the shorter it is, the more frequently GC will run    Garbage collection #  DSE 6+ keeps one core available for GC and other maintanance activities\nWhat consider when tuning GC:\n pause time  length of time the collector stops the application while it frees up memory   throughput  determined by how often the GC runs and pauses the application more often the GC runs, the lower throughput   we want to minimize length of pauses as well as frequency of collection  JVM available memory:\n Permanent generation new generation (ParNew)  contains:  eden 2 survivor spaces   once eden fills up with new object, JVN trigger a minor GC a minor GC stops execution, iterates over the objects in eden, copies any object that are not (yet) garbage to the active survivor space, and clears eden if the minor GC fills up the active survivor space, it performs the same process on the survivor space objects that are still active are moved to the other survivor space, and the JVM clears the old survivor space it\u0026rsquo;s a stop-the-world algorithm fast:  findong and removing garbage   slow:  moving active objects from eden to survivor space moving active objects from survivor spaces to the old gen     old generation (CMS)  contains objects that have survived long enough to not be collected by a minor GC the CSM collector runs then 75% full    Full GC:\n multi-second GC pauses = Major collections happening if the old gen fills up before the CMS collector can finish, the application is paused while a full GC runs checks everything: new gen, old gen and perm gen significant (multi-second) pauses  Heap dump #   useful when troubleshooting high memory utilization ot OutOfMemoryErrors show exactly which objects are consuming most of the heap Cassandra starts Java with the option -XX:+HeapDumpOnOutOfMemoryError  Tuning the kernel #  Time sync\n Cassandra nodes identify valid data using timestamps  all nodes within a Cassandra cluster need to have synchronized clocks   Time Stamp Counter (TSC) is a simple register within the CPU that counts the number of clock cycles  over time, TSC will drift because the clock cycles may vary between CPUs   Network Time Protocol (NTP) is a way to synchronize CPU clocks  nodes communicate wirh a hierarchy of tine servers to djust their clock adjustments occurs every 1-20 minutes    # to view current limits ulimit -a Since Cassandra nodes don\u0026rsquo;t need to share resources, these limits are not helpful. Turn them off globally by editing limits.conf. Limits take effect when you login. For Ubuntu, use root intead of *.\n *-nofile 1048576 *-memlock unlimited *-fsize unlimited *-data unlimited *-rss unlimited *-stack unlimited *-cpu unlimited *-nproc unlimited *-as unlimited *-locks unlimited *-sigpending unlimited *-msgqueue unlimited  Swap:\n for cassandra, swapping if a very bad event you are better having a node go down than limp along swapping to thoroughly disable swap:  turn off swap for the current kernel process remove swap entries from fstab change the swappiness setting   you can check the current list of swap devices by:  swapon -s  you can turn off swap without rebooting this command will not persist (i.e. will not survive a reboot):  swapoff -a  to look at the current swappiness settings use:  cat /proc/sys/vm/swappiness  this lavlue has range of 0-200 (0 is low an 200 is high) to make sure your kernell deiables swapping after a reboot, edir /etc/sysctl.conf change or add a line to set vm.swappiness = 0 use sysctl -p to get the kernel to reload the changes nade to /etc/sysctl.conf  Changing network kernel settings\n net.ipv4.ip_local_port_range = 10000 65535 net.ipv4.tcp_window_scaling = 1 net.ipv4.tcp_rmem = 4096 87380 16777216 net.ipv4.tcp_wmem = 4096 65536 16777216 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.core.netdev_max_backlog = 2500 net.core.somaxconn = 65000  Hardware selections #   persistent storage type  avoid:  SAN storage NAS diveces NFS   Need to use SSD   memory  for both bare metal and VMs:  prod: 16-64GB; the minimum is 8GB dev in non-loading testing environments: no less than 4GB   more memory means:  better read performanse due to caching memtables hold more recently written data     CPU  cassandra if highly concurent and uses as many CPU cores as available prod:  for bare metal: 16-core CPUs are the current price-performance sweet spot   dev:  2-4 core CPUs     network  you should bind your OS interface to separate NetworkInterface Card (NIC) recommended bandwidth os 1000 Mbits/s or grater native protocols use the native_transport_address cassandra\u0026rsquo;s internal storage protocol uses the listen_address    Security considerations #  Authentication #   desabled by default when enabled, client programs must supply a username and password: enable in dse.yaml  Apache Cassandra supports only pluggable authentication mechanisms.\nDseAuthenticator options has three schemes:\n internal  need to restart the node(s) loggin as cassandra with password cassandra the cassandra user is a superuser - has all permissions:  change the default cassandra password Cassandra stores the credentials in the sys_auth keyspace, so lossing data here could be disastrous      ALTER USER cassandra WITH PASSWORD \u0026#39;new_pass\u0026#39;; ALTER KEYSPACE \u0026#34;system_auth\u0026#34; WITH REPLICATION = {\u0026#39;class\u0026#39;: ;NetworkTopologyStrategy\u0026#39;, \u0026#39;dc1\u0026#39;: 2};  LDAP Kerberos  Cassandra users:\nCREATE TABLE system_auth.roles ( role text PRIMARY KEY, can_login boolen. is_superuser boolen, member_os set\u0026lt;text\u0026gt;, salted_hash text ) Role operations:\nCREATE ROLE SomeRole WITH PASSWORD = \u0026#39;some_pass\u0026#39; AND LOGIN = true; LIST ROLES; DROP ROLE SomeRole; Auth best practices:\n create a second superuser role change the default cassandra password and forget it be sure to replicate system_auth keyspace  Authorisation #  GRANT SELECT ON someKeyspace.someTable to somerole; Permissions:\n ALTER  A. KEYSPACE A. TABLE CREATE INDEX DROP INDEX   AUTHORIZE  GRANT REVOKE   CREATE  C. KEYSPACE C. TABLE   DROP  D. KEYSPACE D. TABLE   MODIFY  INSERT DELETE UPDATE TRUNCATE   SELECT  SELECT    OpsCenter and Lifecycle #  WebUI for DSE\nLife Cycle manager (LCM) - mostly configuration and deployment OpsCenter Monitoring - monitoring and management\n"});index.add({'id':21,'href':'/docs/mooc/datastax_academy/casandra_foundations/','title':"DS201: DataStax Enterprise 6 Foundations of Apache Cassandra™",'section':"Courses notes",'content':"# How to start bin/cassandra # for core version ./dse cassandra # for DSE version bin/nodetool status # provide with cluster health info CQL Fundamentals #  -- CQL - very similar to SQL SELECT * FROM USERS; -- Keyspaces - very similar to schemas in RDBMS; it is top level namespace/container; replication parameters required; contains tables CREATE KEYSPACE some_keyspace WITH REPLICATION = { \u0026#39;class\u0026#39;: \u0026#39;SimpleStrategy\u0026#39;, \u0026#39;replication_factor\u0026#39;: 1 }; -- USE switches between keyspaces USE other_keyspace; -- Tables contain data CREATE TABLE table1 ( column1 TEXT, column2 TEXT, column3 INT, PRIMARY KEY (column1) ); -- INSERT syntax similar to SQL syntax INSERT INTO users (user_id, first_name, last_name) VALUES (uuid(), \u0026#39;John\u0026#39;, \u0026#39;Doe\u0026#39;) -- SELECT also similar to SQL syntax SELECT * FROM users; SELECT first_name, last_name FROM users; SELECT * FROM users WHERE user_id = some_uuid_value; -- COPY - uses to import/export data between tables and CSV files COPY table1 (column1, column2, column3) FROM \u0026#39;table1data.csv\u0026#39;; -- Header parameter skips the first line in the file COPY table1 (column1, column2, column3) FROM \u0026#39;table1data.csv\u0026#39; WITH HEADER=true; -- TRUNCATE - removes table TRUNCATE table; -- Get info about table DESCRIBE table; Core datatypes #   text:  UTF8 encoded strings varchar is same as text   int:  signed 32 bits   timestamps:  date and time 64 bit integer stores number of seconds in UNIX epoch   UUID  generate via uuid()   TIMEUUID  contains timestamp value sortable generate via now()    UUID and TIMEUUID - used place of integer IDs because Cassandra is a distributed DB\nPartitions #  Tables sorted by partition key and splited for parts by its value over the cluster nodes. Partition key - first value of primary key\nClustering columns #  This is other half of primary key; they used for sorting data in partitions\n Primary key - is the most important part of your data model in Cassandra\n You can\u0026rsquo;t change primary key if you already have data in table, because in this case you need to redo your data model\nThere might be multiple cluster columns in table\nTo prevent collisions, also need to add to the primary key some uuid field.\nEvery query should have a pertition key\nYou can perform either equality (=) or range queries (\u0026lt;, \u0026gt;) in clustering columns\nAll equality comparisons must come before inequality comparisons\nSince data is sorted on disk, range searches are a binary search followed by a linear read\nChanging default ordering #  By default clustering columns ordered by ascending\nChange ordering direction via WITH CLUSTERING ORDER BY\nMust include all columns including an up to the columns you wish to order descending\nCREATE TABLE users ( state text, city text, name textm id uuid, PRIMARY KEY((state), sity, name, id)) WITH CLUSTERING ORDER BY (city DESC, name ASC); ) Node #  This is strongly recommend to store Casandra data on the local storage, not on SAN (or sort of)\nNode stores all data in distributed hash tables\nApproximate performance of one node - 6000 - 12000 transactions/second/core. Also, one node can effectively store - 2-4 TB of data\nNodetool #  Tool for node management. Located in ${install_root}/bin/\nSub-commands:\n help info status describecluster getlogginglevels setlogginglevel settraceprobability 0.1 drain stopdaemon flush  Ring #  Ring - name of Cassandra cluster.\nEach node handle specific range of all stored in a cluster data. Coordinator - node, which receives data from a client and after that it transmit data to the node, which handle right range,\nJoining the Cluster #  Nodes join the cluster by communicating with any node. Cassandra finds these seed nodes list of possible nodes in cassandra.yaml Seed nodes communicate cluster topology to the joining node. Node the new node joins the cluster, all nodes are peers.\nNode states:\n joining leaving up down  Drivers #  Drivers may choose which node would base coordinate a request\nDiver policies:\n TokenAwarePolicy - driver chooses node which contains the data RoundRobin - driver round robin the ring DCAwareRoundRobinPolicy - driver round robins the target data center  Vnodes #  With Vnodes, each node is now responsible for multiple smaller slices of the ring, instead of the one large slice.\nWhen new node joins to the ring, each current node streams few slices to it in parallel. And now the new node will responsible for this slices.\nAdding/removing nodes with vnodes helps keep the cluster balanced. By default, each node has 128 vnodes. VNodes automate token ring assignment.\nNumber of vnodes may be configured in cassandra.yaml with num_tokens parameter. Each value greater than one turns on vnodes.\nGossip #  Gossit - broadcast protocol\n Each node initiates a gossip round every second pick on to three nodes to gossip with nodes can gossip with ANY other node in the cluster fault tolerant - continues to spread when nodes fail  Gossip spreads only node metadata, not the client data.\nEndpoint State:\n  Heartbeat State:\n generation (time since node bootstrapped) version (increments every second after gossip with ather node)    Application State (stores node metadata):\n STATUS:  BOOTSTRAP NORMAL LEAVING LEFT REMOVING REMOVED   DC (datacenter) RACK SCHEMA (number of schema changes) LOAD (disk space usage) etc    SYN digest schema - endpoint_ip:generation:version (i.e. 127.0.0.1:100:20)\n  ACK also stores digest of outdated info\n  ACK2 - send only updated info.\n  nodetool gossipinfo SELECT peer, data_center, host_id, preferred_ip, rack, release_version, rpc_address, schema_version FROM system.peers; Snitch #   Determines/declares each node\u0026rsquo;s rack and data center The \u0026ldquo;topology\u0026rdquo; of the cluster  There are several types of snitches:\n Regular:  SimpleSnitch  default snitch places all nodes in the same datacenter and rack (datacenter1 and rack1)   PropertyFileSnitch  reads dc and rack info for all nodes from a file you must keep files in sync with all nodes in the cluster cassandra-topology.properties file   GossipingPropertyFileSnutch (the most popular type)  declare the current node\u0026rsquo;s DC/Rack info in a file you must set each individual node\u0026rsquo;s settings but you don\u0026rsquo;t have to copy settings as with property file snitch Gossip spreads the settings through the cluster cassandra-rackdc.properties file   RackInferringSnitch  infers the rack and DB from the IP address:  2nd octet is DB octet 3rd - rack octet 4th - node octet     DynamicSnitch  layered on top of your actual snitch maitains a pulse of each node\u0026rsquo;s performance determones which node to query replicas from depending on node health turned on by default for all snitches     Cloud Based:  Ec2Snitch Ec2MultiRegionSnitch GoogleCloudSnitch CloudstackSnitch    Configured on cassandra.yaml:\nendpoint_snitch: SimpleSnitch Configuring snitches #   All nodes in the cluster must ust the same snotch Changing cluster network topology requires rastarting all nodes Run sequential repair and clean up on each node  Replication #  With replication factor (RF) 2, each node stores not only its own data, but also its neighbour data. And in this case, coordinator writes data on two nodes.\nWith RF=3, each node stores also data of the neighbour of the neighbour.\nConsistency #  CAP theorem; Cassandra choose Partition Tolerance and Availability.\nConsistency levels - number of acknowledges from the target node to the coordinator during the data writing.\n any (storing a hit at minimum is satisfactory) one, two, three quorum local_one (the closest node to coordinator in the same dc) local_quorum (only for one dc) each_quorum (quorum of nodes in each dc, applies to write only) all  -- cql operator/cqlsh command CONSISTENCY # determine which nodes hold the replicas of the partition tag nodetool getendpoints keyspace_name table_name \u0026#39;partition_key_name\u0026#39; Hinted handoff #  If replica node is down, coordinator stores the data until replica will back online.\nSettings:\n cassandra.yaml you can disable hinted handoff choose a directory to store hints file set the amount of time a node will store a hint default is three hours  Read repair #  With Cassandra, you can choose between absolutely synced with each others node and highly available nodes.\nRead with CL=all:\n coordinator reads data from the closest node and request digest (data\u0026rsquo;s checksum) from others replicas coordinator compares data and checksums and if they equal, sends data to the client. if checksums don\u0026rsquo;t compare:  coordinator finds out a timestamp from received data requests full data from all replicas to compare its timestamps after finding the latest version of data, coordinator sends it to the outdated replicas and to the client.    Read repair chance:\n performed when read is at a consistency level less then ALL request reads only a subset of the replicas we can\u0026rsquo;t be sure replicas are in sync generally you are safe, but no guarantees response sent immediately when consistence level is met read repair done asynchronously in the background dclocal_read_repair_chance set to 0.1 (10%) by default  read repair that is confined to the same DC as the coordinator node   read_repair_chance set to 0 by default  for a read repair across all DCs with replicas    Nodetool repair #   syncs all data in the cluster expensive  grows with an amount of data in cluster   use with a cluster servicing high writes/deletes last line of defense run to synchronize a failed node coming back online run on nodes not read from very often  Node sync #  Full repairs:\n full repairs bog down the system bigger the cluster and dataset. the worse the time in times past, we recommended running full repair within gc_grace_seconds  Node sync:\n Runs on the background continuously repairing you data  quiet hub vs everybody stops what you\u0026rsquo;re doing   better to repair in small chunks as we go rather than full repair automatic enabled by default (in DataStax version)  by you must enable it per table   each node runs NodeSync NodeSync continuously validates and repairs data  CREATE TABLE myTable (...) WITH nodesync = {\u0026#39;enabled\u0026#39;: \u0026#39;true\u0026#39;}; Save Points:\n each node splits its local range into segments  small token range of a table   each segment makes a save point  NodeSync repairs a segment then NodeSync saves its progress repeat   NodeSync priorities segments to meet the deadline target  Segments sizes:\n determining token range in a given segment is a simple recursive split target is each segment is less than 200MB  configurable, but good default, segment_size_target_bytes greater than a partition so partitions greater than 200MB win over segments less than 200MB   algorithm doesn\u0026rsquo;t calculate data size but instead assumes acceptable distribution of data among your cluster  Segment failures\n nodes validate/repair segments as a whole if node fails during segment validation, node drops all work for that segment and starts over node records successful segment validation in the system_distributed.nodesync_status table  Segment outcomes\n full_in_sync: all replicas ware in sync full_repaired: some repair necessary partial_in_sync: not all replicas responded (at lest 2 did), but all respondent ware in sync partial_repaired: not all replicas responded (at least 2 did), with some repair needed uncompleted: one node available/responded; no validation occured failed: inexpected error happend; check logs  Segment validation:\n NodeSync simply performs a read repair on the segment read data from all replicas check for inconsistencies repair stale nodes  Write path #  Writes:\n MemTable (in RAM)  always ordered by partition key and clustering column   Commit Log (on disk)  stored sequentially, every record just append to the commit log   After the data stored into MemTable and Commit log, Cassandra sends acknowledge to the client When MemTAble is full, it flushes to the disk and this structure called SSTable (and now this structure is immutable) After that, Cassandra deletes Commit log, because it already has sorted data on the disk  It\u0026rsquo;s recommended to store Commit Log on different storage as SSTables\n# run stress test cassandra-stress write no-warmup n=250000 -port native=9041 -rate threads=1 # show data of the stress test nodetool cfstats keyspace1.standard1 Read path #  Reads:\n from MemTable:  just find value in MemTable with binary search   from SSTAble:  SSTAble file has an index file, which stores partition token and its offset in bytes Partition summary - im memory index of partition indexes result of read is stored in key cache for case if client want to read similar data next time    Bloom filter #  need to google it !!1!\nDSE Read Path Optimizations #   no partition summary partition index changed to a trie-based data structure SSTable lookups in this format scream! huge performance improvements; especially for the large SSTables  Compactions #  Compaction is a process of merging few SSTables into one SSTable.\nDuring compaction Cassandra selected more recent data (with greater timestamps) When you delete data? Cassandra actually writes a tombstone instead deleted record, so during compaction if this record timestamp is greater than 10 days (by default, configurable in cassandra.yaml), it will skipped, in other case it will write to result file.\nCompaction strategies #   Compaction strategies are configurable. These strategies include:  Size Tiered Compaction (default) - triggers when multiple SSTables of a similar size are present Leveled Compaction - groups SSTables into levels, each of which has a fixed size limit which is 10 times larger than the previous level TimeWindow Compaction - creates time windowed buckets of SSTables that are compacted with each other using the Size Tiered Compaction Strategy   Use the ALTER TABLE command to change the strategy  ALTER TABLE myKeySpace.myTable WITH compaction = {\u0026#39;class\u0026#39;: \u0026#39;LeveledCompactionStrategy\u0026#39;}; "});index.add({'id':22,'href':'/docs/k8s/foundation_of_kubernetes/','title':"Foundation of Kubernetes",'section':"Kubernetes",'content':"https://github.com/amorozov87/kubernetes-traning\nKubernetes quick start #  K8S - opensource система оркестрации контейнеров\nОсновная задача - распределять (по нодам) и менеджить контейнеры с приложениями\nПредоставляет:\n service discovery load balancing autoscaling (как приложения, так и самого кластера) HA декларативный механизм обновлений полезной нагрузки  Контроллер - демон, следящий за состоянием некоторого объекта, например приложения, которое задеплоили\nConceptions #   nodes - узлы кластера  master - нода, на которой задеплоен control plane worker - нода с бизнес приложениями   namespace - виртуальное кластерное пространство внутри одного кластера; нужен для логического разделения задеплоенных приложений pods - базовая сущность кластера, абстракция над одним или несколькими контейнерами controllers  controller manager - основной контроллер кластера, входящий в control plane operator - кастомный контроллер; пишется самостоятельно   labels volumes jobs - одноразово запущенный контейнер, который будет рестартоваться только если процесс завершился с ненулевым кодом выхода (обычный контейнер будет рестартоваться при любом завершении процесса) kubectl - утилита управления кластером  Кластерная роль затрагивает весь кластер. Простая роль доступна только в том неймспейсе, в котором она создана.\nРоль даёт права на выполнение каких-либо действий (внутри неймспейса)\nПравили роли:\n verbs - действия, которые мы можем выполнять apiGroups - каждая сущность кластера имеет собственное API (пустое значение означает, все существующие группы) resources - ресурсы кластера, к которым можно применять перечисленные действия  Роли могут наследовать правила\nKubernetes cluster architecture #   master - содержит весь control plane  etcd - KV db, содержит весь стейт кластера; работает по принципу кворума, поэтому должно быть нечетное количество экземпляров API server - центральное звено, все запросы идет через него; для настройки HA требуется внешний балансер, так как kubelet не может смотреть в несколько API серверов Scheduler - отвечает за распределение подов по кластеру; * Controller Manager - следит за работой компонентов control plane; *   worker node  Container engine, например Docker Kubelet - агент, общается с API сервером и управляет подами на локальной ноде; ответственен за то, чтобы состояние рабочей нагрузки соответствовало тому, что указано для нее на API сервере Kubernetes proxy - отвечает за сетевое взаимодействие внутри кластера    * - умеют самостоятельно выбирать лидера в HA-режиме\nОсновная сетевая концепция Kubernetes - любой под должен быть доступен для любого другого пода напрямую, без участия NAT`а\nDeployment exposed #  Namespace #  Создавался для разграничение энвайроментов с большим количеством пользователей.\nРесурсы уникальны в рамках неймспейса.\nОбладает своими квотами.\nkubectl completion -h # добавление автодополнения kubectl get ns # список неймспейсов kubectl create ns $NS_NAME # создание неймспейса kubectl edit clusterrole $ROLE_NAME # редактирование роли Deployment #  Сущность, которая отвечает за абстракцию над задеплоеным приложением\nПредоставляет декларативные механизмы апдейтов для подов и replicaSet`ов\nменяет актуальное состояние на декларированное\nиспользование дейлойментов - best practice, так как при создании отдельных подов, отсутствует автоматический механизм контроля за его состоянием\nпри обновлении дейплоймента создается новый replicaSet и появляется возможность быстро откатиться к предыдущему состоянию\nDeployment workflow:\nYAML\\JSON Template -\u0026gt; Deployment -\u0026gt; ReplicaSet -\u0026gt; Pod -\u0026gt; Containers kubectl create deployment deploymentName --image=imageName kubectl delete deployment deploymentName Deployment use cases #   развернуть replicaSet задекларировать новое состояние для подов откатиться на более раннюю версию деплоймента масштабировать дейлоймент приостановить деплоймент для применения множественных фиксов (вероятно не применяется на практике) просмотр статуса деплоймента  Pods and Containers #    базовый \u0026ldquo;строительный блок\u0026rdquo; кластера\n  абстракция над одним или несколькими контейнерами\n  всегда имеет уникальный в пределах кластера IP-адрес\n  контейнеры в рамках одного пода:\n всегда расположены на одной ноде имеют единый сетевой неймспейс, соответственно всегда общаются напрямую друг с другом    Init containers #  Специализированный контейнер, который стартует перед контейнером с приложением\n могут содержать и запускать дополнительные утилиты, которые не желательно включать в основной контейнер с приложением (например по соображениям безопасности) предоставляют механизм контролируемой задержки старта основного контейнера  Services (discovery) #  Это абстракция, которая определяет логический набор подов и политики доступа к ним. Поды определяются по лейблам.\nТипы сервисов:\n Normal - имя сервиса резолвится в IP кластера Headless - резолвится напрямую в IP-адрес пода  Пример сервиса:\nkind: Service apiVersion: v1 metadata: name: nginx-service spec: clusterIP: None # для создания headless сервиса selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 В качестве протоколов поддерживаются TCP (по умолчанию) и UDP\nСервис без селектора нужен для того чтобы K8S мог ссылаться на внешние объекты. Примеры использования:\n использование внешней БД в проде и внутренней в тесте использование сервиса в другом неймспейсе миграция нагрузки с K8S на сторонние бакенды  Для такого сервиса EndPoint не будет создан автоматически.\nСпособы открыть доступ к сервису:\n ClusterIP  открывает доступ по внутреннему кластерному IP соответственно, в этом случае сервис доступен только внутри кластера дефолтный тип   NodePort  открывает доступ к сервису по адресу ноды на статическом порте с ним можно общаться снаружи ${NodeIP}:${NodePort}   LoadBalancer  открывает доступ к сервису снаружи с использованием LA, предоставляемого облачным провайдером    kubectl patch... # обновление ресурсов Ingress #   Предоставляет доступный извне URL Балансирует трафик (RR) Терминирует SSL-соединение предоставляет основанный на именах виртуальный хостинг требует наличия Ingress-контроллера (одна из имплементаций - NGINX)  Типы ингрессов:\n Single service ingress Simple fanout Name based virtual hosting  Running you app #  Dry run #  Опция kubectl, которая только печатает объект, который должен был бы быть отправлен.\nkubectl create deployment deploymentName --image=imageName --dry-run=true kubectl create deployment deploymentName --image=imageName --dry-run=true -o yaml \u0026gt; deployment.yaml # пример создания файла деплоймента Using env vars #  При создании пода можно установить переменные окружения, которые будут переданы в контейнер.\nspec: containers: ... env: - name: DEMO_KUBERNETES value: \u0026#34;Hello from k8s\u0026#34; Также можно передавать в переменные окружения информацию о самом контейнере:\n metadata.name metadata.namespace metadata.labels metadata.annotations spec.nodeName spec.serviceAccountName status.hostIP status.podIP  spec: containers: ... env: - name: MY_NODE_NAME valueFrom: fieldPath: fieldPath: metadata.name Commands and arguments #  Аналог ENTRYPOINT и CMD инструкций Docker. Задаются в полях command и args конфига. Не могут быть изменены после того, как Под был создан.\nПеретирают дефолтные команды и аргументы, которые указаны при сборке образа.\nЕсли указаны только аргументы, дефолтная команда из образа будет использована с новыми аргументами.\n   Description Docker field Kubernetes field     Команда, которая будет запущена контейнером Entrypoint command   Аргументы, которые будут переданы команде Cmd args       Image Entrypoint Image Cmd Container command Container args Command run     [/ep1] [foo bar]   [ep-1 foo bar]   [/ep1] [foo bar] [/ep-2]  [ep-2]   [/ep1] [foo bar]  [zoo boo] [ep-1 zoo boo]   [/ep1] [foo bar] [/ep-2] [zoo boo] [ep-2 foo bar]    Scheduling #  Механизмы распределения Подов:\n все механизмы используют лейблы Node Selector (позволяет указать, что деплоймент должен быть размещен на ноде с определенным лейблом; устаревший)  можно объединять несколько лейблов    spec: nodeSelector: labelName: labelValue  Node affinity and anti-affinity (похож на первый, но более гибкий)  позволяет операции над лейблами:  And In NotIn Exists DoesNotExists Gt Lt   правила бывают:  soft/preferred - предпочтение отдаётся нодам с указанными лейблами, но если они недоступны, под будет задеплоен на любую доступную ноду hard/required - требуется полное соответствие лейблам   все правила применяются только в момент деплоя пода, позднейшие изменения не будут иметь эффекта, пока под не будет передеплоен под может быть задеплоен на ноду, если одно из условий nodeSelectorTerms удовлетворено \u0026ndash;--, если все условия matchExpressions были удовлетворены    spec: ... spec: affinity: requiredDuringSchedulingInnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: lableName operator: In values: - labelValue  inter-pod affinity and inti-affinity  аналогично, но с подами, а не с нодами вариант применения - исключить деплой реплик приложения на одну ноду   Taints  имеет:  ключ значение эффект  NoSchedule PreferNoSchedule NoExecute       Tolerations  имеет: ключ значение оператор  Exists Equal   эффект    Image pull policies #   ifNotPresent(DEFAULT) - скачивает образ, если его нет в локальном кеше той ноды, куда приезжает приложение Always(:LATEST) Never  Application restarts #   Always (default) OnFailure Never  Применяется ко всем контейнерам в поде. Поды рестартуют с экспоненциальной дажержкой (10с, 20с, 40с\u0026hellip;)\nApplication logs #  kubectl logs ${podName} --\\\\-- --previous # следует использовать в случае, если контейнер завалился Если под имеет несколько контейнеров, то нужно указывать имя конкретного контейнера.\nЕсли контейнер рестартует, k8s сохраняет один остановленный контейнер с логами.\nЕсли под выселяется с ноды, то все относящиеся к нему контейнеры выселяются вместе с логами.\nВ логи пишется только stdout и stderr PID1\nApplication scaling #  Replica sets #  Может использовать независимо от других сущностей\nНо рекомендуется использовать деплойменты, так как в этом случае, не нужно следить за жизнью контейнера\nОписывает конкретное число подов с приложением, которое должно существовать в конкретный момент времени\nScaling #   по дефолту стартует один под можно скейлить до нуля (выключение) пропорциональное обновление  Autoscaling #  kubectl sutoscale deployment podName  предствален в виде отдельно ресурса и контроллера дефолтное значение задержки 30 сек собирает как ресурсные метрики, так и кастомные заданный процент потреблённых ресурсов высчитывается от requests (см. ниже); если это значение не задано, работать не будет  Compute resources managing #  containers: ... resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; Updating an application #  Deployment update #   Kubectl set image  kubectl run nginx --image=nginx:1.12 --replicas=3 kubectl rollout status deploy nginx kubectl set image deploy nginx nginx=nginx:1.13  Kubectl edit  kubectl edit deploy nginx kubectl rollout status nginx  Kubectl apply  kubectl get deploy nginx -o yaml \u0026gt; nginx_deploymetn.yaml kubectl apply -f nginx_deployment.yaml kubectl rollout status deploy nginx  У k8s есть политика, говорящая сколько подов может быть недоступно в ходе апдейта \u0026ndash;\\\u0026ndash;, может быть создано сверх желаемого значение раскатка деплоймента стриггерится только, если поменялся сам шаблон (изменение метадаты к этому не приводит) каждый раз создается новый ReplicaSet  Deployment rollouts and rollbacks #   Rollout  kubectl apply -f nginx_deployment.yaml --record kubectl rollout status deploy nginx kubectl get deployments  Update  kubectl set image deploy nginx nginx=nginx:1.13 --record=true kubectl rollout status deploy nginx kubectl get pods  Rollback  kubectl rollout history deploy nginx kubectl history deploy nginx --revision=2 kubectl rollout ubdo deploy nginx The deployment lifecycle #   Pregressing  создается новый ReplicaSet поднимается новый ReplicaSet тушится старый ReplicaSet   Complete  все реплики обновлены до последней версии все реплики доступны ни одна старая реплика не запущена   Failed  insufficient quota insufficient permissions readiness probe failures image pull errors limit ranges application runtime misconfiguration    Dealing with storage #  Empty dir #   создается, когда под добавляется на ноду существует, пока под запущен на ноде изначально пустой может быть смонтирован по тому же или отличающемуся пути когда под (по любой причине) удаляется с ноды, данные их emptyDir удалятся безвозвратно  spec: containers: ... volumeMounts: - mountPath: /cashe name: cache-volume volumes: - name: cache-volume emptyDir: {} Host path #   монтирует директорию с хостовой машины может вести себя по разному на различных нодах (поэтому не рекомендуется в проде) нужны привелегии для использования  Git repo #   монтирует пустую директорию и клинирует в нее репу  Persistent volumes #   абстракция над работой с хранилищем может быть запровиженен как автоматически, так и администратором PersistentVolumeClaim (PVC) - запрос на хранилище со стороны пользователя StorageClass - механизм автоматического привиженинга  Application configuration #  Configmap #  Объект k8s, предоставляющий механизм хранения KV данных.\nМожет быть использована как:\n аргумент командной строки переменна окружения файл в волюме  kubectl create configmap nginx-config --from-file=/path/to/dir # k - имя файла, v - содержимое файла kubectl create configmap nginx-config --from-file=/path/to/file # k - имя файла, v - содержимое файла kubectl create configmap nginx-config --from-file=${myKeyName}=/path/to/file kubectl create configmap nginx-config --from-literal=someKey=someValue Директивы \u0026ndash;from-file и \u0026ndash;from-literal можно совмещать.\nИспользование:\n env var:  from a single ConfigMap from multiple ConfigMaps   in pod commands:  echo ${KEY_NAME} (правда нужно предварительно создать переменную окружения)   volume:  монтирование тома с данными, сохраненными в ConfigMap добавление ключей ConfigMap в конкретный путь на томе    Secrets #  Аналогично, но хранится в зашифрованном виде\nОграничения:\n должны быть созданы до пода должны быть созжаны в том же неймспейсе каждый секрет не может быть больше 1 мб  Jobs and daemons #  Jobs #   отдельный ресурс со своим контроллером создает один или несколько подов, обеспечивая успешное выполнение указанной в образе команды как только задача успешно завершается, удаляются все поды, относящиеся к ней если задача завершается с ненулевым кодом ответа, под рестартуется (с экспоненциальной задержкой) поды могут выполнять задачу параллельно  Daemons sets #   обеспечивают запуск одному экземпляру пода на каждой (или некоторых) нодах если в кластер будет добавлена новая нода, приложение приедет и на нее (с учетом всех ограничений шедулинга) удаление Daemon set приведет к удалению всех подов  Примеры использования:\n storage daemon (glusterfs, ceph, etc) агенты агрегатора логов мониторинг агенты  Stateful applications #   StatefulSet - разрабатывался для обслуживания stateful приложений и распределенных систем предоставляет гарантии очередности старта подов каждый под представляет из себя уникальный объект спецификации подов такие же как в деплойменте, но поды не взаимозаменяемые каждый под имеет собственное не шареное хранилище каожый под имеет уникальное имя вида ${StatefulSetName}-${Ordinal}  Ограничения:\n удаление не удаляет хранилища требует Headless сервис, так как обычно распределенные приложения плохо работают с прокси  Политики управления подами:\n OrderedReady (default) Parallel  Стратегии обновления:\n On Delete Rolling Updates Partitions  "});index.add({'id':23,'href':'/docs/misc/old_hardware/','title':"Old hardware",'section':"Docs",'content':"Old hardware notes #  Compaq Evo n600c #  Solaris 9 notes #  How to add on-board network interface in the system:\necho \u0026#39;iprb \u0026#34;pci8086,1038\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/driver_aliases ifconfig iprb0 plumb # OR \u0026#34;touch /reconfigure; init 6\u0026#34;, but bit I didn\u0026#39;t check this method echo `hostname` \u0026gt; /etc/hostname.iprb0 sync reboot Source\nAnd configure this interface to using dhcp:\n Edit /etc/default/dhcpagent by root user ( previously need to add write permissions to this file ). Need to uncomment and modify entry:  RELEASE_ON_SIGTERM=yes Create appropriate interface file to enable dhcp:  touch /etc/dhcp.iprb0 Reboot the machine.  Source\n"});index.add({'id':24,'href':'/docs/languages/','title':"Languages",'section':"Docs",'content':"Notes about different programming languages #  "});index.add({'id':25,'href':'/docs/languages/scala/','title':"Scala",'section':"Languages",'content':"Scala #  Переменные #  val x // константа def x // вычисляются каждый раз, когда на них ссылаются lazy val x // вычисляется не больше одного раза в момент первого к ней обращения var x // \u0026#34;настоящая\u0026#34; переменная, которая может быть переопределена Области видимости #  { val x = \u0026#34;Inner\u0026#34; println(x) } println(x) // Error!!1! Пространства имен #  Идентификаторы: #   стабильные  package параметр функции val lazy val object   нестабильные:  def    Импортирование: #  import com.example.Module.{name1, name2} // нескольких сущностей из пакета import com.example.Module._ // все имена из пространства import com.example.Module.{name1 =\u0026gt; newName} // переименование при импорте import com.example.Module.{_, name1 =\u0026gt; _} // импортирование всех имен, кроме указанных Типы #  A \u0026lt;: B // Тип А является подтипом типа В, когда все значения А могут использоваться как значения типа В.  A \u0026lt;: Any // Надтип для любого другого типа  A \u0026lt;: AnyRef // Надтип для любых ссылочных типов A \u0026lt;: AnyVal // Надтип для любых примитивных типов  val x: String = \u0026#34;Some string\u0026#34; val y: AnyRef = x val z: Any = y val a: Int = 4 val b: AnyVal = a val c: Any = b Nothing \u0026lt;: // является подтипом любого типа, в основном используется при генерации ошибок Примитивные типы: #   Целые числа (знаковые)  Byte (1 byte) Short (2 byte) Int (4 byte) Long (8 byte)   Дробные числа:  Float (4 byte) Double (8 byte)   Символы  Char (2 byte)   Булевы значения:  Boolean   Единичный тип:  Unit (значение этого типа константно, используется, например, для типа значения, возвращаемого из функции, если оно нас не интересует)    Числа #  val a = 3 // Int val b = -5L // Long val c = 10.0 // Double val d: Float // Явно указанный Float val e = 1.03e1 // Вариант записи с мантиссой (научная форма записи) Алгебраические операции с числами: #  val x = -5 val y = 3 x + y // Сложение x - y // Вычитание x * y // Умножение x / y // Целочисленное деление x % y // Взятие остатка -x // Унарный минус Побитовые операции с числами: #  val x = 0xF val y = 0XA1 x \u0026gt;\u0026gt; y // побитовый сдвиг вправо x \u0026lt;\u0026lt; y // -//- влево x | y // -//- ИЛИ x \u0026amp; y // -//- И x ^ y // -//- исключающее ИЛИ ~x // -//- инверсия Порядок операций (по возрастанию, определяется по первому символу оператора) #   символы | ^ \u0026amp; = ! \u0026lt;\u0026gt; : + - * / % все остальные знаки  Ссылочные типы для больших чисел #  val x = BigInt(10) // Целочисленный тип, содержащий неограниченное количество цифр val y = BigDecimal(10) // Дробное число, так же не ограниченное по длине val z = BigInt(\u0026#34;1000\u0026#34;) // Эти типы могу быть инициализированы как числом, так и строчным представлением числа  x pow 100 // Операция возведения в степень для BigInt, не дающая переполнения x gcd y // Нахождение наибольшего общего делителя для двух больших чисел Булевые значения #  val a = true val b = false Операции сравнения: #  val a = 1 \u0026gt; 5 // Больше val b = 1 \u0026lt; 5 // Меньше val c = 1 \u0026lt;= 5 // Меньше или равно val d = 1 \u0026gt;= 5 // Больше или равно val e = 1 == 5 // Равно val f = 1 != 5 // Не равно Логические операции: #  val x = 1 \u0026gt; 5 val y = \u0026#34;Example\u0026#34; contains \u0026#34;a\u0026#34; x \u0026amp;\u0026amp; y // И x || y // ИЛИ !x // НЕ Равенство: #  Сравнивать объекты разных типов можно, но компилятор выдаст предупреждение.\nСравнение по ссылке:\nval x = \u0026#34;Te\u0026#34; val y = \u0026#34;st\u0026#34; val a = x + y val b = x + y a == b // true, так как значения одинаковые a eq b // false, так как ссылки разные В Java все константные строки интернированы: компилятор производит оптимизацию, в результате чего, одинаковые строковые литералы ссылаются на один и тот же объект в памяти. Поэтому:\nval s1 = \u0026#34;foo\u0026#34; val s2 = \u0026#34;foo\u0026#34; println(s1 eq s2) // true Строки #  val name = \u0026#34;Username\u0026#34; // Объявление строки val greet = \u0026#34;Hello\u0026#34; + name + \u0026#34;!!!\u0026#34; // Конкатенация val greet2 = s\u0026#34;Hello $name!!!\u0026#34; // Интерполяция val multiLine = \u0026#34;\u0026#34;\u0026#34; multi line string \u0026#34;\u0026#34;\u0026#34; Операции: #  val s = \u0026#34;aaabbb\u0026#34; s.startsWith(\u0026#34;aa\u0026#34;) s.endsWith(\u0026#34;bb\u0026#34;) s.contains(\u0026#34;ab\u0026#34;) s.matches(\u0026#34;a+b+\u0026#34;) // regexp Практически любой объект имеет метод toString:\nval x = 2 x.toString \u0026#34;x is \u0026#34; + x // При конкатенации приведение к строке происходит автоматически Методы #  Способ оформления переиспользуемого фрагмента кода.\ndef plusOne(number: Int): Int = number + 1 | | | | | | | | | | | +-- тело метода | | | | +-- возвращаемый тип | | | +-- тип параметра | | +-- имя параметра | +-- имя метода +-- ключевое слово // Вызов метода plusOne(4) // 5 Тип результата может быть опущен в случае, если он очевидно следует из тела метода. В этом случае тип будет выведен автоматически:\ndef plusOne(number: Int) = number + 1 Несколько параметров передаются через запятую:\ndef plusOne(x: Int, y: Int, z: Int): Int = x + y + z plusOne(1, 2, 3) // 6 У метода может быть несколько списков параметров. В этом случае при вызове метода входные параметры так же должны быть сгруппированы соответствующим образом:\ndef plusOne(x: Int, y: Int)(z: Int): Int = x + y + z plusOne(1, 2)(3) // 6 Так же у метода может не быть параметров, в этом случае, он воспринимается как \u0026ldquo;переменная\u0026rdquo;, значение которой вычисляется каждый раз при обращении (см. начало)\ndef sixty: Int = 10 * 6 sixty // 60 Тело метода может быть обернуто в фигурные скобки. Возвращено будет значение последнего выражения.\ndef plusAndPrint(x: Int, y: Int): Int = { val result = x + y println(s\u0026#34;$x+ $y= $result\u0026#34;) result } plusAndPrint(2, 4) // prints \u0026#34;2 + 4 = 6\u0026#34;, returns 6 Если метод не производит какие-либо действия, не возвращая ничего, то его возвращаемым типом будет Unit.\ndef plusAndPrint(x: Int, y: Int): Unit = { val result = x + y println(s\u0026#34;$x+ $y= $result\u0026#34;) } plusAndPrint(2, 4) // prints \u0026#34;2 + 4 = 6\u0026#34; Методы могут быть вызваны в телах других методов. В этом случае вложенный метод может ссылаться на параметры внешнего:\ndef plusMul(q: Int, x: Int, y: int): Int = { def mul(u: Int) = q * u mul(x) + mul(y) } plusMul(10, 2, 3) // 50 Последний параметр может быть объявлен повторяемым, то есть он может использоваться как коллекция:\ndef sumAllTimes(u: Int, nums: Int*): Int = u * nums.sum sumAllTimes(3, 1, 2, 3) // 3 * (1 + 2 + 3) = 18 Значение по умолчанию и именованные аргументы:\ndef plus3(x: Int, y: Int = 0, z: Int = 0): Int = 100 * x + 10 * y + z plus3(1) // 100 plus3(1, 2) // 120 plus3(1, 2, 3) // 123  plus3(x = 1) // 100 plus3(1, z = 2) // 102 plus3(x = 1, z = 3, y =2) // 123 При передаче параметра по имени его значение вычисляется не всегда а только тогда, когда в этом действительно есть необходимость. Но стоит отметить, что он будет вычислен столько раз, сколько раз на него будут ссылаться, что может привести к сложно отлаживаемым багам:\ndef replaceNegative(x: Int, z: =\u0026gt; Int): Int = if (x \u0026gt;= 0) x else z replaceNegative(1, 3 * 3 * 3) // 1 replaceNegative(-1, 3 * 3 * 3) // 27 Для рекурсивных функций указание типа обязательно\ndef sumRange(from: Int, to: Int): Int = { if ( to \u0026lt; from ) 0 else from + sumRange(from + 1, to) } sumRange(1, 10) // 55 Функции #  Функция - выражение, которое может быть использовано как метод.\nval x: Int =\u0026gt; Int = ... val y: (Int, Int) =\u0026gt; Int = ... // Максимальное количество параметров - 22 Лямбда-абстракция #  Значения функций можно задавать с помощью лямбда-синтаксиса\nval x: Int =\u0026gt; Int = x =\u0026gt; x + 1 val y: (Int, Int) =\u0026gt; Int = (x, y) =\u0026gt; x + y Тип параметров можно указывать прямо в лямбда-выражении, в этом случае компилятор попробует вывести тип самостоятельно\nval addOne = (x: Int) =\u0026gt; x + 1 val plus = (x: Int, y: Int) =\u0026gt; x + y Короткая запись может быть использована в случае, если каждый параметр используется только один раз и вызывается и все параметры вызываются в том же порядке, в котором передаются в функцию:\nval addOne: Int =\u0026gt; Int = _ + 1 val plus = (_: Int) + (_: Int) Эта-конверсия #  Еще один метод объявления функции, путем конвертации метода:\ndef addOnde(x: Int) = x + 1 val add1 = addOne _ def plus(x: Int, y: Int) = x + y val pl: (Int, Int) =\u0026gt; Int = plus // подчеркивание можно опустить, если для компилятора очевидно, что в этом месте ожидается функция "});index.add({'id':26,'href':'/docs/languages/c_sharp/','title':"C#",'section':"Languages",'content':"C# notes #  Hello world #  using System; public class Test { public static void Main() { Console.WriteLine(\u0026#34;Hello World!\u0026#34;); } } Variables #  var varName = \u0026#34;var value\u0026#34;; String varName2 = \u0026#34;another string\u0026#34;; Flow control #  if (bool condition) { ... } else { ... } "});index.add({'id':27,'href':'/docs/mooc/math/','title':"Книга для чтения по высшей математике",'section':"Courses notes",'content':"Книга для чтения по высшей математике #  Н.Л. Белая\nН.Н. Петров\nОгромное спасибо авторам за терпение, проявленное при обучении гуманитариев и за это прекрасное пособие.\n \u0026ldquo;Начала\u0026rdquo; Евклида Теория множеств Числа Комплексные числа Матрицы Аналитическая геометрия Нечёткие множества Теория пределов Производная Интеграл Дифференциальные уравнения Теория вероятностей Статистика Теория игр Теория графов  "});index.add({'id':28,'href':'/docs/languages/c/','title':"C",'section':"Languages",'content':"C notes #  Pointers #  int x = 1, y = 2, z[10]; int *ip; /* ip -\u0026gt; int*/ ip = \u0026amp;x; /* ip -\u0026gt; x */ y = *ip; /* y = 1 */ *ip = 0; /* x = 0 */ ip = \u0026amp;z[0]; /* ip -\u0026gt; z[0]*/ При передаче в функцию, переменные копируются, поэтому для изменения данных во внешней функции нужно передавать указатели.\nFile open modes #     mode description starts\u0026hellip;     r/rb open for reading (The file must exist) beginning   w/wb open for writing (creates file if it doesn\u0026rsquo;t exist). Deletes content and overwrites the file. beginning   a/ab open for appending (creates file if it doesn\u0026rsquo;t exist) end   r+/rb+/r+b open for reading and writing (The file must exist) beginning   w+/wb+/w+b open for reading and writing. If file exists deletes content and overwrites the file, otherwise creates an empty new file beginning   a+/ab+/a+b open for reading and writing (append if file exists) end    "});index.add({'id':29,'href':'/docs/misc/chromeos/','title':"ChromeOS",'section':"Docs",'content':"Chrome OS #  Crew #  Crew is a Crome OS package manager.\nProblem like this:\nchronos@localhost ~/Downloads $ crew install neovim neovim: Neovim is a refactor, and sometimes redactor, in the tradition of Vim (which itself derives from Stevie). https://neovim.io/ version 0.2.0 Precompiled binary available, downloading... curl: error while loading shared libraries: libmetalink.so.3: cannot open shared object file: No such file or directory Traceback (most recent call last): 9: from /usr/local/bin/crew:974:in `\u0026lt;main\u0026gt;\u0026#39; 8: from /usr/local/bin/crew:905:in `install_command\u0026#39; 7: from /usr/local/bin/crew:905:in `each\u0026#39; 6: from /usr/local/bin/crew:909:in `block in install_command\u0026#39; 5: from /usr/local/bin/crew:611:in `resolve_dependencies_and_install\u0026#39; 4: from /usr/local/bin/crew:697:in `install\u0026#39; 3: from /usr/local/bin/crew:430:in `download\u0026#39; 2: from /usr/local/bin/crew:430:in `chdir\u0026#39; 1: from /usr/local/bin/crew:437:in `block in download\u0026#39; /usr/local/bin/crew:437:in `read\u0026#39;: No such file or directory @ rb_sysopen - ./neovim-0.2.0-chromeos-armv7l.tar.xz (Errno::ENOENT) resolvs by:\ncrew remove curl \u0026amp;\u0026amp; crew install curl "});index.add({'id':30,'href':'/docs/languages/java/','title':"Java",'section':"Languages",'content':"Java notes #  Hello World #  public class HelloWorld { public static void main(String[] args) { System.out.println(\u0026#34;Hello, World!!!\u0026#34;); } } Import #  import com.compamy.somepackage.Name; // imports only specified package  import com.compamy.somepackage.*; // imports each class in this package,  // BUT don\u0026#39;t import classes in subpackages Infrastructure #  JRE = JVM + SE Libs\nJDK = JRE + DevTools\nData types #  Elementary (simple) data types:\n Integer  byte (8 bit, -2^7\u0026hellip;2^7-1) short (16 bit, -2^15\u0026hellip;2^15-1) int (32 bit, -2^31\u0026hellip;2^31-1) long (64 bit, -2^63\u0026hellip;2^63-1)   Float  float (32 bit, 1.4e-45\u0026hellip;3.4e+38) double (64 bit, 4.9e-324\u0026hellip;1.8e+308)   Symbol  char (16 bit)   Boolean  boolean  true false      Flow control #  if (a \u0026gt; b) { ... } else { ... } If we skip curly brackets, \u0026ldquo;else\u0026rdquo; owns only first following expression.\nType conversion #  a + b = result if a || b is double: result is double else if a || b is float: result is float else if a || b is long: result is long else: result is int Referencing subclass objects #  Let\u0026rsquo;s define few example classes:\nclass SuperClass { ... } class SubClassOne extends SuperClass { ... } class SubClassTwo extends SuperClass { ... } There are two ways to refer to a subclass object:\n Subclass reference:  SubClassOne varOne = new SubClassOne(); SubClassTwo varTwo = new SubClassTwo();  Superclass reference:  SuperClass varOne = new SubClassOne(); SuperClass varTwo = new SubClassTwo();  We cannot assign an object of one subclass to the reference of another subclass because they don\u0026rsquo;t inherit each other:  SubClassOne varOne = new SubClassTwo(); // Error!!1!  We cannot assign an object of the parent class to the reference of int subclass:  SubClassOne varOne = new SuperClass(); // Error!!1!   Polymorphism #  Polymorphism means that something (an object or another entity) has many forms.\nJava provides two types of polymorphism:\n static (compile-time; achieved by method overloading) dynamic (run-time; based on inheritance and method overriding)  Method overriding is when a subclass redefines a method of the superclass with the same name.\nThe run-time polymorphism relies on two principles:\n a reference variable of the superclass can refer to any subtype object; a superclass method can be overridden in a subclass.  "});index.add({'id':31,'href':'/docs/misc/macos/','title':"macOS",'section':"Docs",'content':"Terminal utils #  Mount (linux distro) ISO #  First we attach ISO to system:\nhdiutil attach -nomount /path/to/ISO returns something like this:\n/dev/disk2 Apple_partition_scheme /dev/disk2s1 Apple_partition_map /dev/disk2s2 Apple_HFS then mount need device to mountpoint:\nmount -t cd9660 /dev/disk2 /path/to/mount And after usage:\numount /dev/disk2 hdiutil detach disk2 Show CPU info #  # Number of CPU cores sysctl -n hw.ncpu # CPU model sysctl -n machdep.cpu.brand_string Flush DNS cache #  # Big Sur sudo dscacheutil -flushcache sudo killall -HUP mDNSResponder "});index.add({'id':32,'href':'/docs/misc/mikrotik/','title':"Mikrotik",'section':"Docs",'content':"NetInstall #  Netinstall tool originaly delevoped for Windows. I don`t test it via wine.\r Disable Firewall Connect PC to Port 1 on Mikrotik Disable all other network interfaces on the PC - LAN, Wireless, Virtualbox Set static IP of 192.168.88.3 subnet mask 255.255.255.0 gateway 192.168.88.1 on PC Run NetInstall (download page) Select \u0026ldquo;Net Booting\u0026rdquo; Mark \u0026ldquo;Boot Serve Enabled\u0026rdquo; Selected Client IP address of 192.168.88.1 Keep reset pressed while powering on Keep holding the reset pin till a beep sounds. Release immediately Router showed up in Netinstall\u0026rsquo;s list Unzipped \u0026ldquo;all_packages-mipsbe-X.X.zip\u0026rdquo; Selected need packages or all of them by browsing to the unzipped directory and using the \u0026ldquo;Select All\u0026rdquo; button Clicked on \u0026ldquo;Install\u0026rdquo; The progress bar moved to 100% as the packages were uploaded The router rebooted - Beep once, some time later (1 min or so if I recollect) a second beep    source oficial wiki  "});index.add({'id':33,'href':'/docs/misc/tips_and_tricks/','title':"Tips \u0026 Tricks",'section':"Docs",'content':"Common #  Generate password #  $ openssl rand -base64 14 $ gpg --gen-random --armor 1 14 $ cat /dev/urandom | tr -dc a-zA-Z0-9 | fold -w 14 | head -n 1 Generate missed pub key from private #  $ ssh-keygen -y -f ~/.ssh/id_rsa \u0026gt; ~/.ssh/id_rsa.pub Create a large file #   Linux \u0026amp; all filesystems  xfs_mkfile 10240m 10Gigfile  Linux \u0026amp; and some filesystems (ext4, xfs, btrfs and ocfs2)  fallocate -l 10G 10Gigfile  OS X, Solaris, SunOS and probably other UNIXes  mkfile 10240m 10Gigfile Source\nDate and time transformations #  # GNU date timestamp -\u0026gt; human-readable date -d @1339471282 human-readable -\u0026gt; timestamp date --date=\u0026#34;Tue 21 Jul 2020 03:56:50 PM MS\u0026#34; +\u0026#34;%s\u0026#34; # BSD date timestamp -\u0026gt; human-readable date -r 1282368345 human-readable -\u0026gt; timestamp date -j -f \u0026#34;%Y-%m-%d %H:%M:%S\u0026#34; \u0026#34;2018-01-30 15:58:50\u0026#34; \u0026#34;+%s\u0026#34; MySQL #  Change user pass #  ALTER USER \u0026#39;user\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;NEW_USER_PASSWORD\u0026#39;; FLUSH PRIVILEGES; Granting User Connections From Remote Hosts: #  GRANT ALL PRIVILEGES ON *.* TO \u0026#39;user\u0026#39;@\u0026#39;host\u0026#39; IDENTIFIED BY \u0026#39;my-new-password\u0026#39; WITH GRANT OPTION; Git #  Empty commit #  git commit --allow-empty -m \u0026#34;Empty commit message\u0026#34; Rename local branch #  git checkout ${TARGET_BRANCH} git branch -m ${NEW_BEANCH_NAME} Copy file from other branch #  git checkout ${SOURCE_BRANCH} /path/to/file Tmux #  :setw synchronize-panes Windows cmd #  reg - regestery managging tool\nreg query HKEY_LOCAL_MACHINE # shows all keys in selected branch wevtutil - shows Windiows logs\nwevtutil el # shows all available logs wevtutil qe ${LOG_NAME} # shows events from selected log systeminfo - provides software and hardware information\nWSL #  wsl -l -all # List of all available distros wsl -s ${distro_name} # Set a distro as default wsl --unregister ${distro_name} # Delete distro K8S #  Merge multiple kubectl configs into a single file #  # Make a copy of your existing config $ cp ~/.kube/config ~/.kube/config.bak # Merge the two config files together into a new config file $ KUBECONFIG=~/.kube/config:/path/to/new/config kubectl config view --flatten \u0026gt; /tmp/config # Replace your old config with the new merged config $ mv /tmp/config ~/.kube/config # (optional) Delete the backup once you confirm everything worked ok $ rm ~/.kube/config.bak "});index.add({'id':34,'href':'/docs/mooc/ml/','title':"Машинное обучение",'section':"Courses notes",'content':"Машинное обучение #  Введение в машинное обучение и основные понятия статистики #  Представление данных #  Данные представляются в виде таблицы, в которой:\n строки - объекты столбцы - признаки  Ограничение табличного представления данных - все объекты должны иметь одинаковое количество признаков.\nТипы признаков #   Количественный (числовой) - область значения - вещественные числа, сам имеет числовую природу Порядковый - задаёт порядок, последовательность объектов Номинальный (категориальный) - не имеет числовой природы, как правило, число возможных значений конечно.  Бинарный - частный случай номинального признака, имеющий два значения    Характеристики признаков #   максимальное и минимальное значения среднее арифметическое значение медиана - центральное значение в выборке, либо среднее от двух центральных значений, если количество элементов четное; значение медианы в том, что на нее не так сильно влияет попадание в выборку аномальных данных мода - наиболее часто встречающееся значение в выборке; в отличии от среднего и медианы имеет смысл и для номинальных признаков средне квадратичное отклонение (отклонение) - отражает отличие элементов выборки друг от друга, т.е. если все элементы одинаковы, отклонение - нулевое, в остальных случаях - положительное  всегда неотрицательное равно нулю, если все значение признака равны друг другу чем больше величина отклонения, тем сильнее разброс значений выборки относительно среднего значения   \\(\rS_p = \\sqrt{\\frac{1}{n-1}\\sum{i=1}_n(p_i - \\overline{p})^2}\r\\)      Если медианное и среднее значения близки друг к другу, выборка называется симметричной. В таких выборках проще искать аномалии.\nНа практике симметричной считается выборка, для которой выполняется неравенство:\n \\(\r|\\overline{p} - h_p| \\leqslant \\frac{3S_p}{\\sqrt{n}}\r\\)  Где  \\(h_p\\)  - медиана\nКоэффициент корреляции #  Коэффициент корреляции - величина, показывающая как значение одного признака определяет значение другого признака; такая величина должна иметь смысл и для признаков с разными единицами измерения.\nС геометрической точки зрения, КК - показатель того, как значения признаков ложатся на прямую.\nФормула КК:\n \\(\rr(P,Q) = \\frac{\\sum{i=1}^n {p_i}{q_i} - n\\overline{p}\\overline{q}}{(n-1){S_p}{S_q}}\r\\)  Свойства КК:\n принадлежит к отрезку [-1, 1] если равен нулю или близок к нему, то очевидной зависимости между признаками нет. КК \u0026gt; 0 - прямая зависимость между признаками КК \u0026lt; 0 - обратная зависимость чем ближе модуль КК к 1, тем зависимость (при 1, она линейная)  Восстановление пропущенных значений #  Виды повреждения данных:\n пустые (пропущенные) значения (заведомо) некорректные данные  Способы борьбы с пропусками данных:\n удаление объекта (строки) удаление столбца (в случае, если в нем много пустых объектов) замена значения в ячейке:  для числовых признаков:  среднее медиана   для номинальных признаков:  мода генерация случайного значения с учетом вероятности, основанной на имеющихся признаках приведение такого признака к числовому (может привести к появлению некорректных данных)      Метрика. Восстановление пропущенных объектов по их близости друг к другу #  Метрика - обобщение понятия расстояния из геометрии и может быть вычислена для объектов произвольной природы.\nДано два набора данных:\n \\(\rP = (p_1, p_2, ..., p_n)\rQ = (q_1, q_2, ..., q_n)\r\\)  Формулы вычисления метрики:\n Евклидова метрика (геометрическая формула расстояния между точками):   \\(\r\\rho(P,Q) = \\sqrt{(p_1 - q_1)^2 \u0026#43; (p_2 - q_2)^2 \u0026#43; ... \u0026#43; (p_n - q_n)^2}\r\\)  Метрика Манхеттен   \\(\r\\rho(P,Q) = |p_1 - q_1| \u0026#43; |p_2 - q_2| \u0026#43; ... \u0026#43; |p_n - q_n|\r\\)  Max-метрика   \\(\r\\rho(P,Q) = max(|p_1 - q_1|, |p_2 - q_2|, ..., |p_n - q_n|)\r\\)  Свойства метрики #   Расстояние от объекта до него же самого должно равняться нулю Расстояние от одного объекта до другого должно равняться обратному расстоянию Расстояние между двумя точками должно быть меньше чем сумма расстояний между этими точками и любой произвольной, не лежащей на прямой между ними (неравенство треугольника)  Формула восстановления данных с помощью метрики\n \\(\rP(A) = \\frac{1}{\\sum_{i=1}^n \\frac{1}{\\rho(A,A_i)}}(\\sum_{j=1}^n \\frac{P(A_j)}{\\rho(A,A_j)})\r\\)  Способы нормирования признаков #  Нормирование - приведение всех признаков к единому масштабу.\nПризнак:  \\(P = (p_1, p_2, ..., p_n)\\)   \\(\\overline{p}\\)  - среднее значение  \\(s\\)  - отклонение\n Приведение всех признаков в интервал [0, 1]:   \\(\rp\u0026#39; = \\frac{p_i - min(p_i)}{max(p_i)-min(p_i)}\r\\)  Выполнить преобразование, после которого среднее значение и отклонение признака будут равны 0 и 1:   \\(\rp\u0026#39;_i = \\frac{p_i - \\overline{p}}{s}\r\\)  Помимо приведенных формул, к значению признака можно предварительно применять различные функции, например логарифмирование.  Использование коэффициента корреляции для восстановления данных #  КК - выражает как сильно значение одного признака влияют на значения другого признака, т.е. меру близости признаков (т.е. столбцов таблицы). Метрика - выражает меру близости объектов (строк таблицы).\nФормула восстановления пропущенного значения с помощью среднего значения и КК:\nПусть P(A) - значение признака P объекта A  \\(\\overline{P}\\)  - среднее значение признака P Требуется определить P(A) по столбцам-признакам  \\(P_1, P_2, ..., P_m\\)   \\(\rP(A) = \\overline{P} \u0026#43; \\frac{\\sum_{j=1}^n r(P,P_i)(P_i(A) - \\overline{P_i})}{\\sum_{i=1}^m |r(P,P_i|)}\r\\)  Все вычисления проводятся без учета строки с пропущенным значением.\nПри использовании КК в работе с данными признаки можно не нормировать. КК также не изменится при изменении масштаба признаков и переводе в другие единицы измерения.\nПрименение метрик и КК в рекомендательных системах #  Алгоритмы восстановления данных можно использовать при проектировании рекомендательных систем.\nРекомендательная система:\n отслеживает историю действий пользователя выдает рекомендации на основе действий других пользователей, предпочтения которых соотносятся с целевым пользователем  С математической точки зрения это таблица, в которой:\n строки соответствуют пользователям столбцы - товарам на пересечении - оценка, которую пользователь поставил конкретному товару  Таким образом, задача рекомендательной системы сводится к восстановлению пропущенного значения в таблице.\nВ случае, если пользователи анонимны, мерой близости может служить информация о том, как часто те или иные товары попадают в один заказ.\nПоиск выбросов и аномалий #  Задача поиска выбросов (outlier detection) заключается в нахождении всех аномальных объектов в заданном множестве.\nВыброс - (зачастую) реально существующий объект, обладающий аномальными свойствами, сильно отличающийся от других объектов выборки.\n Если данные будут использоваться при решении задачи предсказания, то удаление выбросов, как правило, повышает точность предсказания Удаление выбросов позволяет получить нормальные (типичные, эталонные) объекты Многие характеристики (например, среднее значение) очень чувствительны к наличию выбросов  Идеальных методов обнаружения выбросов не бывает потому, что\u0026hellip;\n \u0026hellip; не существует формального определения выброса \u0026hellip; алгоритм, беспощадный к выбросам, будет удалять и часть \u0026ldquo;нормальных\u0026rdquo; объектов \u0026hellip; алгоритм, гуманный к \u0026ldquo;нормальным\u0026rdquo; объектам, будет пропускать часть выбросов  Методы обнаружения выбросов #   Поиск аномальных объектов с помощью здравого смысла; например, если известен нормальный диапазон для значений признака. Методы, основанные на анализе одного признака (каждый признака берётся отдельно и ищутся объекты с аномальными значениями этого признака). Методы, основанные на одновременном анализе нескольких признаков.  Методы, анализирующие признаки по отдельности #  Дано:  \\(P = (p_1, p_2, ..., p_n) \\)  Где,\n \\(\\overline{p} - среднее значение\\)   \\(n - объём выборки\\)   \\(S_p - отклонение\\)  Основная идея поиска аномалий - найти значения  \\(p_i\\)  , расположенные вдали от среднего значения или медианы.\nПростейшие методы поиска:\n Удалить все объекты, у которых величина  \\(|p_i - \\overline{p}|\\)  слишком велика (этот метод не учитывает величину отклонения признака; для некоторых признаков большой показатель отклонения это норма) Удалить все объекты, у которых величина  \\(\\frac{|p_i - \\overline{p}|}{S_p}\\)  слишком велика Более продвинутый критерий Шовене Определение выбросов без использования среднего и отклонения  Простое правило для определения выброса (второй метод) #   Пусть X - подозрительное значение Исключим X из выборки и по оставшимся элементам вычислим среднее и отклонение Если выборка симметричная, то X будет выбросом, если он не принадлежит интервалу  \\(\r(\\overline{p} - 3S_p, \\overline{p} \u0026#43; 3S_p)\r\\)   Если выборка не симметричная, то X будет выбросом, если он не принадлежит интервалу  \\(\r(\\overline{p} - 5S_p, \\overline{p} \u0026#43; 5S_p)\r\\)    Критерий Шовене (Chauvenet) #  Значение  \\(p_i\\)  является выбросом, если выполнено неравенство\n \\(erfc(\\frac{|p_i - \\overline{p}|}{S_p}) \\le \\frac{1}{2n}\\)  Где  \\(erfc(x) = \\frac{2}{\\sqrt{\\pi}}\\int_x^\\infty e^{-t^2}dt\\)  дополнение функции ошибок\nЭтот алгоритм применяется итерационно до тех пор, пока в выборке не перестанут находиться аномалии.\nПоиск выбросов без использования среднего и отклонения #  Значение среднего и отклонения, сильно чувствительны к наличию выбросов. Таким образом, возникает замкнутый круг: мы ищем выбросы с помощью характеристик, чьи значения обусловлены наличием выброса.\nКвартили:\n 1-ая квартиль  \\(Q_{25}\\)   - это такое число, что ровно 25% выборки меньше его 2-ая квартиль  \\(Q_{50}\\)   - это такое число, что ровно 50% выборки меньше его (фактически это медиана) 3-ая квартиль  \\(Q_{75}\\)   - это такое число, что ровно 75% выборки меньше его  Таким образом, 50% выборки принадлежат интервалу  \\([Q_{25}, Q_{75}]\\)  , их можно считать \u0026ldquo;эталонными\u0026rdquo;. Соответственно, элементы, достаточно удаленные от этого интервала можно считать выбросами.\nЕсли элемент не попадает в интервал:  \\(\r(Q_{25} - 1,5*(Q_{75} - Q_{25}), Q_{75} \u0026#43; 1,5*(Q_{75} - Q_{25}))\r\\)  то он является выбросом.\nМетоды, анализирующие несколько признаков #   необходимы в случаях, когда аномальное значение сходно, к примеру, со средним между нормальными значениями аномалии могут характеризоваться не только экстремальными значениями одного признака, но и нестандартными комбинациями нескольких признаков (например, сами по себе вес в 100 кг и рост 150 см не являются аномалиями, но их сочетание у одного объекта - является)   Метрические методы   опираются на функцию расстояния, то есть на метрику основная идея: у выброса мало соседей, а у типичного объекта много  для каждого объекта находится расстояние до всех остальных объектов и определяется ближайший сосед если расстояние от объекта до ближайшего соседа велико, то такой объект - аномалия    Геометрические методы   объекты проецируются на n-мерное пространство (например, на плоскость, в случае, если объекты могут быть описаны двумя признаками) строится выпуклая оболочка - многоугольник (для двумерного пространства) или многогранник, который неким образом описывает точки аномалиями в данном случае будут точки, лежащие на оболочке  Поиск с помощью кластеризации   при этом, объекты будут разделены на группы по некоторому признаку (или их сочетанию) при таком подходе выбросами считаются элементы малых (в том числе одноэлементных групп)  Поиск с помощью моделей предсказания   некоторые вариации метода опорных векторов (SVM) вариация решающих деревьев (decision trees) под названием \u0026ldquo;изолирующий лес\u0026rdquo; с помощью произвольной модели предсказания некоторого признака P по другим признакам таблицы  в этом случае выбросом будет тот объект, для которого предсказанное и имеющееся значения разойдутся очень сильно    Кластеризация (clustering) #  Задача алгоритма кластеризации состоит в разбиении множества данных объектов на несколько групп(кластеров), состоящих из похожих друг на друга объектов.\nЗадачи кластеризации:\n вычисления степени сходства объектов упрощение дальнейшей обработки данных (обработка меньших групп данных) сокращение объема хранимых данных за счет оставления одного представителя (эталона) от каждого кластера (задача сжатия данных) поиск выбросов разбиение признаков на кластеры и оставление по одному признаку из каждого кластера (отбор признаков)  Типы алгоритмов:\n разбивающие данные на заданное число кластеров (то есть число кластеров - входной параметр алгоритма)   пример: алгоритм k-means недостатки:  человеческий фактор(проблемы с предсказанием верного количества конечных кластеров)    в которых число кластеров не определено заранее, а вычисляется самим алгоритмом   пример: алгоритм FOREL недостатки:  алгоритм может выдать слишком много(мало) кластеров; в этом случае вся операция бесполезна    Если алгоритм кластеризации использует метрику на множестве объектов, то значения всех признаков необходимо предварительно нормировать!!1!  Кластеризация с помощью графов #  Данные представляются в виде графа, где:\n вершина - объект ребро - расстояние между объектами  Соответственно перед построением графа необходимо вычислить расстояния между каждой парой объектов\nОписание алгоритма:\n На вход алгоритма подается некоторое число R Из графа удаляются все ребра, метрики которых \u0026gt; R Оставшиеся после этого связными компоненты графа являются кластерами  Описание второго алгоритма\n На вход подается желаемое число кластеров k Строится остовное дерево (это подграф, содержащий все вершины исходного графа и не имеющий циклов) c минимальной суммарной длиной ребер; для этой задачи могут применяться:   алгоритм Краскала алгоритм Прима  Удаляем из дерева k-1 самых длинных ребер В один кластер попадают вершины из связанных компонент  Алгоритм FOREL (формальный элемент) #  Главное свойство алгоритма - количество кластеров не определено заранее Идея - найти точки сгущения объектов и объявить эти сгущения кластерами\nОписание алгоритма:\n На вход подается число R Представление данных: объекты представляются точками в пространстве  \\(R^m\\)  , где m - количество признаков объекта   В произвольную точку пространства добавляем формальный объект F (отсюда и название) Пусть K - все объекты, до которых расстояние от F меньше R Находим центр тяжести объектов из множества K и переносим туда объект F. Переходим к шагу 2   итерируемся по шагам 2-3 до тех пор, пока множество K не стабилизируется (то есть в него перестанут добавляться новые элементы и удаляться старые)  После стабилизации множество K объявляется новым кластером и объекты, попавшие в него удаляются из выборки Возвращаемся на шаг 1, если выборка не пуста, иначе алгоритм завершается  Центр тяжести - точка, координаты которой совпадают со средним значением признака.\nАлгоритмы k-means (k-средних) #   Главное свойство: количество кластеров k определено заранее Идея реализации: одновременно происходит поиск всех центров кластеров  Описание алгоритма (одна из реализаций):\n Вход: число кластеров k Представление данных: объекты представляются точками в пространстве  \\(R^m\\)  , где m - количество признаков объекта   Генерируем k случайных точек - центры кластеров Объект будет отнесен к тому кластеру, чей центр расположен ближе всех к этому объекту В получившихся кластерах центр переносится в центр тяжести, возврат на шаг 2   шаги 2-3 повторяются до тех пор, пока центры кластеров не стабилизируются  Недостатки алгоритма:\n результат зависит от выбора исходных центров кластеров, их оптимальный выбор неизвестен  Выбор оптимального числа кластеров #  Эта проблема актуальна для алгоритмов, в которых число кластеров является входным параметром.\nИдея - будем перебирать значения k, пока \u0026ldquo;качество кластеризации\u0026rdquo; не стабилизируется.\n Пусть  \\(S_k\\)  - сумма расстояний от объектов до центров их кластеров (при условии, что объекты разбиты на k кластеров). Тогда величину  \\(|S_{k\u0026#43;1} - S_k|\\)  можно рассматривать как увеличение качества кластеризации при переходе от k кластеров к (k+1) кластеру. Таким образом, \u0026ldquo;качество кластеризации\u0026rdquo; стабилизируется для такого k, где величина  \\(|S_{k\u0026#43;1} - S_k|\\)  становится небольшой.  Кластеризация по столбцам #  Транспонирование - зеркальное отображение таблицы отнисительно ее диагонали; операция, позволяющая поменять местами строки и столбцы, в нашем случае - объекты и признаки.\nСобственно сама кластеризация по столбцам выполняется в два шага:\n данные транспонируются применяется один из стандартных алгоритмов кластеризации  Назначение:\n позволяет найти близкие (по значению) друг к другу признаки. Можно из каждого кластера оставить по одному признаку и тем самым уменьшить размер данных это оправдано, так как слишком большое число признаков часто мешает анализу данных  Задача предсказания, линейная регрессия #  Предсказание (prediction):\n есть множество объектов M с известными значениями признака Y (целевой признак) требуется найти Y для нового объекта  \\(A \\notin M\\)    Задачи предсказания:\n Предсказание количественного признака Y называется задачей регрессии Предсказание номинального (категориального) признака Y называется задачей классификации  План решения задачи регрессии #  Общий план решения:\n множество объектов разбить на 2 множества:  тренировочную выборку (Train) тестовую или проверочную выборку (Test)   модель предсказания будет строиться по объектам Train, а качество проверяться объектам Test  Показатели качества регрессии:\n MAE (средняя абсолютная ошибка):   \\(\rMAE = \\frac{1}{n} \\sum_{i=1}^n{|y_i - y\u0026#39;_i|}\r\\)  Где:\n \\(n - объем тестовой выборки\\)   \\(y_i - истинные значения\\)   \\(Y\u0026#39;_i - предсказанные значения\\)  MAPE (средняя абсолютная ошибка в процентах):   \\(\rMAPE = \\frac{1}{n} \\sum_{i=1}^n{\\frac{|y_i - y\u0026#39;_i|}{|y_i|}} * 100\\%\r\\)  Модель регрессии не обязана давать точный ответ на объектах тренировочной выборки, так как модель не запоминает значения признаков тренировочной модели, а выстраивает зависимость между значениями целевого и нецелевых признаков.  Принцип работы модели (линейной) регрессии:\nУзлы - это объекты тренировочной выборки\nПредсказываться значения Y в новых точках можно с помощью:\n ломаной линии (линия, соединяющая все точки, лежащие на пересечении целевого и нецелевого признаков)  недостатки:  модель предсказания имеет сложность сравнимую с объемом данных модель нельзя проинтерпретировать нет уверенности, что на тестовой выборке будут небольшие ошибки эту модель нельзя экстраполировать     прямой регрессии, проходящей примерно посередине между объектами  Модель регрессии называется линейной, если значение предсказываемого признака Y вычисляется как сумма известных признаков  \\(X_1, X_2,... X_m\\)  , взятых с некоторыми коэффициентами\n \\(\ry\u0026#39; = w_1x_1 \u0026#43; w_2x_2 \u0026#43; ... \u0026#43; w_mx_m \u0026#43; w_0\r\\)  Задача заключается в нахождении оптимальных весов (коэффициентов)  \\(w_i\\)  .\nПринцип: для объектов тренировочной выборки нужно минимизировать отклонение предсказываемых значений от истинных значений признака Y.\nЛинейная регрессия неустойчива к выбросам, соответственно, их необходимо удалять заранее.  Построение модели линейной регрессии #  Отклонение истинного от предсказанного значения равно  \\(|y\u0026#39; - y| = |w_1x \u0026#43; w_0 - y|\\)  . Эту величину нужно минимизировать для всех объектов тренировочной выборки.\nПоиск точки минимума этой функции осложняется тем, что модуль-функция - недифференцируемая. Поэтому на практике минимизируют несколько иную функцию: сумму квадратов отклонений. Для нахождения точки минимума применяют частные производные.\nВ общем случае:\nКогда нецелевых признаков больше одного, все происходит аналогично, только параметров  \\(w_i\\)  будет больше (и полученная зависимость  \\(y\u0026#39;=...\\)  будет уже определять не прямую, а гиперплоскость).\nТаким образом, основная трудоемкость при построении линейной регрессии заключается в решении системы линейных уравнений на последнем шаге.\nПроблемы модели линейной регрессии #  Получаемая при построении линейной регрессии система линейных уравнений может:\n Не иметь решений Иметь более одного решения  Такие проблемы возникают, когда между нецелевыми признаками существует линейная зависимость или высокая корреляция. Такую проблему еще называют \u0026ldquo;проблемой мультиколлинеарности\u0026rdquo;.\nНапример, в случае, если значения нецелевых признаков совпадают, на выходе мы фактически получим систему, в которой неизвестных больше, чем уравнений (потому что несколько уравнений будут идентичны), а такая система имеет бесконечное количество решений.\nПроблемы системы с бесконечным количеством решений:\n зависимость целевого признака Y никак нельзя проинтерпретировать (так как любая выбранная модель будет одинаково хороша или плоха) возможна большая ошибка на объектах, не попавших в тренировочную выборку; это произойдет, когда в качестве коэффициентов будут выбраны большие числа  Советы по поиску хорошей модели регрессии:\n Отбор признаков. Нужно удалять нецелевые признаки, которые линейно зависят от других или имеют высокую корреляцию с другими признаками Коэффициент регрессии можно минимизировать (\u0026ldquo;регуляризация\u0026rdquo; и \u0026ldquo;лассо\u0026rdquo;)  Обобщение модели линейной регрессии:\n Регуляризация (Ridge-regression)  Основная идея:\n нужно стремиться сделать коэффициент регрессии минимальным величины весов также должны быть минимальными (по модулю)  Способ минимизации:\n \\(\rR = L(w_0, w_1, w_2, ..., w_m) \u0026#43; C(w_0^2, w_1^2, w_2^2, ..., w_m^2)\r\\)  Где C - заданная константа\nЛассо  Полиноминальная регрессия #  "});index.add({'id':35,'href':'/docs/mooc/sams_cv/week1/','title':"Нейрон и нейронная сеть",'section':"Нейронные сети и компьютерное зрение",'content':"Нейрон и нейронная сеть #  Строение биологического нейрона: #   ядро (тело) нейрона дендриты (малые отростки; служат для приема сигнала от других нейронов) аксон (большой отросток; служит для передачи сигнала к другим нейронам) синапс (соединение аксона одного нейрона с дендритом другого нейрона; изменяется со временем, в зависимости от обстоятельств может становиться сильнее или слабее)  сильный (обеспечивает практически полную передачу электрического сигнала) слабый (практически не передает сигнал)    Тренировка биологической нейронной сети заключается в настройке синапса.\nНейрон накапливает заряд до определенного предела и только после этого сигнал уходит по аксону к дендритам других нейронов.\nМатематическая модель нейрона #   сумматор вход выход функция активации (аналог механизма накопления заряда) синаптический вес (коэффициент входящего сигнала; настраиваемый параметр) смещение (настраиваемый параметр)    \\(y = f(z) = f(w_0x_0 \u0026#43; w_1x_1 \u0026#43; w_2x_2 \u0026#43; b) = f(\\lang w,x \\rang \u0026#43; b)\\)  Где:\n y - выходное значение z - результат работы сумматора f(z) - функция активации w - синаптический вес x - входное значение b - смещение  \u0026lt;\u0026gt; - скалярное произведение\nФункция активации #   Пороговая (аналогична существующей в биологическом нейроне)   \\(x = \\begin{cases} 0 \u0026amp;\\text{if } x \u0026lt;= 0 \\\\ 1 \u0026amp;\\text{if } x \u0026gt; 0 \\end{cases}\\)  Разделяющая поверхность - точка, в которой функция меняет значение с 0 на 1. В этой точке аргумент функции активации равен 0. Это плоскость, которая задается вектором w и смещением b. Положительное значение функции по отношению к разделяющей поверхности сонаправлено с вектором w.\nСигмоида   \\(\\sigma(x) = \\dfrac{1}{e^{-x}} \\\\ \\sigma(x) \\rarr 1 \\text{ if } x \\rarr \\infin \\\\ \\sigma(x) \\rarr 0 \\text{ if } x \\rarr -\\infin \\\\\\)  Если ввести параметр температуры (Т), появляется возможность регулировать наклон сигмоиды. При низких значениях температуры график сигмоиды стремится к графику пороговой функции.\n \\(\\sigma(x) = \\dfrac{1}{e^{\\frac{-x}{T}}}\\)  Нейронные сети #  При объединении нейронов между собой разделяющая поверхность перестаёт быть линейной и может, в том числе, образовывать несвязанные друг с другом области.\nКомбинировать полносвязанные линейные нейроны бессмысленно.\nДоп материалы #   Нейроны не любят одиночество Рост отростков одного нейрона Биологический нейрон (учебный фильм СССР) Частная жизнь нейрона 1987 (учебный фильм СССР) Нейроны мозга (Discovery) Science - Structure of Neuron_New Inside the Brain: Unraveling the Mystery of Alzheimer\u0026rsquo;s Disease [HQ] Transport inside the brain: The basic mechanisms of neuronal trafficking  "});index.add({'id':36,'href':'/docs/mooc/sams_cv/','title':"Нейронные сети и компьютерное зрение",'section':"Courses notes",'content':"Конспект курса #  Нейронные сети и компьютерное зрение\n"});index.add({'id':37,'href':'/docs/mooc/sams_cv/week2/','title':"Строим первую нейронную сеть",'section':"Нейронные сети и компьютерное зрение",'content':"Строим первую нейронную сеть #  Восстановление зависимостей #  Размеченная обучающая выборка состоит из объектов, для которых мы знаем:\n некоторые признаки метку объекта  Хорошей практикой является разделение датасета на три поддатасета:\n train (используется непосредственно для обучения модели) valid (используется для подстраивания параметров обучения нашей модели) test (используется для проверки окончательного результата)  Компоненты нейронной сети #   Архитектура нейронной сети Функции потерь (способ определения результата работы создаваемой сети; минимум этой функции соответствует оптимально настроенной сети) Метод оптимизации (говорит о том, как именно нужно изменить настройки сети для минимизации функции потерь) Метрики (показывают насколько успешно сеть решает поставленную задачу; например: точность; в отличии от функции потерь могут быть не дифференцируемыми)  Результирующая зависимость - сумма сигмоидных функций с соответствующими параметрами.\nФункция потерь #  Для задач восстановления скрытых зависимостей зачастую в качестве функции потерь используют функцию среднего квадрата ошибки (MSE, mean squared error):\nMSE=(1/N)*sum_N_i=1(y_av_i - y_i)^2 где:\n y_av_i - результат работы сети y_i - целевые значения  То есть это сумма квадратов отклонения полученных результатов от ожидаемых значений.\nАлгоритмы настройки нейронной сети #  Градиентный спуск #  w0 - вектор весов, который содержит все значения весов и смещений, которые используются в сети.\nГрадиент функции потерь - вектор, состоящий из производных по каждой из координат функции. Градиент указывает в сторону наибольшего роста функции потерь =\u0026gt; требуется сделать шаг из точки w0 в направлении обратном направлению градиента. Далее шаги повторяются.\nГрадиентный спуск находит минимум функции, но не гарантирует нахождения оптимального минимума.\nОграничения, накладываемые на функцию потерь:\n должна быть дифференцируемой (если в некотором множестве точек производная не определена, ее можно доопределить) производная функции потерь не должна быть равна нулю в большинстве точек  Правило цепочки (правило производной сложной функции) #  Граф вычисления - порядок вычисления сложной функции.\nпересмотреть\n"});})();